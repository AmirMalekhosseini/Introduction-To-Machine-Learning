{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8KGbM_biRpY"
   },
   "source": [
    "<b><h4>HW3 - Notebook: Multi-Layer perceptron</h4></b>\n",
    "\n",
    "[CE40477: Machine Learning](https://www.sharifml.ir/)\n",
    "\n",
    "__Course Instructor__: Dr. Sharifi-Zarchi\n",
    "\n",
    "__Notebook Authors__: Amir Ezzati & Ali Bavafa\n",
    "\n",
    "Name: Amir Malekhosseini         \n",
    "Student-ID: 401100528"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3D-dKQK9RSV"
   },
   "source": [
    "# Import & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi1fqVs9jOfx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gdown\n",
    "from sklearn.datasets import fetch_openml\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xc-dSl8s6Jz",
    "outputId": "1bc26c20-f3cb-4629-e184-24bbf1d8889a"
   },
   "outputs": [],
   "source": [
    "def seed_setter(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def train_test_classification(net, criterion, optimizer, train_loader,\n",
    "                              test_loader, num_epochs=1, verbose=True,\n",
    "                              training_plot=False):\n",
    "  \"\"\"\n",
    "  Accumulate training loss/Evaluate performance\n",
    "\n",
    "  Args:\n",
    "    net: Instance of Net class\n",
    "      Describes the model with ReLU activation, batch size 128\n",
    "    criterion: torch.nn type\n",
    "      Criterion combines LogSoftmax and NLLLoss in one single class.\n",
    "    optimizer: torch.optim type\n",
    "      Implements Adam algorithm.\n",
    "    train_loader: torch.utils.data type\n",
    "      Combines the train dataset and sampler, and provides an iterable over the given dataset.\n",
    "    test_loader: torch.utils.data type\n",
    "      Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
    "    num_epochs: int\n",
    "      Number of epochs [default: 1]\n",
    "    verbose: boolean\n",
    "      If True, print statistics\n",
    "    training_plot=False\n",
    "      If True, display training plot\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  net.train()\n",
    "  train_losses = []\n",
    "  for epoch in tqdm(range(num_epochs)):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "      # Get the inputs; data is a list of [inputs, labels]\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.float()\n",
    "      labels = labels.long()\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      outputs = net(inputs)\n",
    "\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if verbose:\n",
    "        train_losses += [loss.item()]\n",
    "\n",
    "  net.eval()\n",
    "\n",
    "  def test(data_loader):\n",
    "    \"\"\"\n",
    "    Function to gauge network performance\n",
    "\n",
    "    Args:\n",
    "      data_loader: torch.utils.data type\n",
    "        Combines the test dataset and sampler, and provides an iterable over the given dataset.\n",
    "\n",
    "    Returns:\n",
    "      acc: float\n",
    "        Performance of the network\n",
    "      total: int\n",
    "        Number of datapoints in the dataloader\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in data_loader:\n",
    "      inputs, labels = data\n",
    "      inputs = inputs.float()\n",
    "      labels = labels.long()\n",
    "\n",
    "      outputs = net(inputs)\n",
    "      _, predicted = torch.max(outputs, 1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    return total, acc\n",
    "\n",
    "  train_total, train_acc = test(train_loader)\n",
    "  test_total, test_acc = test(test_loader)\n",
    "\n",
    "  if verbose:\n",
    "    print(f'\\nAccuracy on the {train_total} training samples: {train_acc:0.2f}')\n",
    "    print(f'Accuracy on the {test_total} testing samples: {test_acc:0.2f}\\n')\n",
    "\n",
    "  if training_plot:\n",
    "    plt.plot(train_losses)\n",
    "    plt.xlabel('Batch')\n",
    "    plt.ylabel('Training loss')\n",
    "    plt.show()\n",
    "\n",
    "  return train_acc, test_acc\n",
    "\n",
    "\n",
    "def shuffle_and_split_data(X, y, seed):\n",
    "  \"\"\"\n",
    "  Helper function to shuffle and split data\n",
    "\n",
    "  Args:\n",
    "    X: torch.tensor\n",
    "      Input data\n",
    "    y: torch.tensor\n",
    "      Corresponding target variables\n",
    "    seed: int\n",
    "      Set seed for reproducibility\n",
    "\n",
    "  Returns:\n",
    "    X_test: torch.tensor\n",
    "      Test data [20% of X]\n",
    "    y_test: torch.tensor\n",
    "      Labels corresponding to above mentioned test data\n",
    "    X_train: torch.tensor\n",
    "      Train data [80% of X]\n",
    "    y_train: torch.tensor\n",
    "      Labels corresponding to above mentioned train data\n",
    "  \"\"\"\n",
    "  # Set seed for reproducibility\n",
    "  torch.manual_seed(seed)\n",
    "  # Number of samples\n",
    "  N = X.shape[0]\n",
    "  # Shuffle data\n",
    "  shuffled_indices = torch.randperm(N)\n",
    "  X = X[shuffled_indices]\n",
    "  y = y[shuffled_indices]\n",
    "\n",
    "  test_size = int(0.2 * N)\n",
    "  X_test = X[:test_size]\n",
    "  y_test = y[:test_size]\n",
    "  X_train = X[test_size:]\n",
    "  y_train = y[test_size:]\n",
    "\n",
    "  return X_test, y_test, X_train, y_train\n",
    "\n",
    "\n",
    "def sample_grid(M=500, x_max=2.0):\n",
    "  \"\"\"\n",
    "  Helper function to simulate sample meshgrid\n",
    "\n",
    "  Args:\n",
    "    M: int\n",
    "      Size of the constructed tensor with meshgrid\n",
    "    x_max: float\n",
    "      Defines range for the set of points\n",
    "\n",
    "  Returns:\n",
    "    X_all: torch.tensor\n",
    "      Concatenated meshgrid tensor\n",
    "  \"\"\"\n",
    "  ii, jj = torch.meshgrid(torch.linspace(-x_max, x_max,M),\n",
    "                          torch.linspace(-x_max, x_max, M),\n",
    "                          indexing='ij')\n",
    "  X_all = torch.cat([ii.unsqueeze(-1),\n",
    "                     jj.unsqueeze(-1)],\n",
    "                     dim=-1).view(-1, 2)\n",
    "  return X_all\n",
    "\n",
    "\n",
    "def plot_decision_map(X_all, y_pred, X_test, y_test,\n",
    "                      M=500, x_max=2.0, eps=1e-3):\n",
    "  \"\"\"\n",
    "  Helper function to plot decision map\n",
    "\n",
    "  Args:\n",
    "    X_all: torch.tensor\n",
    "      Concatenated meshgrid tensor\n",
    "    y_pred: torch.tensor\n",
    "      Labels predicted by the network\n",
    "    X_test: torch.tensor\n",
    "      Test data\n",
    "    y_test: torch.tensor\n",
    "      Labels of the test data\n",
    "    M: int\n",
    "      Size of the constructed tensor with meshgrid\n",
    "    x_max: float\n",
    "      Defines range for the set of points\n",
    "    eps: float\n",
    "      Decision threshold\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  decision_map = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "  for i in range(len(X_test)):\n",
    "    indeces = (X_all[:, 0] - X_test[i, 0])**2 + (X_all[:, 1] - X_test[i, 1])**2 < eps\n",
    "    decision_map[indeces] = (K + y_test[i]).long()\n",
    "\n",
    "  decision_map = decision_map.view(M, M).cpu()\n",
    "  plt.imshow(decision_map, extent=[-x_max, x_max, -x_max, x_max], cmap='jet')\n",
    "  plt.axis('off')\n",
    "  plt.plot()\n",
    "\n",
    "\n",
    "def plot_function_apx(x_vals, relu_activations, predicted_output):\n",
    "    \"\"\"\n",
    "    Visualizes ReLU activations and the resulting function approximation.\n",
    "\n",
    "    Args:\n",
    "      x_vals: torch.tensor\n",
    "        Input x-axis data points.\n",
    "      relu_activations: torch.tensor\n",
    "        Calculated ReLU activations along x-axis for each ReLU unit.\n",
    "      predicted_output: torch.tensor\n",
    "        Estimated output labels or function approximations (weighted sum of ReLUs).\n",
    "\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(8, 6))\n",
    "\n",
    "    ## Plot ReLU activations\n",
    "    axes[0].plot(x_vals, relu_activations.T)\n",
    "    axes[0].set_xlabel('x-values', fontsize=12)\n",
    "    axes[0].set_ylabel('Activation Levels', fontsize=12)\n",
    "    axes[0].set_title('ReLU Activations (Basis Functions)', fontsize=14)\n",
    "    relu_labels = [f\"ReLU {i + 1}\" for i in range(relu_activations.shape[0])]\n",
    "    axes[0].legend(relu_labels, ncol=2)\n",
    "\n",
    "    ## Plot the function approximation and the ground truth\n",
    "    axes[1].plot(x_vals, torch.sin(x_vals), label='Ground Truth (sin(x))', color='g', linewidth=2)\n",
    "    axes[1].plot(x_vals, predicted_output, label='Predicted Output', linestyle='--', color='r')\n",
    "    axes[1].set_xlabel('x-values', fontsize=12)\n",
    "    axes[1].set_ylabel('y(x)', fontsize=12)\n",
    "    axes[1].set_title('Function Approximation vs Ground Truth', fontsize=14)\n",
    "    axes[1].legend()\n",
    "\n",
    "    plt.tight_layout(pad=2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_images(image, num_row=2, num_col=5):\n",
    "    # plot images\n",
    "    image_size = int(np.sqrt(image.shape[-1]))\n",
    "    image = np.reshape(image, (image.shape[0], image_size, image_size))\n",
    "    fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "    for i in range(num_row*num_col):\n",
    "        ax = axes[i//num_col, i%num_col]\n",
    "        ax.imshow(image[i], cmap='gray', vmin=0, vmax=1)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "SEED = 2024\n",
    "# Call `seed_setter` to ensure reproducibility.\n",
    "seed_setter(seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko8kskcC9PVC"
   },
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Psmj-hddxrHV",
    "outputId": "abf8cabb-4ec8-4c37-d8b3-41424bda4b58"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "mnist_data = fetch_openml(\"mnist_784\")\n",
    "x = mnist_data[\"data\"].astype('float').to_numpy()\n",
    "y = mnist_data[\"target\"].astype('int')\n",
    "\n",
    "# Normalize\n",
    "x /= 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "num_labels = len(np.unique(y))\n",
    "y_new = to_categorical(y, num_labels)\n",
    "\n",
    "# -------------------------------- TODO -------------------------------------\n",
    "# In this section, split the data into training and test datasets, using 60,000 samples for training and the remaining samples for testing.\n",
    "train_size = 60000\n",
    "\n",
    "# Split the data into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y_new, train_size=train_size, random_state=14\n",
    ")\n",
    "# -------------------------------- TODO -------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "pG5s84eyyeh8",
    "outputId": "c62f9d7b-8e22-4808-b161-bbd51d52aeda"
   },
   "outputs": [],
   "source": [
    "print(\"Training data: {} {}\".format(x_train.shape, y_train.shape))\n",
    "print(\"Test data: {} {}\".format(x_test.shape, y_test.shape))\n",
    "show_images(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0yB4KcT9ffj"
   },
   "source": [
    "# Activation Functions & Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFGJmu06sAAh"
   },
   "source": [
    "There are many activation functions which some of them listed below. In this section you should implement them.\n",
    "\n",
    "1. Sigmoid:\n",
    "$$\n",
    "\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "2. Softmax: $$\\operatorname{softmax}(\\mathbf X)_{ij} = \\frac{\\exp(\\mathbf X_{ij})}{\\sum_k \\exp \\mathbf X_{ik})}.$$\n",
    "\n",
    "\n",
    "3. Tanh (Hyperbolic Tangent):\n",
    "$$\n",
    "\\operatorname{tanh}(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{\\exp(x) - \\exp(-x)}{\\exp(x) + \\exp(-x)}\n",
    "$$\n",
    "\n",
    "4. ReLU: $$\\operatorname{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "5. Leaky ReLU:\n",
    "$$\n",
    "\\operatorname{LeakyReLU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x \\geq 0 \\\\\n",
    "\\alpha x & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where \\( $\\alpha$ \\) is a small constant (e.g., 0.01).\n",
    "\n",
    "6. ELU (Exponential Linear Unit):\n",
    "$$\n",
    "\\operatorname{ELU}(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where ($\\alpha$) is a positive constant.\n",
    "\n",
    "7. SELU (Scaled Exponential Linear Unit):\n",
    "$$\n",
    "\\operatorname{SELU}(x) =\n",
    "\\lambda\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\\\\n",
    "\\alpha (\\exp(x) - 1) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "where \\( $\\lambda$ \\) and \\( $\\alpha$ \\) are predefined constants, typically \\( $\\lambda \\approx 1.0507$ \\) and \\( $\\alpha \\approx 1.67326$ \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_cQudDfrG7D"
   },
   "source": [
    "------\n",
    "There are several optimizer algorithms for optimizing the neural network over epochs. You should implement some of those mentioned in this section:\n",
    "\n",
    "***Stochastic Gradient Descent (SGD)***\n",
    "\n",
    "SGD is the basic form of gradient descent, where each weight (\\( w \\)) and bias (\\( b \\)) parameter is updated in the opposite direction of its gradient.\n",
    "\n",
    "**Update Rule:**\n",
    "$$\n",
    "w^{(t+1)} = w^{(t)} - \\eta \\cdot \\nabla_w L(w^{(t)}, b^{(t)})\n",
    "$$\n",
    "$$\n",
    "b^{(t+1)} = b^{(t)} - \\eta \\cdot \\nabla_b L(w^{(t)}, b^{(t)})\n",
    "$$\n",
    "\n",
    "- \\( w \\): Weight parameters\n",
    "- \\( b \\): Bias parameters\n",
    "- \\( $\\eta$ \\): Learning rate\n",
    "- \\( $\\nabla_w L(w, b)$ \\): Gradient of the loss with respect to \\( w \\)\n",
    "- \\( $\\nabla_b L(w, b)$ \\): Gradient of the loss with respect to \\( b \\)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***Momentum Optimizer***\n",
    "\n",
    "Momentum optimizer smooths parameter updates by adding a velocity term to accumulate past gradients.\n",
    "\n",
    "**Update Rule:**\n",
    "1. **Velocity Update for Weights and Biases:**\n",
    "   $$\n",
    "   v_w^{(t+1)} = \\beta \\cdot v_w^{(t)} + (1 - \\beta) \\cdot \\nabla_w L(w^{(t)}, b^{(t)})\n",
    "   $$\n",
    "   $$\n",
    "   v_b^{(t+1)} = \\beta \\cdot v_b^{(t)} + (1 - \\beta) \\cdot \\nabla_b L(w^{(t)}, b^{(t)})\n",
    "   $$\n",
    "\n",
    "2. **Parameter Update:**\n",
    "   $$\n",
    "   w^{(t+1)} = w^{(t)} - \\eta \\cdot v_w^{(t+1)}\n",
    "   $$\n",
    "   $$\n",
    "   b^{(t+1)} = b^{(t)} - \\eta \\cdot v_b^{(t+1)}\n",
    "   $$\n",
    "\n",
    "- \\( $v_w$ \\): Velocity term for weights\n",
    "- \\( $v_b$ \\): Velocity term for biases\n",
    "- \\( $\\beta$ \\): Momentum term, typically set to 0.9\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***Adam Optimizer***\n",
    "\n",
    "Adam combines momentum and adaptive learning rates, using both first (momentum) and second (RMSprop) moment estimates.\n",
    "\n",
    "**Update Rule:**\n",
    "1. **Compute biased first and second moment estimates:**\n",
    "   $$\n",
    "   m_w^{(t+1)} = \\beta_1 \\cdot m_w^{(t)} + (1 - \\beta_1) \\cdot \\nabla_w L(w^{(t)}, b^{(t)})\n",
    "   $$\n",
    "   $$\n",
    "   m_b^{(t+1)} = \\beta_1 \\cdot m_b^{(t)} + (1 - \\beta_1) \\cdot \\nabla_b L(w^{(t)}, b^{(t)})\n",
    "   $$\n",
    "   $$\n",
    "   v_w^{(t+1)} = \\beta_2 \\cdot v_w^{(t)} + (1 - \\beta_2) \\cdot \\left( \\nabla_w L(w^{(t)}, b^{(t)}) \\right)^2\n",
    "   $$\n",
    "   $$\n",
    "   v_b^{(t+1)} = \\beta_2 \\cdot v_b^{(t)} + (1 - \\beta_2) \\cdot \\left( \\nabla_b L(w^{(t)}, b^{(t)}) \\right)^2\n",
    "   $$\n",
    "\n",
    "2. **Compute bias-corrected estimates:**\n",
    "   $$\n",
    "   \\hat{m}_w^{(t+1)} = \\frac{m_w^{(t+1)}}{1 - \\beta_1^{t+1}}, \\quad \\hat{m}_b^{(t+1)} = \\frac{m_b^{(t+1)}}{1 - \\beta_1^{t+1}}\n",
    "   $$\n",
    "   $$\n",
    "   \\hat{v}_w^{(t+1)} = \\frac{v_w^{(t+1)}}{1 - \\beta_2^{t+1}}, \\quad \\hat{v}_b^{(t+1)} = \\frac{v_b^{(t+1)}}{1 - \\beta_2^{t+1}}\n",
    "   $$\n",
    "\n",
    "3. **Parameter Update:**\n",
    "   $$\n",
    "   w^{(t+1)} = w^{(t)} - \\eta \\cdot \\frac{\\hat{m}_w^{(t+1)}}{\\sqrt{\\hat{v}_w^{(t+1)}} + \\epsilon}\n",
    "   $$\n",
    "   $$\n",
    "   b^{(t+1)} = b^{(t)} - \\eta \\cdot \\frac{\\hat{m}_b^{(t+1)}}{\\sqrt{\\hat{v}_b^{(t+1)}} + \\epsilon}\n",
    "   $$\n",
    "\n",
    "- \\( $m_w$ \\), \\( $m_b$ \\): First moment estimates (mean of gradients) for weights and biases\n",
    "- \\( $v_w$ \\), \\( $v_b$ \\): Second moment estimates (uncentered variance of gradients) for weights and biases\n",
    "- \\( $\\beta_1$ \\): Decay rate for first moment, typically 0.9\n",
    "- \\( $\\beta_2$ \\): Decay rate for second moment, typically 0.999\n",
    "- \\( $\\epsilon$ \\): Small constant for numerical stability (e.g., \\(10^{-8}\\))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83ucvivl9vec"
   },
   "source": [
    "# MLP from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FGfYG_DfJL9b"
   },
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, sizes, activation='sigmoid'):\n",
    "        self.sizes = sizes\n",
    "        self.num_layers = len(sizes) - 1  # number of layers excluding input\n",
    "        self.activation_name = activation\n",
    "        self.momemtum_opt={}\n",
    "        self.adam_opt={}\n",
    "\n",
    "        # Choose activation function\n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = self.sigmoid\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = self.tanh\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "        # Do the same for other activation functions\n",
    "        elif activation == 'elu':\n",
    "            self.activation = self.elu\n",
    "        elif activation == 'selu':\n",
    "            self.activation = self.selu\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = self.softmax\n",
    "        elif activation == 'leakyrelu':\n",
    "            self.activation = self.leaky_relu\n",
    "\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"This activation function is currently not supported!\")\n",
    "\n",
    "        # Save all weights\n",
    "        self.params = self.initialize()\n",
    "        # Save all intermediate values, i.e. activations\n",
    "        self.cache = {}\n",
    "\n",
    "    # Activation functions remain the same as defined in your original code\n",
    "    def relu(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            relu(x) = max(0, x)\n",
    "            In other word,\n",
    "            relu(x) = 0, if x < 0\n",
    "                    = x, if x >= 0\n",
    "\n",
    "            Backward path:\n",
    "            ∇relu(x) = 0, if x < 0\n",
    "                     = 1, if x >=0\n",
    "        '''\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        if derivative:\n",
    "            return np.where(x > 0, 1, 0)\n",
    "        else:\n",
    "            return np.maximum(0, x)\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def tanh(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "            Backward path:\n",
    "            ∇tanh(x) = 1 - tanh(x)^2\n",
    "        '''\n",
    "        tanh_out=np.tanh(x)\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        if derivative:\n",
    "            return 1 - tanh_out**2\n",
    "        else:\n",
    "            return tanh_out\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            σ(x) = 1 / 1+exp(-z)\n",
    "\n",
    "            Backward path:\n",
    "            ∇σ(x) = exp(-z) / (1+exp(-z))^2\n",
    "        '''\n",
    "        sigmoid_out = 1 / (1 + np.exp(-x))\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        if derivative:\n",
    "            return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "        else:\n",
    "            return sigmoid_out\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def softmax(self, x):\n",
    "        '''\n",
    "            softmax(x) = exp(x) / ∑exp(x)\n",
    "        '''\n",
    "        # Numerically stable with large exponentials\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / exp_x.sum(axis=0, keepdims=True)\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def leaky_relu(self, x, alpha=0.01, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            leaky_relu(x) = x if x >= 0 else αx\n",
    "\n",
    "            Backward path:\n",
    "            ∇leaky_relu(x) = 1 if x >= 0 else α\n",
    "        '''\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        if derivative:\n",
    "            return np.where(x >= 0, 1, alpha)\n",
    "        else:\n",
    "            return np.where(x >= 0, x, alpha*x)\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def elu(self, x, alpha=1.0, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            elu(x) = x if x > 0 else α(exp(x) - 1)\n",
    "\n",
    "            Backward path:\n",
    "            ∇elu(x) = 1 if x > 0 else α * exp(x)\n",
    "        '''\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        if derivative:\n",
    "            return np.where(x > 0, 1, alpha*np.exp(x))\n",
    "        else:\n",
    "            return np.where(x > 0, x, alpha*(np.exp(x) - 1))\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def selu(self, x, lambda_=1.0507, alpha=1.67326, derivative=False):\n",
    "        '''\n",
    "            Forward path:\n",
    "            selu(x) = λ * (x if x > 0 else α * (exp(x) - 1))\n",
    "\n",
    "            Backward path:\n",
    "            ∇selu(x) = λ * (1 if x > 0 else α * exp(x))\n",
    "        '''\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        if derivative:\n",
    "            return lambda_ * np.where(x > 0, 1, alpha*np.exp(x))\n",
    "        else:\n",
    "            return lambda_ * np.where(x > 0, x, alpha*(np.exp(x) - 1))\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def initialize(self):\n",
    "        params = {}\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            input_layer = self.sizes[i - 1]\n",
    "            output_layer = self.sizes[i]\n",
    "            params[f\"W{i}\"] = np.random.randn(\n",
    "                output_layer, input_layer) * np.sqrt(1. / input_layer)\n",
    "            params[f\"b{i}\"] = np.zeros((output_layer, 1))\n",
    "        return params\n",
    "\n",
    "    def feed_forward(self, x):\n",
    "        self.cache[\"A0\"] = x.T\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        # Perform feedforward pass through each hidden layer, applying weights, biases, and activation function\n",
    "\n",
    "        for l in range(1, self.num_layers + 1):\n",
    "            \n",
    "          Z = self.params[f\"W{l}\"] @ self.cache[f\"A{l-1}\"] + \\\n",
    "              self.params[f\"b{l}\"]\n",
    "          self.cache[f\"Z{l}\"] = Z\n",
    "\n",
    "          if l == self.num_layers:\n",
    "            self.cache[f\"A{l}\"] = self.softmax(Z)\n",
    "          else:\n",
    "            self.cache[f\"A{l}\"] = self.activation(Z)\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "        return self.cache[f\"A{self.num_layers}\"]\n",
    "\n",
    "    def back_propagate(self, y, output):\n",
    "        current_batch_size = y.shape[0]\n",
    "        grads = {}\n",
    "\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        # Backpropagate gradients through Output layer and each hidden layers\n",
    "        \n",
    "        dA = -(y.T - output)\n",
    "\n",
    "        for l in range(self.num_layers, 0, -1):\n",
    "          Z = self.cache[f\"Z{l}\"]\n",
    "          if self.activation_name == 'softmax' and l == self.num_layers:\n",
    "            dZ = dA\n",
    "          else:\n",
    "            dZ = dA * self.activation(Z, derivative=True)\n",
    "\n",
    "          dW = (1 / current_batch_size) * (dZ @ self.cache[f\"A{l-1}\"].T)\n",
    "          db = (1 / current_batch_size) * np.sum(dZ, axis=1, keepdims=True)\n",
    "\n",
    "          grads[f\"dW{l}\"] = dW\n",
    "          grads[f\"db{l}\"] = db\n",
    "\n",
    "          if l > 1:\n",
    "            dA = self.params[f\"W{l}\"].T @ dZ\n",
    "            \n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "        self.grads = grads\n",
    "        return self.grads\n",
    "\n",
    "    def cross_entropy_loss(self, y, output):\n",
    "        '''\n",
    "            L(y, ŷ) = −∑ylog(ŷ).\n",
    "        '''\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        # Compute cross-entropy loss as the negative average log-likelihood of the correct labels\n",
    "        \n",
    "        output = np.clip(output, 1e-12, 1.0)\n",
    "        L = -np.sum(y * np.log(output).T) / y.shape[0]\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        return L\n",
    "\n",
    "    def initialize_momemtum_optimizer(self):\n",
    "        momentum_opt = {}\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            # -------------------------------- TODO -------------------------------------\n",
    "            # Initialize zero matrices for momentum updates of weights and biases in each layer\n",
    "            \n",
    "            momentum_opt[f\"dW{i}\"] = np.zeros_like(self.params[f\"W{i}\"])\n",
    "            momentum_opt[f\"db{i}\"] = np.zeros_like(self.params[f\"b{i}\"])\n",
    "            \n",
    "            # -------------------------------- TODO -------------------------------------\n",
    "        return momentum_opt\n",
    "\n",
    "    def initialize_adam_optimizer(self):\n",
    "        adam_opt = {}\n",
    "        for i in range(1, self.num_layers + 1):\n",
    "            \n",
    "            adam_opt[f\"mW{i}\"] = np.zeros_like(self.params[f\"W{i}\"])\n",
    "            adam_opt[f\"mb{i}\"] = np.zeros_like(self.params[f\"b{i}\"])\n",
    "        \n",
    "            adam_opt[f\"vW{i}\"] = np.zeros_like(self.params[f\"W{i}\"])\n",
    "            adam_opt[f\"vb{i}\"] = np.zeros_like(self.params[f\"b{i}\"])\n",
    "            \n",
    "        return adam_opt\n",
    "\n",
    "\n",
    "    def optimize(self, l_rate=0.1, beta=.9, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for key in self.params:\n",
    "                # -------------------------------- TODO -------------------------------------\n",
    "                # Update rule for SGD\n",
    "                \n",
    "                self.params[key] -= l_rate * self.grads[f\"d{key}\"]\n",
    "                \n",
    "                # -------------------------------- TODO -------------------------------------\n",
    "        elif self.optimizer == \"momemtum\":\n",
    "            for key in self.params:\n",
    "                # -------------------------------- TODO -------------------------------------\n",
    "                # Update rule for Momentum\n",
    "                \n",
    "                self.momemtum_opt[f\"d{key}\"] = beta * \\\n",
    "                    self.momemtum_opt[f\"d{key}\"] + \\\n",
    "                    (1 - beta) * self.grads[f\"d{key}\"]\n",
    "                self.params[key] -= l_rate * self.momemtum_opt[f\"d{key}\"]\n",
    "                \n",
    "                # -------------------------------- TODO -------------------------------------\n",
    "        elif self.optimizer == \"adam\":\n",
    "            for key in self.params:\n",
    "                \n",
    "                self.adam_opt[f\"m{key}\"] = beta1 * self.adam_opt[f\"m{key}\"] + (1 - beta1) * self.grads[f\"d{key}\"]\n",
    "                self.adam_opt[f\"v{key}\"] = beta2 * self.adam_opt[f\"v{key}\"] + (1 - beta2) * (self.grads[f\"d{key}\"] ** 2)\n",
    "\n",
    "                m_hat = self.adam_opt[f\"m{key}\"] / (1 - beta1 ** t)\n",
    "                v_hat = self.adam_opt[f\"v{key}\"] / (1 - beta2 ** t)\n",
    "\n",
    "                self.params[key] -= l_rate * (m_hat / (np.sqrt(v_hat) + epsilon))\n",
    "\n",
    "                # -------------------------------- TODO -------------------------------------\n",
    "        else:\n",
    "            raise ValueError(\"This optimizer is not supported!\")\n",
    "\n",
    "    def accuracy(self, y, output):\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "        # Calculate accuracy by comparing the pred and truth matrices\n",
    "        \n",
    "        accuracy = np.mean(np.argmax(y, axis=1) == np.argmax(output, axis=0))\n",
    "        return accuracy\n",
    "        # -------------------------------- TODO -------------------------------------\n",
    "\n",
    "    def train(self, x_train, y_train, x_val, y_val, epochs=10,\n",
    "              batch_size=64, optimizer='momemtum', l_rate=0.1, beta=.9, beta1=0.9, beta2=0.999, epsilon=1e-8, t=1):\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        num_batches = -(-x_train.shape[0] // self.batch_size)\n",
    "\n",
    "        # Initialize optimizer\n",
    "        self.optimizer = optimizer\n",
    "        if self.optimizer == 'momemtum':\n",
    "          self.momemtum_opt = self.initialize_momemtum_optimizer()\n",
    "        elif self.optimizer == 'adam':\n",
    "          self.adam_opt = self.initialize_adam_optimizer()\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train\n",
    "        for i in range(self.epochs):\n",
    "            # Shuffle\n",
    "            permutation = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuffled = x_train[permutation]\n",
    "            y_train_shuffled = y_train[permutation]\n",
    "\n",
    "            for j in range(num_batches):\n",
    "                # Batch\n",
    "                begin = j * self.batch_size\n",
    "                end = min(begin + self.batch_size, x_train.shape[0]-1)\n",
    "                x = x_train_shuffled[begin:end]\n",
    "                y = y_train_shuffled[begin:end]\n",
    "\n",
    "                # Forward\n",
    "                output = self.feed_forward(x)\n",
    "                # Backprop\n",
    "                grad = self.back_propagate(y, output)\n",
    "                # Optimize\n",
    "                self.optimize(l_rate=l_rate, beta=beta, beta1=beta1,\n",
    "                              beta2=beta2, epsilon=epsilon, t=t)\n",
    "\n",
    "            # Evaluate performance\n",
    "            # Training data\n",
    "            output = self.feed_forward(x_train)\n",
    "            train_acc = self.accuracy(y_train, output)\n",
    "            train_loss = self.cross_entropy_loss(y_train, output)\n",
    "            # Test data\n",
    "            output = self.feed_forward(x_val)\n",
    "            val_acc = self.accuracy(y_val, output)\n",
    "            val_loss = self.cross_entropy_loss(y_val, output)\n",
    "            print(f'Epoch {i+1}: {time.time()-start_time:.2f}s, train acc={train_acc:.2f}, train loss={train_loss:.2f}, '\n",
    "                  f'validation acc={val_acc:.2f}, validation loss={val_loss:.2f}')\n",
    "\n",
    "    def evaluate(self, x_test, y_test):\n",
    "        # Test data\n",
    "        output = self.feed_forward(x_test)\n",
    "        test_acc = self.accuracy(y_test, output)\n",
    "        test_loss = self.cross_entropy_loss(y_test, output)\n",
    "        print(f'\\nTest acc={test_acc:.2f}, Test loss={test_loss:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnSR8IkArgeT"
   },
   "source": [
    "Please run experiments with different combinations of optimizers (SGD, Momentum, and Adam) and activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU, SELU, and ELU) over 20 epochs each. Consider networks with three layers (without input layer) whose number of neurons is 128, 64 and 10 respectively. Also consider two learning rates 0.01 and 0.001.\n",
    "\n",
    "\n",
    "After testing all combinations, report which combination of optimizer, activation function, and learning rate performs best.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zAksBBzrigw"
   },
   "outputs": [],
   "source": [
    "# Run your experiments here ...\n",
    "\n",
    "optimizers = ['momemtum', 'sgd', 'adam']\n",
    "activations = ['elu', 'sigmoid', 'tanh', 'leakyrelu', 'selu', 'relu']\n",
    "learning_rates = [0.01, 0.001]\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "layer_sizes = [784,128, 64, 10]\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for optimizer in optimizers:\n",
    "    for activation in activations:\n",
    "        for lr in learning_rates:\n",
    "            print(f\"Testing {optimizer} + {activation} + lr={lr}\")\n",
    "\n",
    "            model = MLP(sizes=layer_sizes, activation=activation)\n",
    "\n",
    "            model.train(x_train, y_train, x_test, y_test,\n",
    "                        epochs=epochs, batch_size=batch_size,\n",
    "                        optimizer=optimizer, l_rate=lr)\n",
    "\n",
    "            output_value = model.feed_forward(x_test)\n",
    "            value_accuracy = model.accuracy(y_test, output_value)\n",
    "            value_loss = model.cross_entropy_loss(y_test, output_value)\n",
    "\n",
    "            # Save results\n",
    "            results_list.append((optimizer, activation, lr, value_accuracy, value_loss))\n",
    "            print(\n",
    "                f\"Validation accuracy={value_accuracy:.2f}, Validation loss={value_loss:.2f}\\n\\n\")\n",
    "\n",
    "# Find the best configuration\n",
    "best_config = max(results_list, key=lambda x: x[3])\n",
    "print(\"\\nBest configuration:\")\n",
    "print(\n",
    "    f\"Optimizer: {best_config[0]}, Activation: {best_config[1]}, Learning Rate: {best_config[2]}\")\n",
    "print(\n",
    "    f\"Validation Accuracy: {best_config[3]:.2f}, Validation Loss: {best_config[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABnVeI2Gx2nk"
   },
   "source": [
    "# MLP as function Approximator\n",
    "We know that a MLP with one hidden layer could  approximate any smooth function! <br> Here you will manually fit a sine function using ReLU activation. <br>You need to set the correct weights on ReLUs so the linear combination approximates the desired function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "1F__DcOEz2y1",
    "outputId": "c9ed07ce-80fd-4542-d455-045500da6522"
   },
   "outputs": [],
   "source": [
    "def relu_approximation(input_data, target_labels):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      input_data: torch.tensor\n",
    "        Input training data\n",
    "      target_labels: torch.tensor\n",
    "        Ground truth labels for training data\n",
    "\n",
    "    Returns:\n",
    "      activation_map: torch.tensor\n",
    "        ReLU activations computed across the x-axis\n",
    "      predicted_labels: torch.tensor\n",
    "        Estimated labels or class predictions\n",
    "        Derived from weighted sum of ReLU activations along the x-axis\n",
    "      x_points: torch.tensor\n",
    "        Points along the x-axis\n",
    "    \"\"\"\n",
    "    ##############################---TODO---##############################\n",
    "    # Number of ReLU units\n",
    "    num_relus = input_data.shape[0] - 1\n",
    "\n",
    "    # Generate dense x-axis points for interpolation\n",
    "    x_points = torch.linspace(torch.min(input_data),\n",
    "                              torch.max(input_data), 1000)\n",
    "\n",
    "    # Compute bias for ReLU units\n",
    "    # Biases are set to input values except the last point\n",
    "    biases = input_data[:-1].view(-1)\n",
    "\n",
    "    # Initialize activation map to store ReLU activations\n",
    "    activation_map = torch.zeros((num_relus, x_points.shape[0]))\n",
    "\n",
    "    # Compute ReLU activations\n",
    "    for relu_idx in range(num_relus):\n",
    "        activation_map[relu_idx, :] = torch.relu(x_points - biases[relu_idx])\n",
    "\n",
    "    # WEIGHTED SUM OF RELUS\n",
    "    relu_weights = torch.zeros(num_relus)\n",
    "    previous_slope = 0\n",
    "    for j in range(num_relus):\n",
    "        # Derive slope of each segment\n",
    "        next_slope = (target_labels[j + 1] - target_labels[j]\n",
    "                      ) / (input_data[j + 1] - input_data[j])\n",
    "        relu_weights[j] = next_slope - previous_slope\n",
    "        previous_slope = next_slope\n",
    "\n",
    "    # Compute predicted labels as weighted sum of ReLU activations\n",
    "    predicted_labels = relu_weights @ activation_map\n",
    "    \n",
    "    ##############################---TODO---##############################\n",
    "\n",
    "    return predicted_labels, activation_map, x_points\n",
    "\n",
    "\n",
    "# Generate synthetic training data using sine function\n",
    "num_samples = 10\n",
    "input_data = torch.linspace(0, 2 * np.pi, num_samples).view(-1, 1)\n",
    "target_labels = torch.sin(input_data)\n",
    "\n",
    "predicted_output, activation_map, x_points = relu_approximation(input_data, target_labels)\n",
    "plot_function_apx(x_points, activation_map, predicted_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPk-_Xll83dA"
   },
   "source": [
    "# Depth vs. Width\n",
    "Here we wanna see the effect of width vs. depth on a classification task.First you are going to implement MLP again; But with 2 differences: <br>\n",
    "\n",
    "1.   It is general purpose (i.e. works for desired depth and activation functions)\n",
    "2.   You will implement it using pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjigdUhZw_em"
   },
   "outputs": [],
   "source": [
    "class MLP_pytorch(nn.Module):\n",
    "  \"\"\"\n",
    "  Simulate MLP Network\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, activation_fn, input_feature_num, hidden_unit_nums, output_feature_num):\n",
    "    \"\"\"\n",
    "    Initialize MLP Network parameters\n",
    "\n",
    "    Args:\n",
    "      activation_fn: string\n",
    "        Activation function\n",
    "      input_feature_num: int\n",
    "        Number of input features\n",
    "      hidden_unit_nums: list\n",
    "        Number of units per hidden layer. List of integers\n",
    "      output_feature_num: int\n",
    "        Number of output features\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(MLP_pytorch, self).__init__()\n",
    "    self.input_feature_num = input_feature_num\n",
    "    self.mlp = nn.Sequential()\n",
    "    \n",
    "    if activation_fn == 'ReLU':\n",
    "        self.activation_fn = nn.ReLU()\n",
    "    elif activation_fn == 'Sigmoid':\n",
    "        self.activation_fn = nn.Sigmoid()\n",
    "    elif activation_fn == 'Tanh':\n",
    "        self.activation_fn = nn.Tanh()\n",
    "    elif activation_fn == 'LeakyReLU':\n",
    "        self.activation_fn = nn.LeakyReLU()\n",
    "    elif activation_fn == 'ELU':\n",
    "        self.activation_fn = nn.ELU()\n",
    "\n",
    "    in_num = input_feature_num\n",
    "    ##############################---TODO---##############################\n",
    "    for i in range(len(hidden_unit_nums)):\n",
    "        hidden_layer = nn.Linear(in_num, hidden_unit_nums[i])\n",
    "        self.mlp.add_module(f'Hidden_Layer_{i}', hidden_layer)\n",
    "        self.mlp.add_module(f'Hidden_{i+1}_Activation', self.activation_fn)\n",
    "\n",
    "        in_num = hidden_unit_nums[i]  # Update in_num for next layer\n",
    "    out_layer = nn.Linear(in_num, output_feature_num) # final layer\n",
    "    self.mlp.add_module('Output_Linear', out_layer)\n",
    "\n",
    "    ##############################---TODO---##############################\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Simulate forward pass of MLP Network\n",
    "\n",
    "    Args:\n",
    "      x: torch.tensor\n",
    "        Input data\n",
    "\n",
    "    Returns:\n",
    "      logits: Instance of MLP\n",
    "        Forward pass of MLP\n",
    "    \"\"\"\n",
    "    # Reshape inputs to (batch_size, input_feature_num)\n",
    "    x = x.view(-1, self.input_feature_num)\n",
    "    logits = self.mlp(x) # forward pass\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ali1hMF6vpY5"
   },
   "source": [
    "Now let's make a spiral dataset that follows this formula:\n",
    "\\begin{equation}\n",
    "\\begin{array}{c}\n",
    "X_{k}(t)=t\\left(\\begin{array}{c}\n",
    "\\sin \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma\\right) \\\\\n",
    "\\cos \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma\\right)\n",
    "\\end{array}\\right)\n",
    "\\end{array}, \\quad 0 \\leq t \\leq 1, \\quad k=1, \\ldots, K\n",
    "\\end{equation}\n",
    "\n",
    "Run cell below to create the data and load it as tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-f3enyo551x5"
   },
   "outputs": [],
   "source": [
    "# @title Data Loader\n",
    "K = 4\n",
    "sigma = 0.4\n",
    "N = 1000\n",
    "t = torch.linspace(0, 1, N)\n",
    "X = torch.zeros(K*N, 2)\n",
    "y = torch.zeros(K*N)\n",
    "for k in range(K):\n",
    "  X[k*N:(k+1)*N, 0] = t*(torch.sin(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))\n",
    "  X[k*N:(k+1)*N, 1] = t*(torch.cos(2*np.pi/K*(2*t+k)) + sigma**2*torch.randn(N))\n",
    "  y[k*N:(k+1)*N] = k\n",
    "\n",
    "\n",
    "X_test, y_test, X_train, y_train = shuffle_and_split_data(X, y, seed=SEED)\n",
    "\n",
    "# DataLoader with random seed\n",
    "batch_size = 128\n",
    "g_seed = torch.Generator()\n",
    "g_seed.manual_seed(SEED)\n",
    "\n",
    "test_data = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size,\n",
    "                         shuffle=False, num_workers=0,\n",
    "                         worker_init_fn=seed_worker,\n",
    "                         generator=g_seed,\n",
    "                         )\n",
    "\n",
    "train_data = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          shuffle=True,\n",
    "                          worker_init_fn=seed_worker,\n",
    "                          generator=g_seed,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1REALnz4Jw_"
   },
   "source": [
    "Now we will add more polynomial features to the dataset to make the first layer wider. Afterwards, train a single linear layer. We could use the same MLP network with no hidden layers (though it would not be called an MLP anymore!).\n",
    "\n",
    "Add polynomial terms up to $P=50$ which means that for every $x_1^n x_2^m$ term, $n+m\\leq P$. Total number of polynomial features up to $P$ follows this formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{# of terms} = \\frac{(P+1)(P+2)}{2}\n",
    "\\end{equation}\n",
    "\n",
    "Also, we don't need the polynomial term with degree zero (which is the constatnt term) since `nn.Linear` layers have bias terms. Therefore we will have one fewer polynomial feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtuvmsKf37jg"
   },
   "outputs": [],
   "source": [
    "def polynomial_classifier(poly_degree, seed=0):\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  Helper function to run the polynomial classifier\n",
    "\n",
    "\n",
    "  Args:\n",
    "\n",
    "    poly_degree: int\n",
    "\n",
    "      Degree of the polynomial\n",
    "\n",
    "    seed: int\n",
    "\n",
    "      A non-negative integer that defines the random state.\n",
    "\n",
    "\n",
    "  Returns:\n",
    "\n",
    "    num_features: int\n",
    "\n",
    "      Number of features\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  def polynomial_features(poly_degree, X):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Function to define the number of polynomial features except the bias term\n",
    "\n",
    "\n",
    "    Args:\n",
    "\n",
    "      poly_degree: int\n",
    "\n",
    "        Degree of the polynomial\n",
    "\n",
    "      X: torch.tensor\n",
    "\n",
    "        Input data\n",
    "\n",
    "\n",
    "    Returns:\n",
    "\n",
    "      num_features: int\n",
    "\n",
    "        Number of features\n",
    "\n",
    "      poly_X: torch.tensor\n",
    "\n",
    "        Polynomial term\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ##############################---TODO---##############################\n",
    "\n",
    "    from itertools import combinations_with_replacement\n",
    "        \n",
    "    num_features = (poly_degree + 1) * (poly_degree + 2) // 2 - 1  # Total number of terms minus the constant term\n",
    "    n_samples, n_dims = X.shape\n",
    "        \n",
    "    # Prepare the polynomial feature matrix\n",
    "    poly_X = torch.zeros((n_samples, num_features))\n",
    "    feature_idx = 0\n",
    "\n",
    "    # Iterate through all combinations of degrees\n",
    "    for degree in range(1, poly_degree + 1):  # Skip degree=0 to exclude the constant term\n",
    "        for powers in combinations_with_replacement(range(n_dims), degree):\n",
    "            # Compute the polynomial term\n",
    "            term = torch.prod(X[:, powers], dim=1)\n",
    "            poly_X[:, feature_idx] = term\n",
    "            feature_idx += 1\n",
    "    \n",
    "\n",
    "    return poly_X, num_features\n",
    "\n",
    "    ##############################---TODO---##############################\n",
    "\n",
    "\n",
    "  poly_X_test, num_features = polynomial_features(poly_degree, X_test)\n",
    "\n",
    "  poly_X_train, _ = polynomial_features(poly_degree, X_train)\n",
    "\n",
    "\n",
    "  batch_size = 128\n",
    "\n",
    "\n",
    "  g_seed = torch.Generator()\n",
    "\n",
    "  g_seed.manual_seed(seed)\n",
    "\n",
    "  poly_test_data = TensorDataset(poly_X_test, y_test)\n",
    "\n",
    "  poly_test_loader = DataLoader(poly_test_data,\n",
    "\n",
    "                                batch_size=batch_size,\n",
    "\n",
    "                                shuffle=False,\n",
    "\n",
    "                                num_workers=0,\n",
    "\n",
    "                                worker_init_fn=seed_worker,\n",
    "\n",
    "                                generator=g_seed)\n",
    "\n",
    "\n",
    "  poly_train_data = TensorDataset(poly_X_train, y_train)\n",
    "\n",
    "  poly_train_loader = DataLoader(poly_train_data,\n",
    "\n",
    "                                 batch_size=batch_size,\n",
    "\n",
    "                                 shuffle=True,\n",
    "\n",
    "                                 num_workers=0,\n",
    "\n",
    "                                 worker_init_fn=seed_worker,\n",
    "\n",
    "                                 generator=g_seed)\n",
    "\n",
    "\n",
    "  # Defining a linear model using MLP class\n",
    "\n",
    "  poly_net = MLP_pytorch('ReLU()', num_features, [], K)\n",
    "\n",
    "\n",
    "  # Train and test\n",
    "\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "  optimizer = optim.Adam(poly_net.parameters(), lr=1e-3)\n",
    "\n",
    "  _, _ = train_test_classification(poly_net, criterion, optimizer,\n",
    "                                   poly_train_loader, poly_test_loader,\n",
    "\n",
    "                                   num_epochs=100)\n",
    "\n",
    "  X_all = sample_grid()\n",
    "\n",
    "  poly_X_all, _ = polynomial_features(poly_degree, X_all)\n",
    "\n",
    "  y_pred = poly_net(poly_X_all)\n",
    "\n",
    "\n",
    "  # Plot\n",
    "\n",
    "  plot_decision_map(X_all.cpu(), y_pred.cpu(), X_test.cpu(), y_test.cpu())\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "  return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pp3bFmq7JcC"
   },
   "source": [
    "### Train the network. How does it generalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542,
     "referenced_widgets": [
      "7af5397b5dcc4c7d9ca54ef393f52082",
      "00779d55101740cbba83f1cdf0317ed4",
      "63a07ef04c624444b8e076bbc5748198",
      "4bd6b736e5f946e7abb039592e520c0b",
      "eb2578e5db2b42b0b86c918b789e36ce",
      "ea4b007821f14c89a8c31ba19e6dc4a4",
      "af7a3f36cae74ca99b03bb9e0f4ea882",
      "949db35178724049859a9a277393a3fe",
      "9811ee5c85504289b6bf35ced680b4bb",
      "6dd2c90a67944cb7a9e51b25fafe86f1",
      "a968e1eb527f4fabafc475f0e66f617f"
     ]
    },
    "id": "TW3_-gzM7W3d",
    "outputId": "e7780b10-1e07-45d7-f942-962e06261590"
   },
   "outputs": [],
   "source": [
    "seed_setter(seed=SEED)\n",
    "max_poly_deg = 50\n",
    "num_features = polynomial_classifier(max_poly_deg)\n",
    "print(f'Number of features: {num_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz_CvYoQ_lit"
   },
   "source": [
    "Now create another instance of `MLP_pytorch` class having a hidden layer of 128 neurons and train it. Compare the result with the wide network. How does deeper model generalize? Is the decision boundaries ideal? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################---TODO---##############################\n",
    "seed_setter(SEED)\n",
    "net2 = MLP_pytorch('ReLU', 2, [128], K)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net2.parameters(), lr=1e-3)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "_, _ = train_test_classification(net2, criterion, optimizer, train_loader,\n",
    "                                 test_loader, num_epochs=num_epochs,\n",
    "                                 training_plot=True)\n",
    "\n",
    "X_all = sample_grid()\n",
    "y_pred = net2(X_all).cpu()\n",
    "plot_decision_map(X_all, y_pred, X_test, y_test)\n",
    "\n",
    "##############################---TODO---##############################"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
