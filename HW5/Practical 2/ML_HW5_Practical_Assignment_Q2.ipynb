{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VivaFsd3Q6cj"
      },
      "source": [
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=150 height=150> <br>\n",
        "<font color=0F5298 size=7>\n",
        "    Machine learning <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Fall 2024<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 5 - NLP - Transformer & Bert <br>\n",
        "</div>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0CBCDF size=4>\n",
        "   &#x1F349; Masoud Tahmasbi  &#x1F349;  &#x1F353; Arash Ziyaei &#x1F353;\n",
        "<br>\n",
        "<font color=0CBCDF size=4>\n",
        "   &#x1F335; Amirhossein Akbari  &#x1F335;\n",
        "</div>\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k15QziPnmC6d"
      },
      "source": [
        "<font color=9999FF size=4>\n",
        "&#x1F388; Full Name : Amir Malekhosseini\n",
        "<br>\n",
        "<font color=9999FF size=4>\n",
        "&#x1F388; Student Number : 401100528"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOfpEN2xmbN8"
      },
      "source": [
        "<font color=0080FF size=3>\n",
        "This notebook covers two key topics. First, we implement a transformer model from scratch and apply it to a specific task. Second, we fine-tune the BERT model using LoRA for efficient adaptation to a downstream task.\n",
        "</font>\n",
        "<br>\n",
        "\n",
        "**Note:**\n",
        "<br>\n",
        "<font color=66B2FF size=2>In this notebook, you are free to use any function or model from PyTorch to assist with the implementation. However, TensorFlow is not permitted for this exercise. This ensures consistency and alignment with the tools being focused on.</font>\n",
        "<br>\n",
        "<font color=red size=3>**Run All Cells Before Submission**</font>: <font color=FF99CC size=2>Before saving and submitting your notebook, please ensure you run all cells from start to finish. This practice guarantees that your notebook is self-consistent and can be evaluated correctly by others.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpvvM0995ieR"
      },
      "source": [
        "# Section 1: Transformer\n",
        "\n",
        "The transformer architecture consists of two main components: an encoder and a decoder. Each of these components is made up of multiple layers that include self-attention mechanisms and feedforward neural networks. The self-attention mechanism is central to the transformer, as it enables the model to assess the importance of different words in a sentence by considering their relationships with one another.\n",
        "\n",
        "\n",
        "In this assignment, you should design a transformer model from scratch. You are required to implement the Encoder and Decoder components of a Transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzIob6-Gq7Lw"
      },
      "outputs": [],
      "source": [
        "# Importing libraries\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Math\n",
        "import math\n",
        "\n",
        "# HuggingFace libraries\n",
        "from datasets import load_dataset\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "# Pathlib\n",
        "from pathlib import Path\n",
        "\n",
        "# typing\n",
        "from typing import Any\n",
        "\n",
        "# Library for progress bars in loops\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Importing library of warnings\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-71SfIAJprox"
      },
      "source": [
        "## Part 1: Input Embeddings\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we observe the Transformer architecture image above, we can see that the Embeddings represent the first step of both blocks.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>InputEmbedding</code> class below is responsible for converting the input text into numerical vectors of <code>d_model</code> dimensions. To prevent that our input embeddings become extremely small, we normalize them by multiplying them by the $\\sqrt{d_{model}}$.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the image below, we can see how the embeddings are created. First, we have a sentence that gets split into tokensâ€”we will explore what tokens are later onâ€”. Then, the token IDsâ€”identification numbersâ€”are transformed into the embeddings, which are high-dimensional vectors.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-pyrJlu4Nl7"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Creating Input Embeddings\n",
        "class InputEmbeddings(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # Dimension of vectors\n",
        "        self.vocab_size = vocab_size  # Size of the vocabulary\n",
        "        # PyTorch layer that converts integer indices to dense embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Normalizing the variance of the embeddings\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWBlo2XorJGW"
      },
      "source": [
        "## Part 2: positional encoding\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the authors add the positional encodings to the input embeddings at the bottom of both the encoder and decoder blocks so the model can have some information about the relative or absolute position of the tokens in the sequence. The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two vectors can be summed and we can combine the semantic content from the word embeddings and positional information from the positional encodings.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>PositionalEncoding</code> class below, we will create a matrix of positional encodings <code>pe</code> with dimensions <code>(seq_len, d_model)</code>. We will start by filling it with $0$s.We will then apply the sine function to even indices of the positional encoding matrix while the cosine function is applied to the odd ones.</p>\n",
        "\n",
        "<p style=\"\n",
        "    margin-bottom: 5;\n",
        "    font-size: 22px;\n",
        "    font-weight: 300;\n",
        "    font-family: 'Helvetica Neue', sans-serif;\n",
        "    color: #000000;\n",
        "  \">\n",
        "    \\begin{equation}\n",
        "    \\text{Odd Indices } (2i + 1): \\quad \\text{PE(pos, } 2i + 1) = \\cos\\left(\\frac{\\text{pos}}{10000^{2i / d_{model}}}\\right)\n",
        "    \\end{equation}\n",
        "</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We apply the sine and cosine functions because it allows the model to determine the position of a word based on the position of other words in the sequence, since for any fixed offset $k$, $PE_{pos + k}$ can be represented as a linear function of $PE_{pos}$. This happens due to the properties of sine and cosine functions, where a shift in the input results in a predictable change in the output.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZG5DhVcrVVm"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Creating the Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, dimensionality_model: int, maximum_sequence_length: int, dropout_rate: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dimensionality_model = dimensionality_model  # Dimensionality of the model\n",
        "        self.maximum_sequence_length = maximum_sequence_length  # Maximum sequence length\n",
        "        self.dropout_layer = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Creating a positional encoding matrix of shape (maximum_sequence_length, dimensionality_model) filled with zeros\n",
        "        positional_encoding_matrix = torch.zeros(\n",
        "            maximum_sequence_length, dimensionality_model)\n",
        "\n",
        "        position_tensor = torch.arange(0, maximum_sequence_length, dtype=torch.float).unsqueeze(\n",
        "            1)\n",
        "\n",
        "        division_term_tensor = torch.exp(torch.arange(\n",
        "            0, dimensionality_model, 2).float() * (-math.log(10000.0) / dimensionality_model))\n",
        "\n",
        "        # Apply sin to even indices in positional_encoding_matrix\n",
        "        positional_encoding_matrix[:, 0::2] = torch.sin(\n",
        "            position_tensor * division_term_tensor)\n",
        "        # Apply cos to odd indices in positional_encoding_matrix\n",
        "        positional_encoding_matrix[:, 1::2] = torch.cos(\n",
        "            position_tensor * division_term_tensor)\n",
        "\n",
        "        positional_encoding_matrix = positional_encoding_matrix.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('positional_encoding_matrix',\n",
        "                             positional_encoding_matrix)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        # Adding positional encoding to the input tensor\n",
        "        input_tensor = input_tensor + \\\n",
        "            (self.positional_encoding_matrix[:, :input_tensor.shape[1], :]).requires_grad_(\n",
        "                False)\n",
        "        return self.dropout_layer(input_tensor)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T92iydQErh-P"
      },
      "source": [
        "## Part 3: layer normalization\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the encoder and decoder blocks, we see several normalization layers called <b><i>Add &amp; Norm</i></b>.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>LayerNormalization</code> class below performs layer normalization on the input data. During its forward pass, we compute the mean and standard deviation of the input data. We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called epsilon to avoid any divisions by zero. This process results in a normalized output with a mean 0 and a standard deviation 1.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will then scale the normalized output by a learnable parameter <code>alpha</code> and add a learnable parameter called <code>bias</code>. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kVGQRsmKrwZu"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Creating Layer Normalization\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "    def __init__(self, eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "\n",
        "        # Returning the normalized input\n",
        "        return self.alpha * (x-mean) / (std + self.eps) + self.bias\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-IbSGQMr1Ye"
      },
      "source": [
        "## Part 4: Feed Forward Network\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the fully connected feed-forward network, we apply two linear transformations with a ReLU activation in between. We can mathematically represent this operation as:</p>\n",
        "\n",
        "<p style=\"\n",
        "    margin-bottom: 5;\n",
        "    font-size: 22px;\n",
        "    font-weight: 300;\n",
        "    font-family: 'Helvetica Neue', sans-serif;\n",
        "    color: #000000;\n",
        "  \">\n",
        "    \\begin{equation}\n",
        "    \\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
        "    \\end{equation}\n",
        "</p>\n",
        "\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">$W_1$ and $W_2$ are the weights, while $b_1$ and $b_2$ are the biases of the two linear transformations.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>FeedForwardBlock</code> below, we will define the two linear transformationsâ€”<code>self.linear_1</code> and <code>self.linear_2</code>â€”and the inner-layer <code>d_ff</code>. The input data will first pass through the <code>self.linear_1</code> transformation, which increases its dimensionality from <code>d_model</code> to <code>d_ff</code>. The output of this operation passes through the ReLU activation function, which introduces non-linearity so the network can learn more complex patterns, and the <code>self.dropout</code> layer is applied to mitigate overfitting. The final operation is the <code>self.linear_2</code> transformation to the dropout-modified tensor, which transforms it back to the original <code>d_model</code> dimension.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3H8kyccsEUW"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Creating Feed Forward Layers\n",
        "class FeedForwardBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEa1kF6csIvV"
      },
      "source": [
        "## Part 5: Multi Head Attention\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention is the most crucial component of the Transformer. It is responsible for helping the model to understand complex relationships and patterns in the data.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The image below displays how the Multi-Head Attention works. It doesn't include <code>batch</code> dimension because it only illustrates the process for one single sentence.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The Multi-Head Attention block receives the input data split into queries, keys, and values organized into matrices $Q$, $K$, and $V$. Each matrix contains different facets of the input, and they have the same dimensions as the input.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We then linearly transform each matrix by their respective weight matrices $W^Q$, $W^K$, and $W^V$. These transformations will result in new matrices $Q'$, $K'$, and $V'$, which will be split into smaller matrices corresponding to different heads $h$, allowing the model to attend to information from different representation subspaces in parallel. This split creates multiple sets of queries, keys, and values for each head.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Finally, we concatenate every head into an $H$ matrix, which is then transformed by another weight matrix $W^o$ to produce the multi-head attention output, a matrix $MH-A$ that retains the input dimensionality.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ujcqPp1sOU9"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Creating the Multi-Head Attention block\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:  # h = number of heads\n",
        "\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "\n",
        "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
        "        self.d_k = d_model // h\n",
        "\n",
        "        # Defining the weight matrices\n",
        "        self.w_q = nn.Linear(d_model, d_model)  # W_q\n",
        "        self.w_k = nn.Linear(d_model, d_model)  # W_k\n",
        "        self.w_v = nn.Linear(d_model, d_model)  # W_v\n",
        "        self.w_o = nn.Linear(d_model, d_model)  # W_o\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
        "\n",
        "        d_k = query.shape[-1]\n",
        "\n",
        "        # We calculate the Attention(Q,K,V)\n",
        "        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "\n",
        "        return (attention_scores @ value), attention_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "\n",
        "\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(\n",
        "            1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(\n",
        "            1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(\n",
        "            1, 2)\n",
        "\n",
        "        x, self.attention_scores = MultiHeadAttentionBlock.attention(\n",
        "            query, key, value, mask, self.dropout)\n",
        "\n",
        "        # H matrix\n",
        "        x = x.transpose(1, 2).contiguous().view(\n",
        "            x.shape[0], -1, self.h * self.d_k)\n",
        "\n",
        "        return self.w_o(x)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCaLjCVxsWIc"
      },
      "source": [
        "## Part 6: Residual Connection\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">When we look at the architecture of the Transformer, we see that each sub-layer, including the <i>self-attention</i> and <i>Feed Forward</i> blocks, adds its output to its input before passing it to the <i>Add &amp; Norm</i> layer. This approach integrates the output with the original input in the <i>Add &amp; Norm</i> layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>ResidualConnection</code> class below is responsible for this process.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-bvuGhIsdfu"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Building Residual Connection\n",
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "    def __init__(self, dropout: float) -> None:\n",
        "\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "\n",
        "        # Create the residual connections.\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YYI5vpasdGm"
      },
      "source": [
        "## Part 7: Encoder\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now build the encoder. We create the <code>EncoderBlock</code> class, consisting of the Multi-Head Attention and Feed Forward layers, plus the residual connections.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the original paper, the Encoder Block repeats six times. We create the <code>Encoder</code> class as an assembly of multiple <code>EncoderBlock</code>s. We also add layer normalization as a final step after processing the input through all its blocks.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRtppwE1s0-t"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Building Encoder Block\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(\n",
        "            dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # The first residual connection\n",
        "        x = self.residual_connections[0](\n",
        "            x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "\n",
        "        # The second residual connection\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSq7BZWcs5s1"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Building Encoder\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers \n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0HXE1fH5g0W"
      },
      "source": [
        "## Part 8: Decoder\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Similarly, the Decoder also consists of several DecoderBlocks that repeat six times in the original paper. The main difference is that it has an additional sub-layer that performs multi-head attention with a <i>cross-attention</i> component that uses the output of the Encoder as its keys and values while using the Decoder's input as queries.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For the Output Embedding, we can use the same <code>InputEmbeddings</code> class we use for the Encoder. You can also notice that the self-attention sub-layer is <i>masked</i>, which restricts the model from accessing future elements in the sequence.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will start by building the <code>DecoderBlock</code> class, and then we will build the <code>Decoder</code> class, which will assemble multiple <code>DecoderBlock</code>s.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Aof9mb4PJX"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Building Decoder Block\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.cross_attention_block = cross_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList(\n",
        "            [ResidualConnection(dropout) for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "        # Self-Attention block ,target language mask\n",
        "        x = self.residual_connections[0](\n",
        "            x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
        "\n",
        "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(\n",
        "            x, encoder_output, encoder_output, src_mask))\n",
        "\n",
        "        # Feed-forward block with residual connections\n",
        "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
        "        return x\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwdthvkrtNUM"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Building Decoder\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "\n",
        "        # Iterating over each DecoderBlock stored in self.layers\n",
        "        for layer in self.layers:\n",
        "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qm4g_8O1tS3d"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">You can see in the Decoder image that after running a stack of <code>DecoderBlock</code>s, we have a Linear Layer and a Softmax function to the output of probabilities. The <code>ProjectionLayer</code> class below is responsible for converting the output of the model into a probability distribution over the <i>vocabulary</i>, where we select each output token from a vocabulary of possible tokens.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UbWVoNintThN"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Buiding Linear Layer\n",
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super().__init__()\n",
        "        # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.log_softmax(self.proj(x), dim=-1)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waCzPEAxtaR8"
      },
      "source": [
        "## Part 9: Building the Transformer\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally have every component of the Transformer architecture ready. We may now construct the Transformer by putting it all together.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>Transformer</code> class below, we will bring together all the components of the model's architecture.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXbPW4oCtk2G"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Creating the Transformer Architecture\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    # This takes in the encoder, decoder, embeddings, and PositionalEncoding.\n",
        "    def __init__(self, encoder_layer: Encoder, decoder_layer: Decoder, source_embedding: InputEmbeddings, target_embedding: InputEmbeddings, source_positional_encoding: PositionalEncoding, target_positional_encoding: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
        "        super().__init__()\n",
        "        self.encoder_layer = encoder_layer\n",
        "        self.decoder_layer = decoder_layer\n",
        "        self.source_embedding = source_embedding\n",
        "        self.target_embedding = target_embedding\n",
        "        self.source_positional_encoding = source_positional_encoding\n",
        "        self.target_positional_encoding = target_positional_encoding\n",
        "        self.projection_layer = projection_layer\n",
        "\n",
        "    # Encoder\n",
        "    def encode(self, source_input, source_mask):\n",
        "\n",
        "        source_input = self.source_embedding(source_input)\n",
        "        source_input = self.source_positional_encoding(source_input)\n",
        "        return self.encoder_layer(source_input, source_mask)\n",
        "\n",
        "    # Decoder\n",
        "    def decode(self, encoder_output, source_mask, target_input, target_mask):\n",
        "\n",
        "        target_input = self.target_embedding(target_input)\n",
        "        target_input = self.target_positional_encoding(target_input)\n",
        "\n",
        "        return self.decoder_layer(target_input, encoder_output, source_mask, target_mask)\n",
        "\n",
        "    def project(self, decoder_output):\n",
        "        return self.projection_layer(decoder_output)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6znypMaetmRk"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The architecture is finally ready. We now define a function called <code>build_transformer</code>, in which we define the parameters and everything we need to have a fully operational Transformer model for the task of <b>machine translation</b>.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will set the same parameters as in the original paper, <a href = \"https://arxiv.org/pdf/1706.03762.pdf\"><i>Attention Is All You Need</i></a>, where $d_{model}$ = 512, $N$ = 6, $h$ = 8, dropout rate $P_{drop}$ = 0.1, and $d_{ff}$ = 2048.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqGnJ6w2twJc"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Building & Initializing Transformer\n",
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
        "\n",
        "    # Creating Embedding layers\n",
        "    source_embedding = InputEmbeddings(d_model, src_vocab_size)\n",
        "    target_embedding = InputEmbeddings(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Creating Positional Encoding layers\n",
        "    source_position = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    target_position = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Creating EncoderBlocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        encoder_self_attention_block = MultiHeadAttentionBlock(\n",
        "            d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(\n",
        "            d_model, d_ff, dropout)\n",
        "\n",
        "        # Combine layers into an EncoderBlock\n",
        "        encoder_block = EncoderBlock(\n",
        "            encoder_self_attention_block, feed_forward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Creating DecoderBlocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(N):\n",
        "        decoder_self_attention_block = MultiHeadAttentionBlock(\n",
        "            d_model, h, dropout)\n",
        "        decoder_cross_attention_block = MultiHeadAttentionBlock(\n",
        "            d_model, h, dropout)\n",
        "        feed_forward_block = FeedForwardBlock(\n",
        "            d_model, d_ff, dropout)\n",
        "\n",
        "        # Combining layers into a DecoderBlock\n",
        "        decoder_block = DecoderBlock(\n",
        "            decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Creating the Encoder and Decoder\n",
        "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Creating projection layer\n",
        "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Creating the transformer\n",
        "    transformer = Transformer(\n",
        "        encoder, decoder, source_embedding, target_embedding, source_position, target_position, projection_layer)\n",
        "\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iw7CWf4bt3yr"
      },
      "source": [
        "The model is now ready to be trained!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_7Z3fEYuTK0"
      },
      "source": [
        "## Part 10: Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDinqTghqr_Q"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Tokenization is a crucial preprocessing step for our Transformer model. In this step, we convert raw text into a number format that the model can process.  </p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">There are several Tokenization strategies. We will use the <i>word-level tokenization</i> to transform each word in a sentence into a token.</p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "at-cYYjnqr_Q"
      },
      "source": [
        "<center>\n",
        "    <img src = \"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5e749c-b0bd-4496-85a1-9b4397ad935f_1400x787.jpeg\" width = 800, height= 800>\n",
        "<p style = \"font-size: 16px;\n",
        "            font-family: 'Georgia', serif;\n",
        "            text-align: center;\n",
        "            margin-top: 10px;\">Different tokenization strategies. Source: <a href = \"https://shaankhosla.substack.com/p/talking-tokenization\">shaankhosla.substack.com</a>.</p>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjRMr2N6qr_Q"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">After tokenizing a sentence, we map each token to an unique integer ID based on the created vocabulary present in the training corpus during the training of the tokenizer. Each integer number represents a specific word in the vocabulary.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Besides the words in the training corpus, Transformers use special tokens for specific purposes. These are some that we will define right away:</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢ [UNK]:</b> This token is used to identify an unknown word in the sequence.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢ [PAD]:</b> Padding token to ensure that all sequences in a batch have the same length, so we pad shorter sentences with this token. We use attention masks to <i>\"tell\"</i> the model to ignore the padded tokens during training since they don't have any real meaning to the task.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢  [SOS]:</b> This is a token used to signal the <i>Start of Sentence</i>.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\"><b>â€¢  [EOS]:</b> This is a token used to signal the <i>End of Sentence</i>.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>build_tokenizer</code> function below, we ensure a tokenizer is ready to train the model. It checks if there is an existing tokenizer, and if that is not the case, it trains a new tokenizer.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh9pOItduxHq"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Defining Tokenizer\n",
        "def build_tokenizer(config, ds, lang):\n",
        "\n",
        "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
        "\n",
        "    if not Path.exists(tokenizer_path):\n",
        "\n",
        "        tokenizer = Tokenizer(WordLevel(unk_token='[UNK]'))\n",
        "        tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\",\n",
        "                                                   \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
        "\n",
        "        tokenizer.train_from_iterator(\n",
        "            get_all_sentences(ds, lang), trainer=trainer)\n",
        "        tokenizer.save(str(tokenizer_path))\n",
        "    else:\n",
        "        tokenizer = Tokenizer.from_file(str(tokenizer_path))\n",
        "    return tokenizer\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oodlr4eouxTU"
      },
      "source": [
        "## Part 11: Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdVFowgUqr_Q"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">For this task, we will use the <a href = \"opus_books Â· Datasets at Hugging Face\">OpusBooks dataset</a>, available on ðŸ¤—Hugging Face. This dataset consists of two features, <code>id</code> and <code>translation</code>. The <code>translation</code> feature contains pairs of sentences in different languages, such as Spanish and Portuguese, English and French, and so forth.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I first tried translating sentences from English to Portugueseâ€”my native tongue â€” but there are only 1.4k examples for this pair, so the results were not satisfying in the current configurations for this model. I then tried to use the English-French pair due to its higher number of examplesâ€”127kâ€”but it would take too long to train with the current configurations. I then opted to train the model on the English-Italian pair, the same one used in the <a href = \"https://youtu.be/ISNdQcPhsts?si=253J39cose6IdsLv\">Coding a Transformer from scratch on PyTorch, with full explanation, training and inference\n",
        "</a> video, as that was a good balance between performance and time of training.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by defining the <code>get_all_sentences</code> function to iterate over the dataset and extract the sentences according to the language pair definedâ€”we will do that later.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvRuuTpIveZS"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Iterating through dataset\n",
        "def get_all_sentences(ds, lang):\n",
        "    for pair in ds:\n",
        "        yield pair['translation'][lang]\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA13IRYEqr_R"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_ds</code> function is defined to load and prepare the dataset for training and validation. In this function, we build or load the tokenizer, split the dataset, and create DataLoaders, so the model can successfully iterate over the dataset in batches. The result of these functions is tokenizers for the source and target languages plus the DataLoader objects.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkTRqP8LvpVy"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "def get_ds(config):\n",
        "\n",
        "    ds_raw = load_dataset(\n",
        "        'opus_books', f'{config[\"lang_src\"]}-{config[\"lang_tgt\"]}', split='train')\n",
        "\n",
        "    # Building or loading tokenizer for both the source and target languages\n",
        "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
        "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
        "\n",
        "    train_ds_size = int(0.9 * len(ds_raw))\n",
        "    val_ds_size = len(ds_raw) - train_ds_size\n",
        "    train_ds_raw, val_ds_raw = random_split(\n",
        "        ds_raw, [train_ds_size, val_ds_size]) \n",
        "\n",
        "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt,\n",
        "                                config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt,\n",
        "                              config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
        "\n",
        "    max_length_source = 0\n",
        "    max_length_target = 0\n",
        "    for pair in ds_raw:\n",
        "        src_ids = tokenizer_src.encode(\n",
        "            pair['translation'][config['lang_src']]).ids\n",
        "        tgt_ids = tokenizer_src.encode(\n",
        "            pair['translation'][config['lang_tgt']]).ids\n",
        "        max_length_source = max(max_length_source, len(src_ids))\n",
        "        max_length_target = max(max_length_target, len(tgt_ids))\n",
        "\n",
        "    print(f'Max length of source sentence: {max_length_source}')\n",
        "    print(f'Max length of target sentence: {max_length_target}')\n",
        "\n",
        "    # Creating dataloaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_ds, batch_size=config['batch_size'], shuffle=True)\n",
        "    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)\n",
        "\n",
        "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK3d2-AVqr_R"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We define the <code>casual_mask</code> function to create a mask for the attention mechanism of the decoder. This mask prevents the model from having information about future elements in the sequence. </p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We start by making a square grid filled with ones. We determine the grid size with the <code>size</code> parameter. Then, we change all the numbers above the main diagonal line to zeros. Every number on one side becomes a zero, while the rest remain ones. The function then flips all these values, turning ones into zeros and zeros into ones. This process is crucial for models that predict future tokens in a sequence.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTgMYaY2vvWq"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "def casual_mask(size):\n",
        "    mask = torch.triu(torch.ones(1, size, size), diagonal=1).type(torch.int)\n",
        "    return mask == 0\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccdK5XnMqr_R"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>BilingualDataset</code> class processes the texts of the target and source languages in the dataset by tokenizing them and adding all the necessary special tokens. This class also certifies that the sentences are within a maximum sequence length for both languages and pads all necessary sentences.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x9v94mdgv3y6"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "\n",
        "class BilingualDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, source_tokenizer, target_tokenizer, source_language, target_language, sequence_length) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.dataset = dataset\n",
        "        self.source_tokenizer = source_tokenizer\n",
        "        self.target_tokenizer = target_tokenizer\n",
        "        self.source_language = source_language\n",
        "        self.target_language = target_language\n",
        "\n",
        "        # Defining special tokens by using the target language tokenizer\n",
        "        self.start_of_sequence_token = torch.tensor(\n",
        "            [target_tokenizer.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
        "        self.end_of_sequence_token = torch.tensor(\n",
        "            [target_tokenizer.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
        "        self.padding_token = torch.tensor(\n",
        "            [target_tokenizer.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: Any) -> Any:\n",
        "        source_target_pair = self.dataset[index]\n",
        "        source_text = source_target_pair['translation'][self.source_language]\n",
        "        target_text = source_target_pair['translation'][self.target_language]\n",
        "\n",
        "        # Tokenizing source and target texts\n",
        "        encoded_source_tokens = self.source_tokenizer.encode(source_text).ids\n",
        "        encoded_target_tokens = self.target_tokenizer.encode(target_text).ids\n",
        "\n",
        "        # Source tokens\n",
        "        source_padding_tokens_count = self.sequence_length - \\\n",
        "            len(encoded_source_tokens) - 2\n",
        "        # Target tokens\n",
        "        target_padding_tokens_count = self.sequence_length - \\\n",
        "            len(encoded_target_tokens) - 1\n",
        "\n",
        "        if source_padding_tokens_count < 0 or target_padding_tokens_count < 0:\n",
        "            raise ValueError('Sentence is too long')\n",
        "\n",
        "        # Building the encoder input tensor\n",
        "        encoder_input = torch.cat(\n",
        "            [\n",
        "                self.start_of_sequence_token,\n",
        "                torch.tensor(encoded_source_tokens, dtype=torch.int64),\n",
        "                self.end_of_sequence_token,  # Inserting the '[EOS]' token\n",
        "                torch.tensor([self.padding_token] * source_padding_tokens_count,\n",
        "                             dtype=torch.int64) \n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Building the decoder input tensor\n",
        "        decoder_input = torch.cat(\n",
        "            [\n",
        "                self.start_of_sequence_token,  # inserting the '[SOS]' token\n",
        "                torch.tensor(encoded_target_tokens, dtype=torch.int64),\n",
        "                torch.tensor([self.padding_token] * target_padding_tokens_count,\n",
        "                             dtype=torch.int64)\n",
        "            ]\n",
        "\n",
        "        )\n",
        "\n",
        "        label = torch.cat(\n",
        "            [\n",
        "                # Inserting the tokenized target text\n",
        "                torch.tensor(encoded_target_tokens, dtype=torch.int64),\n",
        "                self.end_of_sequence_token,  # Inserting the '[EOS]' token\n",
        "                torch.tensor([self.padding_token] * target_padding_tokens_count,\n",
        "                             dtype=torch.int64)\n",
        "\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        assert encoder_input.size(0) == self.sequence_length\n",
        "        assert decoder_input.size(0) == self.sequence_length\n",
        "        assert label.size(0) == self.sequence_length\n",
        "\n",
        "        return {\n",
        "            'encoder_input': encoder_input,\n",
        "            'decoder_input': decoder_input,\n",
        "            'encoder_mask': (encoder_input != self.padding_token).unsqueeze(0).unsqueeze(0).int(),\n",
        "            'decoder_mask': (decoder_input != self.padding_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
        "            'label': label,\n",
        "            'source_text': source_text,\n",
        "            'target_text': target_text\n",
        "        }\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7cXlNUfv5uL"
      },
      "source": [
        "## Part 12: Validation Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf8Wt860qr_R"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will now create two functions for the validation loop. The validation loop is crucial to evaluate model performance in translating sentences from data it has not seen during training.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We will define two functions. The first function, <code>greedy_decode</code>, gives us the model's output by obtaining the most probable next token. The second function, <code>run_validation</code>, is responsible for running the validation process in which we decode the model's output and compare it with the reference text for the target sentence.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1rzcAkpv8Ew"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Define function to obtain the most probable next token\n",
        "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
        "\n",
        "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
        "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
        "\n",
        "    # Computing the output of the encoder for the source sequence\n",
        "    encoder_output = model.encode(source, source_mask)\n",
        "    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n",
        "\n",
        "    while True:\n",
        "        if decoder_input.size(1) == max_len:\n",
        "            break\n",
        "\n",
        "        decoder_mask = casual_mask(decoder_input.size(\n",
        "            1)).type_as(source_mask).to(device)\n",
        "\n",
        "        out = model.decode(encoder_output, source_mask,\n",
        "                           decoder_input, decoder_mask)\n",
        "\n",
        "        # Applying the projection layer to get the probabilities for the next token\n",
        "        prob = model.project(out[:, -1])\n",
        "\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        decoder_input = torch.cat([decoder_input, torch.empty(1, 1). type_as(\n",
        "            source).fill_(next_word.item()).to(device)], dim=1)\n",
        "\n",
        "        if next_word == eos_idx:\n",
        "            break\n",
        "\n",
        "    return decoder_input.squeeze(0)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iF7v9L0owcLT"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Defining function to evaluate the model on the validation dataset\n",
        "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
        "    model.eval()\n",
        "    count = 0\n",
        "\n",
        "    console_width = 80\n",
        "\n",
        "    # Creating evaluation loop\n",
        "    with torch.no_grad():\n",
        "        for batch in validation_ds:\n",
        "            count += 1\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "\n",
        "            assert encoder_input.size(\n",
        "                0) == 1, 'Batch size must be 1 for validation.'\n",
        "\n",
        "            model_out = greedy_decode(\n",
        "                model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
        "\n",
        "            source_text = batch['source_text'][0]\n",
        "            target_text = batch['target_text'][0]  # True translation\n",
        "            # Decoded, human-readable model output\n",
        "            model_out_text = tokenizer_tgt.decode(\n",
        "                model_out.detach().cpu().numpy())\n",
        "\n",
        "            print_msg('-'*console_width)\n",
        "            print_msg(f'SOURCE: {source_text}')\n",
        "            print_msg(f'TARGET: {target_text}')\n",
        "            print_msg(f'PREDICTED: {model_out_text}')\n",
        "\n",
        "            if count == num_examples:\n",
        "                break\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qw3nykKxwkIh"
      },
      "source": [
        "## Part 13: Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az_Kwq4Zqr_S"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We are ready to train our Transformer model on the OpusBook dataset for the English to Italian translation task.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We first start by defining the <code>get_model</code> function to load the model by calling the <code>build_transformer</code> function we have previously defined. This function uses the <code>config</code> dictionary to set a few parameters.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7QMn1BULwnBl"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
        "    model = build_transformer(vocab_src_len, vocab_tgt_len,\n",
        "                              config['seq_len'], config['seq_len'], config['d_model'])\n",
        "    return model\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ord2DlVkqr_S"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">I have mentioned the <code>config</code> dictionary several times throughout this notebook. Now, it is time to create it.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the following cell, we will define two functions to configure our model and the training process.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In the <code>get_config</code> function, we define crucial parameters for the training process. <code>batch_size</code> for the number of training examples used in one iteration, <code>num_epochs</code> as the number of times the entire dataset is passed forward and backward through the Transformer, <code>lr</code> as the learning rate for the optimizer, etc. We will also finally define the pairs from the OpusBook dataset, <code>'lang_src': 'en'</code> for selecting English as the source language and <code>'lang_tgt': 'it'</code> for selecting Italian as the target language.</p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">The <code>get_weights_file_path</code> function constructs the file path for saving or loading model weights for any specific epoch.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXt82CejxeHZ"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "def get_config():\n",
        "    return {\n",
        "        'batch_size': 10,\n",
        "        'num_epochs': 10,\n",
        "        'lr': 10**-4,\n",
        "        'seq_len': 350,\n",
        "        'd_model': 512,\n",
        "        'lang_src': 'en',\n",
        "        'lang_tgt': 'it',\n",
        "        'model_folder': 'weights',\n",
        "        'model_basename': 'tmodel_',\n",
        "        'preload': None,\n",
        "        'tokenizer_file': 'tokenizer_{0}.json',\n",
        "        'experiment_name': 'runs/tmodel'\n",
        "    }\n",
        "\n",
        "\n",
        "def get_weights_file_path(config, epoch: str):\n",
        "    model_folder = config['model_folder']\n",
        "    model_basename = config['model_basename']\n",
        "    model_filename = f\"{model_basename}{epoch}.pt\"  \n",
        "    return str(Path('.') / model_folder / model_filename)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw7SjmrDqr_S"
      },
      "source": [
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">We finally define our last function, <code>train_model</code>, which takes the <code>config</code> arguments as input. </p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">In this function, we will set everything up for the training. We will load the model and its necessary components onto the GPU for faster training, set the <code>Adam</code> optimizer, and configure the <code>CrossEntropyLoss</code> function to compute the differences between the translations output by the model and the reference translations from the dataset. </p>\n",
        "\n",
        "<p style = \"font-family: 'Helvetica Neue', Arial, sans-serif; text-align: left; font-size: 17.5px\">Every loop necessary for iterating over the training batches, performing backpropagation, and computing the gradients is in this function. We will also use it to run the validation function and save the current state of the model.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qK9wAjRxoDQ"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "\n",
        "def train_model(config):\n",
        "    # Setting up device to run on GPU to train faster\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device {device}\")\n",
        "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(\n",
        "        config)\n",
        "\n",
        "    model = get_model(config, tokenizer_src.get_vocab_size(),\n",
        "                      tokenizer_tgt.get_vocab_size()).to(device)\n",
        "    writer = SummaryWriter(config['experiment_name'])\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps=1e-9)\n",
        "\n",
        "    initial_epoch = 0\n",
        "    global_step = 0\n",
        "\n",
        "    if config['preload']:\n",
        "        model_filename = get_weights_file_path(config, config['preload'])\n",
        "        print(f'Preloading model {model_filename}')\n",
        "        state = torch.load(model_filename)  # Loading model\n",
        "\n",
        "        initial_epoch = state['epoch'] + 1\n",
        "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "        global_step = state['global_step']\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer_src.token_to_id(\n",
        "        '[PAD]'),label_smoothing=0.1).to(device)\n",
        "\n",
        "    #  Training loop\n",
        "    for epoch in range(initial_epoch, config['num_epochs']):\n",
        "\n",
        "        batch_iterator = tqdm(\n",
        "            train_dataloader, desc=f'Processing epoch {epoch:02d}')\n",
        "\n",
        "        for batch in batch_iterator:\n",
        "            model.train()  # Train\n",
        "\n",
        "            # Loading input data and masks onto the GPU\n",
        "            encoder_input = batch['encoder_input'].to(device)\n",
        "            decoder_input = batch['decoder_input'].to(device)\n",
        "            encoder_mask = batch['encoder_mask'].to(device)\n",
        "            decoder_mask = batch['decoder_mask'].to(device)\n",
        "\n",
        "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
        "            decoder_output = model.decode(\n",
        "                encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
        "            proj_output = model.project(decoder_output)\n",
        "\n",
        "            # Loading the target labels onto the GPU\n",
        "            label = batch['label'].to(device)\n",
        "\n",
        "            # Computing loss between output and true labels\n",
        "            loss = loss_fn(\n",
        "                proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
        "\n",
        "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
        "\n",
        "            writer.add_scalar('train loss', loss.item(), global_step)\n",
        "            writer.flush()\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "\n",
        "        # We run the 'run_validation' function\n",
        "\n",
        "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt,\n",
        "                       config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
        "\n",
        "        # Saving model\n",
        "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
        "        torch.save({\n",
        "            'epoch': epoch, \n",
        "            'model_state_dict': model.state_dict(),  \n",
        "            'optimizer_state_dict': optimizer.state_dict(),  \n",
        "            'global_step': global_step  \n",
        "        }, model_filename)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrMmfyi8xrXw"
      },
      "source": [
        "We can now train the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28425EYaxrsi",
        "outputId": "5ab8f5b3-ef9f-49ec-dd6b-932705b59ffa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cuda\n",
            "Max length of source sentence: 309\n",
            "Max length of target sentence: 274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 00: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:41<00:00,  2.46it/s, loss=5.380]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: There must have been a boat attached to that tow-line at some time or other, that was certain; but what had become of it, what ghastly fate had overtaken it, and those who had been left in it, was buried in mystery.\n",
            "TARGET: Ci doveva essere stata in qualche momento una barca attaccata a quel cavo, questo era certo; ma che fosse successo della barca, qual triste destino lâ€™avesse raggiunta con quelli che erano stati abbandonati dentro, era impossibile dire.\n",
            "PREDICTED: Ma non a me , ma non a , e che non , e non a , e che non a .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: The small patch that was left in the last corner was mown in five minutes; and whilst the last mowers were finishing their swaths, those in front, carrying their coats over their shoulders, were already crossing the road toward Mashkin Heights.\n",
            "TARGET: Il tratto di campo che era rimasto in angolo fu tagliato in cinque minuti. Non ancora gli ultimi falciatori tagliavano la falciata, che giÃ  quelli avanti avevano gettato i gabbani sulle spalle e si avviavano sulla strada verso il MaÅ¡kin Verch.\n",
            "PREDICTED: Il suo momento era una volta , e la sua sua volta , e la sua sua sua sua sua , e la sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua sua .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 01: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:41<00:00,  2.46it/s, loss=4.789]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'Well then, let's dine together.'\n",
            "TARGET: â€” Su via, andiamo a pranzare insieme.\n",
            "PREDICTED: â€” E , come c â€™ Ã¨ un .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Kitty played the prelude and looked round at Varenka.\n",
            "TARGET: Kitty ne accennÃ² le prime battute e si voltÃ² a guardare Varenâ€™ka.\n",
            "PREDICTED: Kitty si mise a e si mise a parlare .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 02: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:40<00:00,  2.46it/s, loss=4.499]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: He did not feel comfortable at the thought of leaving his brother alone all day long, and he also feared that Koznyshev might laugh at him.\n",
            "TARGET: Gli rincresceva lasciare il fratello solo per giornate intere, e poi temeva che non avesse a prendersi giuoco di lui per questo.\n",
            "PREDICTED: Non era un â€™ altra cosa , che era stato un â€™ altra cosa , e , come se ne era stato stato un â€™ altra cosa .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: First, I imagined that upon seeing my light they might have put themselves into their boat, and endeavoured to make the shore: but that the sea running very high, they might have been cast away.\n",
            "TARGET: Primieramente supposi che, alla vista del segnale dato da me, si fossero gettati veramente nella loro scialuppa e ingegnati di salvarsi alla spiaggia, ma che il flutto troppo grosso ne gli avesse respinti.\n",
            "PREDICTED: che il mio giorno , il mio moschetto , e il mio moschetto , e il mio moschetto , e il mio moschetto , e il mio moschetto .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 03: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:40<00:00,  2.46it/s, loss=4.621]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: It was moonlight and gaslight besides, and very still and serene. The balcony was furnished with a chair or two; I sat down, and took out a cigar,--I will take one now, if you will excuse me.\"\n",
            "TARGET: \"Era lume di luna e il gaz era acceso; la notte era calma e serena, e, sedutomi sul terrazzo, accesi un sigaro. Ne accenderÃ² uno ancora, se me lo permettete.\n",
            "PREDICTED: Era un ' aria e un ' altra , e un po ' di seta , che mi , e mi alzai e mi , e mi , e mi , e mi , e mi , e mi , se ne , se ne .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: \"The human and fallible should not arrogate a power with which the divine and perfect alone can be safely intrusted.\"\n",
            "TARGET: â€” Gli uomini fallibili non dovrebbero arrogarsi un potere che non puÃ² essere affidato sicuramente che agli esseri perfetti e divini.\n",
            "PREDICTED: â€” e non nÃ© , nÃ© , e che la natura .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 04: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:40<00:00,  2.46it/s, loss=4.584]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: CHAPTER II The Pool of Tears\n",
            "TARGET: II LO STAGNO DI LAGRIME\n",
            "PREDICTED: II\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: You know it's only some seventy versts off. I shall certainly go over.\n",
            "TARGET: PerchÃ© sono a settanta verste da voi, e io pure ci andrÃ² certamente.\n",
            "PREDICTED: Sapete che Ã¨ un â€™ idea di a .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 05: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:40<00:00,  2.46it/s, loss=3.941]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: 'What an absurd fancy!\n",
            "TARGET: â€” Che fantasia sciocca!\n",
            "PREDICTED: â€” Che sciocchezza !\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: One thing specially surprised me, and that was, there were no journeyings backward and forward, no visits to Ingram Park: to be sure it was twenty miles off, on the borders of another county; but what was that distance to an ardent lover?\n",
            "TARGET: Una cosa specialmente mi meravigliava, ed era di non vedere mai nessuno degli Ingram alla villa e non veder mai il signor Rochester andare a Ingram-Park: Ã¨ vero che era distante venti miglia, ma che cosa era quella distanza per un innamorato ardente?\n",
            "PREDICTED: Una cosa sola mi fece notare che non c ' era , e non c ' era nulla di piÃ¹ , ma una delle cinque miglia di cittÃ  , che era un ' altra cosa , che era una donna di cui si doveva essere ?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 06: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:42<00:00,  2.45it/s, loss=4.129]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: This lane inclined up-hill all the way to Hay; having reached the middle, I sat down on a stile which led thence into a field. Gathering my mantle about me, and sheltering my hands in my muff, I did not feel the cold, though it froze keenly; as was attested by a sheet of ice covering the causeway, where a little brooklet, now congealed, had overflowed after a rapid thaw some days since.\n",
            "TARGET: Giunta a metÃ , mi sedei sui gradini di una scaletta che conduceva a un campo, mi avvolsi nel mantello e nascosi le mani nel manicotto, perchÃ© il freddo era intenso.\n",
            "PREDICTED: In mezzo alla collina mi tutta la strada , e vidi che una strada , la strada in mezzo alla mia abitazione , e le mani , e le mani , e le braccia , le braccia , e come un , , mi , come un , ora , prima che le , , le pareti , le ombre .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: The worst of the matter is, that she is already...\n",
            "TARGET: E il peggio di tutto Ã¨ che giÃ ...\n",
            "PREDICTED: Ãˆ il fatto che Ã¨ giÃ  finito ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 07: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3638/3638 [24:42<00:00,  2.45it/s, loss=3.650]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "SOURCE: Our lads took the horses that way last night.'\n",
            "TARGET: I ragazzi iersera hanno spinto lÃ  le bestie.\n",
            "PREDICTED: I ragazzi si misero a vedere i cavalli .\n",
            "--------------------------------------------------------------------------------\n",
            "SOURCE: He was glad he had said it to her, and that she now knew it and was thinking about it.\n",
            "TARGET: Ed era felice di averglielo detto, era felice châ€™ella lo sapesse e ci pensasse.\n",
            "PREDICTED: Egli aveva detto di non aver detto a lei , e ora che ora sapeva ora e che ora era proprio .\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing epoch 08:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 2547/3638 [17:18<07:24,  2.45it/s, loss=4.014]"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "    warnings.filterwarnings('ignore')\n",
        "    config = get_config()\n",
        "    train_model(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQDjj1f3R-m5"
      },
      "source": [
        "Got colab limit for GPU usage:)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3axMN7QWiVH"
      },
      "source": [
        "# Section 2: BERT and LoRA\n",
        "\n",
        "Welcome to Section 2 of our Machine Learning assignment! I hope you've been enjoying the journey so far! ðŸ˜Š\n",
        "\n",
        " In this section, you will gain hands-on experience with [BERT](https://arxiv.org/abs/1810.04805) (Bidirectional Encoder Representations from Transformers) and [LoRA](https://arxiv.org/abs/2106.09685) (Low-Rank Adaptation) for text classification tasks. The section is divided into three main parts, each focusing on different aspects of NLP techniques.\n",
        "\n",
        "## Assignment Structure\n",
        "\n",
        "### Part 1: Data Preparation and Preprocessing\n",
        "In this part, you will work with a text classification dataset. You will learn how to:\n",
        "- Download and load the dataset\n",
        "- Perform necessary preprocessing steps\n",
        "- Implement data cleaning and transformation techniques\n",
        "- Prepare the data in a format suitable for BERT training\n",
        "\n",
        "### Part 2: Building a Small BERT Model\n",
        "You will create and train a small BERT model from scratch using the Hugging Face [Transformers](https://huggingface.co/docs/transformers/en/index) library. This part will help you understand:\n",
        "- The architecture of BERT\n",
        "- How to configure and initialize a BERT model\n",
        "- Training process and optimization\n",
        "- Model evaluation and performance analysis\n",
        "\n",
        "### Part 3: Fine-tuning with LoRA\n",
        "In the final part, you will work with a pre-trained [TinyBERT](https://arxiv.org/abs/1909.10351) model and use LoRA for efficient fine-tuning. You will:\n",
        "- Load a pre-trained TinyBERT model\n",
        "- Implement LoRA adaptation and fine-tune the model on our classification task\n",
        "- Compare the results with the previous approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6FKcSFbOTMd"
      },
      "source": [
        "---\n",
        "\n",
        "> **NOTE**:  \n",
        "> Throughout this notebook, make an effort to include sufficient visualizations to enhance understanding:  \n",
        "> - In the data processing section, display the results of your operations (e.g., show data samples or distributions after preprocessing).  \n",
        "> - In the classification section, report various evaluation metrics such as accuracy, precision, recall, and F1-score to thoroughly assess your model's performance.  \n",
        "> - Additionally, take a moment to compare the sizes of the models discussed in this notebook with todayâ€™s enormous models. This will help you appreciate the challenges and computational demands associated with training such massive models. ðŸ˜µâ€ðŸ’«\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHKw2r6yYV7n"
      },
      "source": [
        "## Part 1: Data Preparation and Preprocessing\n",
        "We'll be working with the [Consumer Complaint](https://catalog.data.gov/dataset/consumer-complaint-database) dataset, which contains ***complaints*** submitted by consumers about financial products and services. Our goal is to build a classifier that can automatically identify the type of complaint based on the consumer's text description. For this task, we will work with a smaller subset of the dataset, available for download through this [link](https://drive.google.com/file/d/1SpIHksR-WzruEgUjp1SQKGG8bZPnJJoN/view?usp=sharing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ELMR8kXUh3o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcAIXYPZPd_j",
        "outputId": "03997089-8948-4782-cbb0-e6842230a96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oJXlKLYeymq"
      },
      "source": [
        "### 1.2 Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mGga8BmnUcl0",
        "outputId": "c632729e-44db-4e4e-c6b4-59645308f463"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"######################  TODO  ########################\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Product\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Credit reporting, credit repair services, or other personal consumer reports\",\n          \"Student loan\",\n          \"Credit reporting or other personal consumer reports\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Consumer complaint narrative\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Beginning in XX/XX/XXXX I had taken out student loans with the Department of Education. This was my first year of college. There was no classes or anything needed to be taken in order to receive these loans. Over the past 6 years I had changed colleges a couple times. After completing my XXXX degree my loans were bought out from XXXX XXXX to Navient. I was unaware and had no choices about this buy out. IN XX/XX/XXXX, they suggested me to consolidated to pay one payment. They never informed me that this would make my loan forgiveness go back to square one. In XX/XX/XXXX I applied for a XXXX degree which put my consolidated loans in to deferment. I do not know how much interest has accumulated over my loans, no one has ever gave me ADVICE on my loans to pay them off. I have had a job every year since I have been in college and no one has ever told me you can make payments on my loans while you are in college. ( THIS WOULD HAVE BEEN HELPFUL. EVEN IF IT WAS {$20.00} ) After I completed my XXXX  program. Another representative advised me to re consolidate both loans and now I can pay one payment. Only after I asked about my loan forgiveness date was I than informed it was recalculated to XX/XX/XXXX because I re consolidated. These people are liars and are making it IMPOSSIBLE to get out of student debt. I have {$120000.00} in loans and almost HALF of it is interest. And since I consolidated after completing my schooling after 10 years, I still have to pay another 20 years. Navient has no solutions. They do not help you and if you do not ask questions you will never be able to understand your loans. There are so many charges on my account I have no idea where to start and if all of this is accurate.\",\n          \"I am a federally protected consumer and I am aware of my rights laid out by the FCRA. XXXX aware that you as a consumer reporting agency must maintain accurate records at all times across all credit reporting agencies on consumers files. \\nPlease investigate the bellow accounts validity within my consumer report that you maintain under my name and update my report to your findings within 30 days of your receipt of this dispute notice as per FCRA guidelines. \\n\\nI have attached screenshots of the erroneous information within my report for this account. Please complete this account within 30 days and forward me the results. If this account can not be validated, DELETE from my consumer report. \\n\\nACCOUNT : XXXX XXXX XXXX Under 15 U.S. code 1681e ( b ) Accuracy of report Whenever a consumer reporting agency prepares a consumer report, it shall follow reasonable procedures to assure the maximum possible accuracy of the information concerning the individual about whom the report relates. \\n\\n15 U.S. Code 1681i ( 5 ) Treatment of inaccurate or unverifiable information ( A ) In general if, after any reinvestigation under paragraph ( 1 ) of any information disputed by a consumer, an item of information is found to be inaccurate or incomplete or can not be verified the consumer reporting agency shall -- - ( i ) promptly delete that item of information from the file of the consumer, or modify that item of information, as appropriate, based on the results of the reinvestigation; and ( ii ) promptly notify the furnisher of that information that the information has been modified or deleted from the file of the consumer.\\n\\nI have experienced much mental hardship over this erroneous account as it is prohibiting me from refinancing my home.\\n\\nStandards established in the credit industry were created to ensure integrity of data and uniform communication between ALL credit-reporting agencies to maintain maximum possible accuracy. If these standards are broken then liabilities can arise. \\nYour agency is obligated to conduct a thoughtful and fair investigation about this very important matter as I am attempting to handle this issue privately, now. \\nI have attached the erroneous account ( s ) in question. This account ( s ) needs to be DELETED due to inaccurate information reported within my consumer report if it can not be 100 % verified/validated within 30 days of your receipt of this dispute notice. Failure to do so could constitute willful non-compliance. \\nPlease DELETE this erroneous, inaccurate information from my consumer report within 30 days of your receipt of this letter if this information can not be made complete. \\n\\nThank you for your rapid response in this very important matter, XXXX XXXX.\",\n          \"I am disputing a charge-off on my account that remains inaccurate, despite my previous attempts to resolve it with the creditor. I have provided documentation and evidence to support my dispute, but the charge-off has not been corrected. This inaccuracy is negatively affecting my credit report and financial standing, and I am seeking assistance from the CFPB to ensure that the creditor accurately reflects the status of the account.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7960d079-0964-4ec0-92e8-216103319230\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Product</th>\n",
              "      <th>Consumer complaint narrative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>My credit reports are inaccurate. These inaccu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Student loan</td>\n",
              "      <td>Beginning in XX/XX/XXXX I had taken out studen...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Credit reporting or other personal consumer re...</td>\n",
              "      <td>I am disputing a charge-off on my account that...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Credit reporting, credit repair services, or o...</td>\n",
              "      <td>I did not consent to, authorize, nor benefit f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Credit reporting or other personal consumer re...</td>\n",
              "      <td>I am a federally protected consumer and I am a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7960d079-0964-4ec0-92e8-216103319230')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7960d079-0964-4ec0-92e8-216103319230 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7960d079-0964-4ec0-92e8-216103319230');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ecc344e9-84e0-4e98-b103-529dd441a2db\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ecc344e9-84e0-4e98-b103-529dd441a2db')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ecc344e9-84e0-4e98-b103-529dd441a2db button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                             Product  \\\n",
              "0  Credit reporting, credit repair services, or o...   \n",
              "1                                       Student loan   \n",
              "2  Credit reporting or other personal consumer re...   \n",
              "3  Credit reporting, credit repair services, or o...   \n",
              "4  Credit reporting or other personal consumer re...   \n",
              "\n",
              "                        Consumer complaint narrative  \n",
              "0  My credit reports are inaccurate. These inaccu...  \n",
              "1  Beginning in XX/XX/XXXX I had taken out studen...  \n",
              "2  I am disputing a charge-off on my account that...  \n",
              "3  I did not consent to, authorize, nor benefit f...  \n",
              "4  I am a federally protected consumer and I am a...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "# Load the dataset\n",
        "\n",
        "file_path = '/content/drive/MyDrive/complaints_small/complaints_small.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9hr8-FNgpVO"
      },
      "source": [
        "### 1.3 Data Sampling and Class Distribution Analysis\n",
        "\n",
        "Working with large datasets can be computationally intensive during development. Additionally, imbalanced class distribution can affect model performance. In this section, you'll sample the data and analyze class distributions to make informed decisions about your training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl_g_ZU4h5RG"
      },
      "source": [
        "---\n",
        "\n",
        "We'll work with a manageable portion of the data to develop and test our approach. While using the complete dataset would likely yield better results, a smaller sample allows us to prototype our solution more efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAJUXNCFhYsf",
        "outputId": "fa947f25-76df-4a7a-c483-b3e5a01423eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 5 rows of sampled dataset:\n",
            "                                                  Product  \\\n",
            "22598   Credit reporting, credit repair services, or o...   \n",
            "933208                        Credit card or prepaid card   \n",
            "157321                        Checking or savings account   \n",
            "87881   Credit reporting, credit repair services, or o...   \n",
            "587669  Credit reporting or other personal consumer re...   \n",
            "\n",
            "                             Consumer complaint narrative  \n",
            "22598   I do not recognize this XXXX  XXXX account on ...  \n",
            "933208  Barclay 's Bank U.S. closed my 10-year old acc...  \n",
            "157321  VOLATIONS Federal Reserve Act Section 16. Note...  \n",
            "87881   XXXX, XXXX and Transunion have violated my con...  \n",
            "587669  I have been trying to dispute incorrect and/or...  \n",
            "\n",
            "Shape of original dataset: (941128, 2)\n",
            "Shape of sampled dataset: (9411, 2)\n"
          ]
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# - Sample a portion of the complete dataset\n",
        "# - Display the first few rows of your sampled dataset\n",
        "# - Print the shape of your original and sampled datasets\n",
        "\n",
        "\n",
        "sampled_df = df.sample(frac=0.01, random_state=14)  # Set random_state for reproducibility\n",
        "\n",
        "# Display the first 5 rows of the sampled dataset\n",
        "print(\"First 5 rows of sampled dataset:\")\n",
        "print(sampled_df.head())\n",
        "\n",
        "# Print the shape of the original and sampled datasets\n",
        "print(\"\\nShape of original dataset:\", df.shape)\n",
        "print(\"Shape of sampled dataset:\", sampled_df.shape)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50a4NJeMiBb6"
      },
      "source": [
        "---\n",
        "\n",
        "Let's examine the distribution of ***complaints*** types in our dataset. You'll notice that some products have significantly more instances than others, and some categories are quite similar. For example:\n",
        "\n",
        "- Multiple categories might refer to similar financial products\n",
        "- Some categories might have very few examples\n",
        "- Certain categories might be subcategories of others\n",
        "\n",
        "You have two main approaches to handle this situation:\n",
        "\n",
        "1. **Merging Similar Classes:** Identify categories that represent similar products/services and Combine them to create more robust, general categories\n",
        "\n",
        "2. **Selecting Major Classes:** Only select the categories with sufficient representation\n",
        "\n",
        "\n",
        "\n",
        "> You may choose any approach, but after this step, your data must include **at least five** distinct classes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nby2Hrwwjd46",
        "outputId": "5184f9cd-7824-4a4c-e31d-a57e61713b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Complaint counts per category:\n",
            " Product\n",
            "Credit reporting, credit repair services, or other personal consumer reports    322966\n",
            "Credit reporting or other personal consumer reports                             252019\n",
            "Debt collection                                                                 117285\n",
            "Mortgage                                                                         49358\n",
            "Checking or savings account                                                      44580\n",
            "Credit card or prepaid card                                                      43575\n",
            "Credit card                                                                      24776\n",
            "Student loan                                                                     18742\n",
            "Money transfer, virtual currency, or money service                               17962\n",
            "Vehicle loan or lease                                                            13777\n",
            "Credit reporting                                                                 12641\n",
            "Payday loan, title loan, or personal loan                                         6940\n",
            "Bank account or service                                                           5920\n",
            "Consumer Loan                                                                     3881\n",
            "Prepaid card                                                                      2402\n",
            "Payday loan, title loan, personal loan, or advance loan                           2300\n",
            "Payday loan                                                                        723\n",
            "Debt or credit management                                                          593\n",
            "Money transfers                                                                    571\n",
            "Other financial service                                                            110\n",
            "Virtual currency                                                                     7\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Updated complaint counts:\n",
            " Product\n",
            "Credit reporting, credit repair services, or other personal consumer reports    322966\n",
            "Credit reporting or other personal consumer reports                             252019\n",
            "Debt collection                                                                 117285\n",
            "Mortgage                                                                         49358\n",
            "Checking or savings account                                                      44580\n",
            "Credit card or prepaid card                                                      43575\n",
            "Credit card                                                                      24776\n",
            "Student loan                                                                     18742\n",
            "Money transfer, virtual currency, or money service                               17962\n",
            "Vehicle loan or lease                                                            13777\n",
            "Credit reporting                                                                 12641\n",
            "Payday loan, title loan, or personal loan                                         6940\n",
            "Bank account or service                                                           5920\n",
            "Consumer Loan                                                                     3881\n",
            "Prepaid card                                                                      2402\n",
            "Payday loan, title loan, personal loan, or advance loan                           2300\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# 1. Display the number of complaints in each product category\n",
        "complaint_counts = df['Product'].value_counts()\n",
        "print(\"Complaint counts per category:\\n\", complaint_counts)\n",
        "\n",
        "# 2. Identify and Keep Major Classes (e.g., categories with at least 1000 complaints)\n",
        "major_classes = complaint_counts[complaint_counts >= 1000].index\n",
        "df = df[df['Product'].isin(major_classes)]\n",
        "\n",
        "# Display the updated complaint counts (after keeping major classes)\n",
        "updated_complaint_counts = df['Product'].value_counts()\n",
        "print(\"\\nUpdated complaint counts:\\n\", updated_complaint_counts)\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lD3oISsijt1P"
      },
      "source": [
        "---\n",
        "### 1.4 Data Encoding and Text Preprocessing\n",
        "\n",
        "Before training our model, we need to prepare both our target labels and text data. This involves converting categorical labels into numerical format and cleaning our text data to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAmaRU92mGyT"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# 1. Label Encoding\n",
        "# Create a label encoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the 'Product' column and transform it\n",
        "df['Product_encoded'] = label_encoder.fit_transform(df['Product'])\n",
        "\n",
        "# 2. Text Preprocessing\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove special characters and punctuation (keep only alphanumeric and spaces)\n",
        "    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)\n",
        "\n",
        "    # Remove very short complaints (less than 10 words)\n",
        "    if len(text.split()) < 10:\n",
        "        return ''  # Return empty string for short complaints\n",
        "\n",
        "    return text.lower()\n",
        "\n",
        "df['Consumer complaint narrative'] = df['Consumer complaint narrative'].apply(preprocess_text)\n",
        "df = df[df['Consumer complaint narrative'] != '']\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4jVvN4oopUU"
      },
      "source": [
        "## 1.5 Dataset Creation and Tokenization\n",
        "\n",
        "For training our BERT model, we need to:\n",
        "1. Create a custom Dataset class that will handle tokenization\n",
        "2. Split the data into training and testing sets\n",
        "3. Use BERT's tokenizer to convert text into a format suitable for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHLQgJhopEh5"
      },
      "outputs": [],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class ComplaintDataset(Dataset):\n",
        "    \"\"\"A custom Dataset class for handling consumer complaints text data with BERT tokenization.\n",
        "\n",
        "    Parameters:\n",
        "        texts (List[str]): List of complaint texts to be processed\n",
        "        labels (List[int]): List of encoded labels corresponding to each text\n",
        "        tokenizer (BertTokenizer): A BERT tokenizer instance for text processing\n",
        "        max_len (int, optional): Maximum length for padding/truncating texts. Defaults to 512\n",
        "\n",
        "    Returns:\n",
        "        dict: For each item, returns a dictionary containing:\n",
        "            - input_ids (torch.Tensor): Encoded token ids of the text\n",
        "            - attention_mask (torch.Tensor): Attention mask for the padded sequence\n",
        "            - labels (torch.Tensor): Encoded label as a tensor\n",
        "    \"\"\"\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize the text\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',  \n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-js5x8M5mksA",
        "outputId": "7c9870f9-817f-4d39-fbc6-4cf195ceb4fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# 'Consumer complaint narrative' is the text column,\n",
        "# and 'Product_encoded' is the encoded label column\n",
        "\n",
        "# 1. Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['Consumer complaint narrative'],\n",
        "    df['Product_encoded'],\n",
        "    test_size=0.2,\n",
        "    random_state=14\n",
        ")\n",
        "\n",
        "# 2. Initialize tokenizer and create datasets\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "train_dataset = ComplaintDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = ComplaintDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "\n",
        "# 3. Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
        "test_loader = DataLoader(test_dataset, batch_size=16)  \n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMcc2gsbt0iJ"
      },
      "source": [
        "## Part 2: Training a Small-Size BERT Model\n",
        "\n",
        "In this part, we will explore how to build and train a small-sized BERT model for our classification task. Instead of using the full-sized BERT model, which is computationally expensive, we will create a smaller version using the Transformers library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RS5oBz3qmvu",
        "outputId": "9cefe3bf-4e3a-4392-8027-d04fc510cb5b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total trainable parameters: 109,494,544\n"
          ]
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
        "\n",
        "# 1. Define the BERT model for sequence classification\n",
        "# Load the configuration for a smaller BERT model (e.g., 'bert-base-uncased')\n",
        "config = AutoConfig.from_pretrained(\"bert-base-uncased\", num_labels=len(label_encoder.classes_))\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=config)\n",
        "# 2. Print the total number of trainable parameters\n",
        "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Total trainable parameters: {total_params:,}\")\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr4Z14a6wL2c"
      },
      "source": [
        "---\n",
        "\n",
        "Now that you have defined your model, it's time to train it!â˜ ï¸\n",
        "\n",
        "Training a model of this size can take some time, depending on the available resources. To manage this, you can train your model for just **2â€“3 epochs** to demonstrate progress. Here are some hints:\n",
        "- **Training Metrics:** Ensure you print enough metrics, such as loss and accuracy, to track the training progress.\n",
        "- **Interactive Monitoring:** Use the `tqdm` library to display the progress of your training loop in real-time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRZW-F9Dw6AI",
        "outputId": "0f7ad5bd-a8d6-4cca-ef21-c4445c80a784"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1534/1534 [1:11:21<00:00,  2.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 1.0823, Accuracy: 0.6392\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1534/1534 [1:11:23<00:00,  2.79s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Loss: 0.7816, Accuracy: 0.7291\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 767/767 [05:53<00:00,  2.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.7184\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# 1. Define optimizer and number of epochs\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 2\n",
        "model.to(device)\n",
        "\n",
        "# 2. Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # For accuracy calculation\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    print(f\"Training Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# 3. Evaluation on test dataset\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad(): \n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_labels, all_predictions)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yHtTYcpz6AW"
      },
      "source": [
        "## Part 3: Fine-Tuning TinyBERT with LoRA\n",
        "\n",
        "As you have experienced, training even a small-sized BERT model can be computationally intensive and time-consuming. To address these challenges, we explore **Parameter-Efficient Fine-Tuning (PEFT)** methods, which allow us to utilize the power of large pretrained models without requiring extensive resources.\n",
        "\n",
        "---\n",
        "\n",
        "### **Parameter-Efficient Fine-Tuning (PEFT)**\n",
        "\n",
        "PEFT methods focus on fine-tuning only a small portion of the modelâ€™s parameters while keeping most of the pretrained weights frozen. This drastically reduces the computational and storage requirements while leveraging the rich knowledge embedded in pretrained models.\n",
        "\n",
        "One popular PEFT method is LoRA (Low-Rank Adaptation).\n",
        "\n",
        "- **What is LoRA?**\n",
        "\n",
        "LoRA introduces a mechanism to fine-tune large language models by injecting small low-rank matrices into the model's architecture. Instead of updating all parameters during training, LoRA trains these small matrices while keeping the majority of the original parameters frozen.  This is achieved as follows:\n",
        "\n",
        "1. **Frozen Weights**: The pretrained weights of the model, represented as a weight matrix $ W \\in \\mathbb{R}^{d \\times k} $, remain **frozen** during fine-tuning.\n",
        "\n",
        "2. **Low-Rank Decomposition**:\n",
        "   Instead of directly updating $ W $, LoRA introduces two trainable matrices, $ A \\in \\mathbb{R}^{d \\times r} $ and $ B \\in \\mathbb{R}^{r \\times k} $, where $ r \\ll \\min(d, k) $.  \n",
        "   These matrices approximate the update to $ W $ as:\n",
        "   $$\n",
        "   \\Delta W = A \\cdot B\n",
        "   $$\n",
        "\n",
        "   Here, $ r $, the rank of the decomposition, is a key hyperparameter that determines the trade-off between computational cost and model capacity.\n",
        "\n",
        "3. **Adaptation**:\n",
        "   During training, instead of updating $ W $, the adapted weight is:\n",
        "   $$\n",
        "   W' = W + \\Delta W = W + A \\cdot B\n",
        "   $$\n",
        "   Only the low-rank matrices $ A $ and $ B $ are optimized, while $ W $ remains fixed.\n",
        "\n",
        "4. **Efficiency**:\n",
        "   Since $ r $ is much smaller than $ d $ and $ k $, the number of trainable parameters in $ A $ and $ B $ is significantly less than in $ W $. This makes the approach highly efficient both in terms of computation and memory.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Fine-Tuning TinyBERT**\n",
        "\n",
        "For this part, we will fine-tune **TinyBERT**, a distilled version of BERT, using the LoRA method.\n",
        "\n",
        "- **What is TinyBERT?**\n",
        "\n",
        "TinyBERT is a lightweight version of the original BERT model created through knowledge distillation. It significantly reduces the model size and inference latency while preserving much of the original BERTâ€™s effectiveness. Here are some key characteristics of TinyBERT:\n",
        "- It is designed to be more resource-efficient for tasks such as classification, question answering, and more.\n",
        "- TinyBERT retains a compact structure with fewer layers and parameters, making it ideal for fine-tuning with limited computational resources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_Og-pBeV5x6"
      },
      "source": [
        "> Similar to the previous section, training this model might take some time. Given the resource limitations, you can train the model for just **2-3 epochs** to demonstrate the process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe1vGCZwU7MZ"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b127b31c0ba64477870097d43da921d2",
            "c0a961f2fba544d48c70517760761e00",
            "8895392e64d64998a2941acef42575d9",
            "d4475a5e2f74401cb33d7501db181487",
            "3b228a09ca5247a9bda134cf62272d72",
            "473c2bcfd3b849ea91693dea395681c8",
            "f0279bc4543b428b9f6ac5e092354d0a",
            "ff54d704a0c14928810d186bda868a00",
            "666f02f26556418c926c589f9da16788",
            "4c09a3dbb59f46edbac496f2b6e8fbb9",
            "5e90b43ed3fd492ebee69e20a40d956c",
            "b9300eb1022e4ce6bd4abf23722aab92",
            "3196219cfc054caea128d0eeac3f4b45",
            "7bd9c6b85004442894f229b35bbcaa54",
            "ca4cef16a08e4348a7ee8744a6e134ef",
            "b15a2ccee2464393814269c8d9831f46",
            "e4775bbe6cf54aa8bd6e6188d4d3464e",
            "f53c77d3aa52421f9133fa6db705152b",
            "284be2e4f27b48e786d0be8ee8b00024",
            "87ef3a4ce8314e47b77df215f33de96c",
            "a07936037b1b4b0984497bdd459e5682",
            "d8f57e53663d4e2c91fde17fc598d612",
            "1c1b1f49ca7a4e43b84264597fb5f3de",
            "9a32bc952f4a40ec83b0f05df361532d",
            "b20efb127aab47a5a3d378caeb681665",
            "24e046c562a742f6a175c0a6a675f49a",
            "294647ae4e444dc3958757b4c838c272",
            "8a17f2dc061c4eb58ffe0070c211c1ad",
            "2895a56edac0434eae4f05503d97b0b5",
            "6cb65dea7eaa441ba474b78c35de9c80",
            "37a7da6dad95413eacb2c3c440a498b4",
            "f7f91f2fd7924543abc0ad6bc7913901",
            "341d899313a7401896351e7a8ea7a43f"
          ]
        },
        "id": "LIyN5vOLLWz6",
        "outputId": "e6a2b7bf-e515-4db5-b131-a2e5f25fa00e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.31.0\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/116.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.4.0\n",
            "  Downloading peft-0.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (2.32.3)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (0.5.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.31.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0) (1.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.31.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.4.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.31.0) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0) (3.0.2)\n",
            "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.4.0-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.0\n",
            "    Uninstalling tokenizers-0.21.0:\n",
            "      Successfully uninstalled tokenizers-0.21.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.47.1\n",
            "    Uninstalling transformers-4.47.1:\n",
            "      Successfully uninstalled transformers-4.47.1\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.31.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed peft-0.4.0 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "463a95deb2e14d4896cf59e52f7bde17",
              "pip_warning": {
                "packages": [
                  "peft",
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b127b31c0ba64477870097d43da921d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/285 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9300eb1022e4ce6bd4abf23722aab92",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c1b1f49ca7a4e43b84264597fb5f3de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Load the pre-trained TinyBERT\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model_name = \"prajjwal1/bert-tiny\"\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) \n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define LoRA Configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  \n",
        "    lora_alpha=32,  \n",
        "    lora_dropout=0.1,  \n",
        "    bias=\"none\",  \n",
        "    task_type=\"SEQ_CLS\", \n",
        ")\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMgwZ8YmLuZ_",
        "outputId": "0b5926d7-bbf0-4456-d6d7-bd401158c81c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 8,450 || all params: 4,394,628 || trainable%: 0.1923\n",
            "Trainable parameters: 8450\n"
          ]
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "# Apply LoRA to model\n",
        "lora_model = get_peft_model(base_model, lora_config)\n",
        "lora_model.print_trainable_parameters()  \n",
        "\n",
        "# Show the number of trainable parameters\n",
        "print(f\"Trainable parameters: {lora_model.num_parameters(only_trainable=True)}\")\n",
        "\n",
        "from torch.optim import Adam\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = Adam(lora_model.parameters(), lr=1e-3) \n",
        "criterion = nn.CrossEntropyLoss() \n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "J395FrcWMbmx",
        "outputId": "c5265eac-dc5d-40f8-a2e9-5e503fcbd3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: torch.Size([32, 512])\n",
            "Max sequence length: 512\n",
            "Labels: tensor([ 6,  6,  7,  7,  7,  7,  6,  7,  6, 15,  6,  7,  6,  1,  7, 11,  8,  6,\n",
            "         7,  6,  7,  9,  8,  7, 10,  7,  6,  6,  6,  7, 10,  7])\n",
            "Device: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1534 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a05dd9585a82>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Now move the truncated tensors to the device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ],
      "source": [
        "######################  TODO  ########################\n",
        "######################  TODO  ########################\n",
        "\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(lora_model.parameters(), lr=5e-5)  \n",
        "num_epochs = 2\n",
        "\n",
        "lora_model.to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    lora_model.train()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Get the maximum sequence length allowed by the model\n",
        "        max_seq_len = lora_model.config.max_position_embeddings\n",
        "\n",
        "        batch['input_ids'] = batch['input_ids'][:, :max_seq_len]\n",
        "        batch['attention_mask'] = batch['attention_mask'][:, :max_seq_len]\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = lora_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(outputs.logits, 1)\n",
        "        correct_predictions += (predicted_labels == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = correct_predictions / total_samples\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Evaluation on test dataset\n",
        "lora_model.eval()\n",
        "total_loss = 0\n",
        "correct_predictions = 0\n",
        "total_samples = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = lora_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        _, predicted_labels = torch.max(outputs.logits, 1)\n",
        "        correct_predictions += (predicted_labels == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "avg_loss = total_loss / len(test_loader)\n",
        "accuracy = correct_predictions / total_samples\n",
        "print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "######################  TODO  ########################\n",
        "######################  TODO  ########################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I don't know the error :)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c1b1f49ca7a4e43b84264597fb5f3de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a32bc952f4a40ec83b0f05df361532d",
              "IPY_MODEL_b20efb127aab47a5a3d378caeb681665",
              "IPY_MODEL_24e046c562a742f6a175c0a6a675f49a"
            ],
            "layout": "IPY_MODEL_294647ae4e444dc3958757b4c838c272"
          }
        },
        "24e046c562a742f6a175c0a6a675f49a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7f91f2fd7924543abc0ad6bc7913901",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_341d899313a7401896351e7a8ea7a43f",
            "value": "â€‡232k/232kâ€‡[00:00&lt;00:00,â€‡4.00MB/s]"
          }
        },
        "284be2e4f27b48e786d0be8ee8b00024": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2895a56edac0434eae4f05503d97b0b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "294647ae4e444dc3958757b4c838c272": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3196219cfc054caea128d0eeac3f4b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4775bbe6cf54aa8bd6e6188d4d3464e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f53c77d3aa52421f9133fa6db705152b",
            "value": "pytorch_model.bin:â€‡100%"
          }
        },
        "341d899313a7401896351e7a8ea7a43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37a7da6dad95413eacb2c3c440a498b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b228a09ca5247a9bda134cf62272d72": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473c2bcfd3b849ea91693dea395681c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c09a3dbb59f46edbac496f2b6e8fbb9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e90b43ed3fd492ebee69e20a40d956c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "666f02f26556418c926c589f9da16788": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cb65dea7eaa441ba474b78c35de9c80": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bd9c6b85004442894f229b35bbcaa54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_284be2e4f27b48e786d0be8ee8b00024",
            "max": 17756393,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_87ef3a4ce8314e47b77df215f33de96c",
            "value": 17756393
          }
        },
        "87ef3a4ce8314e47b77df215f33de96c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8895392e64d64998a2941acef42575d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff54d704a0c14928810d186bda868a00",
            "max": 285,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_666f02f26556418c926c589f9da16788",
            "value": 285
          }
        },
        "8a17f2dc061c4eb58ffe0070c211c1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a32bc952f4a40ec83b0f05df361532d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a17f2dc061c4eb58ffe0070c211c1ad",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2895a56edac0434eae4f05503d97b0b5",
            "value": "vocab.txt:â€‡100%"
          }
        },
        "a07936037b1b4b0984497bdd459e5682": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b127b31c0ba64477870097d43da921d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0a961f2fba544d48c70517760761e00",
              "IPY_MODEL_8895392e64d64998a2941acef42575d9",
              "IPY_MODEL_d4475a5e2f74401cb33d7501db181487"
            ],
            "layout": "IPY_MODEL_3b228a09ca5247a9bda134cf62272d72"
          }
        },
        "b15a2ccee2464393814269c8d9831f46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b20efb127aab47a5a3d378caeb681665": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cb65dea7eaa441ba474b78c35de9c80",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_37a7da6dad95413eacb2c3c440a498b4",
            "value": 231508
          }
        },
        "b9300eb1022e4ce6bd4abf23722aab92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3196219cfc054caea128d0eeac3f4b45",
              "IPY_MODEL_7bd9c6b85004442894f229b35bbcaa54",
              "IPY_MODEL_ca4cef16a08e4348a7ee8744a6e134ef"
            ],
            "layout": "IPY_MODEL_b15a2ccee2464393814269c8d9831f46"
          }
        },
        "c0a961f2fba544d48c70517760761e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_473c2bcfd3b849ea91693dea395681c8",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f0279bc4543b428b9f6ac5e092354d0a",
            "value": "config.json:â€‡100%"
          }
        },
        "ca4cef16a08e4348a7ee8744a6e134ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a07936037b1b4b0984497bdd459e5682",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d8f57e53663d4e2c91fde17fc598d612",
            "value": "â€‡17.8M/17.8Mâ€‡[00:00&lt;00:00,â€‡35.5MB/s]"
          }
        },
        "d4475a5e2f74401cb33d7501db181487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c09a3dbb59f46edbac496f2b6e8fbb9",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5e90b43ed3fd492ebee69e20a40d956c",
            "value": "â€‡285/285â€‡[00:00&lt;00:00,â€‡19.5kB/s]"
          }
        },
        "d8f57e53663d4e2c91fde17fc598d612": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4775bbe6cf54aa8bd6e6188d4d3464e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0279bc4543b428b9f6ac5e092354d0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f53c77d3aa52421f9133fa6db705152b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7f91f2fd7924543abc0ad6bc7913901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff54d704a0c14928810d186bda868a00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
