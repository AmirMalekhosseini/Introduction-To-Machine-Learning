{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "be138f6f",
    "papermill": {
     "duration": 0.02181,
     "end_time": "2024-12-18T17:13:30.324107",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.302297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Knowdge Distillation using Contrastive Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "376ecc79",
    "papermill": {
     "duration": 0.014958,
     "end_time": "2024-12-18T17:13:30.355437",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.340479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " #### Student Name:\n",
    "\n",
    "\n",
    " #### Student ID:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "a1dba4d2",
    "papermill": {
     "duration": 0.014965,
     "end_time": "2024-12-18T17:13:30.385132",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.370167",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this exercise, we aim to distill knowledge from a large monolingual model into a smaller multilingual model using contrastive learning, specifically leveraging the CLIP model loss.\n",
    "\n",
    "We employ a small paired English-Persian dataset to define the loss pairs for our CLIP training. Given the substantial dataset size and batch size typically required for CLIP's loss computation (exceeding 19,000 samples per batch in standard tasks), which is impractical for our setup on Colab, we use a reduced batch size to focus on learning the procedure rather than achieving optimal performance, so we don't expect actual real-world results, only the training prcodure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "90e95f9e",
    "papermill": {
     "duration": 0.014841,
     "end_time": "2024-12-18T17:13:30.414686",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.399845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "CLIP (Contrastive Language-Image Pretraining) is a foundational model introduced by OpenAI to bridge the gap between text and image modalities. By aligning text descriptions and corresponding images in a shared embedding space, CLIP achieves remarkable zero-shot generalization capabilities across a wide range of tasks. It is trained on a massive dataset of image-text pairs using contrastive loss, ensuring that image embeddings align closely with their corresponding textual descriptions while remaining distinct from unrelated samples. This cross-modal alignment enables CLIP to perform tasks like image retrieval, captioning, and classification with minimal fine-tuning.\n",
    "\n",
    "The CLIP loss plays a crucial role in training the model by implementing a cross-entropy loss function in the contrastive learning framework. This loss operates on paired data, where each image-text pair is treated as a positive match, while all other pair combinations in the batch are considered negatives. The loss ensures that positive pairs receive high similarity scores, while negatives are penalized. However, achieving optimal results with CLIP loss often requires large batch sizes to provide sufficient negative samples, which can be computationally intensive. This makes training with limited resources challenging, necessitating adaptations such as smaller batch sizes or alternative strategies to approximate the training dynamics.\n",
    "\n",
    "\n",
    "Knowledge distillation is a technique used in machine learning to transfer knowledge from a large, complex model (the \"teacher\") to a smaller, more efficient model (the \"student\"). The primary goal of this process is to retain the performance and accuracy of the larger model while significantly reducing computational and memory requirements. This is achieved by training the student model to mimic the outputs of the teacher model, often through techniques such as matching soft probability distributions or intermediate representations. Knowledge distillation has become an essential approach in deploying machine learning models on resource-constrained devices such as smartphones and edge devices.\n",
    "\n",
    "In practice, knowledge distillation is not limited to replicating predictions; it can also involve transferring knowledge about internal features or learned representations.\n",
    "\n",
    "\n",
    "### Challenges in Resource-Constrained Settings\n",
    "CLIP’s reliance on large-scale datasets and batch sizes makes direct implementation computationally demanding. This exercise demonstrates an adaptation of the process, reducing batch size and dataset size to provide a practical understanding of the training procedure. While this approach sacrifices performance and real-world applicability, it highlights the mechanics of using CLIP loss for contrastive learning and lays the foundation for extending the process to larger datasets and batch sizes in future applications.\n",
    "\n",
    "\n",
    "\n",
    "### About CLIP and Contrastive Learning\n",
    "CLIP, developed by OpenAI, bridges the gap between text and image modalities by aligning corresponding embeddings in a shared space. It leverages contrastive loss to train on image-text pairs, ensuring that embeddings of positive pairs (e.g., an image and its corresponding caption) are highly similar, while embeddings of unrelated pairs remain distinct. The cross-entropy-based contrastive loss evaluates the similarity between positive pairs while penalizing mismatches for all other combinations within a batch.\n",
    "\n",
    "### Key aspects of CLIP loss include:\n",
    "\n",
    "- Positive Pairing: Encourages high similarity scores for embeddings of paired text and image data.\n",
    "- Negative Sampling: Penalizes mismatched pairs within the batch, requiring large batch sizes for effective performance due to the need for a diverse set of negative samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "id": "ba7c7274",
    "papermill": {
     "duration": 0.01457,
     "end_time": "2024-12-18T17:13:30.444777",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.430207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## !!! Note !!! You Should Answer to all the TODOs\n",
    "\n",
    "Also feel free to ask your questions on Quera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "e7da461e",
    "papermill": {
     "duration": 0.014905,
     "end_time": "2024-12-18T17:13:30.474341",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.459436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "06e48967",
    "papermill": {
     "duration": 0.014958,
     "end_time": "2024-12-18T17:13:30.503840",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.488882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We install Required Packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69339c7a",
    "outputId": "784d6905-fc77-4161-999a-a4f723d9d0b3",
    "papermill": {
     "duration": 20.264414,
     "end_time": "2024-12-18T17:13:50.783672",
     "exception": false,
     "start_time": "2024-12-18T17:13:30.519258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown\n",
    "!gdown \"https://drive.google.com/uc?id=1MVx_gIkX4tQ8ya2OsHt0mqLmw1Pf2CcK\"\n",
    "!gdown \"https://drive.google.com/uc?id=1Co-dwJfWw-C_ral0hoAS_X94wN-_vbCj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70d93c5e",
    "outputId": "98aad28f-dd0f-4aec-8a74-356a090e9aff",
    "papermill": {
     "duration": 24.516699,
     "end_time": "2024-12-18T17:14:15.316512",
     "exception": false,
     "start_time": "2024-12-18T17:13:50.799813",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install setuptools\n",
    "import sys\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "\n",
    "def installPackages(packages):\n",
    "    def installPackage(package):\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "    for package in REQUIRED_PACKAGES:\n",
    "        try:\n",
    "            dist = pkg_resources.get_distribution(package)\n",
    "            print('{} ({}) is installed'.format(dist.key, dist.version))\n",
    "        except pkg_resources.DistributionNotFound:\n",
    "            print('{} is NOT installed'.format(package))\n",
    "            installPackage(package)\n",
    "            print('{} was successfully installed.'.format(package))\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'open_clip-torch',\n",
    "    'pandas',\n",
    "    'numpy',\n",
    "    'matplotlib',\n",
    "    'transformers',\n",
    "    'tqdm',\n",
    "    'torch',\n",
    "    'datasets',\n",
    "]\n",
    "\n",
    "installPackages(REQUIRED_PACKAGES)\n",
    "\n",
    "import gc\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import random\n",
    "import string\n",
    "import uuid\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import open_clip\n",
    "from open_clip import model as TE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from datasets import load_dataset, Dataset, Features, Array2D, Value\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModel, TFAutoModel\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {
    "id": "f4c39c86",
    "papermill": {
     "duration": 0.01563,
     "end_time": "2024-12-18T17:14:15.349709",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.334079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Beware to use cuda for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd519039",
    "outputId": "1a8e3de8-9c79-475d-a6a6-5df278542bfa",
    "papermill": {
     "duration": 0.024557,
     "end_time": "2024-12-18T17:14:15.389794",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.365237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDevice(which=\"cuda:0\", yellAtCpu=True):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(which)\n",
    "    else:\n",
    "        if yellAtCpu:\n",
    "             raise Exception(\"I won't run on CPU!\")\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    return device\n",
    "\n",
    "print(f\"Device: {getDevice()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "67a2eff9",
    "papermill": {
     "duration": 0.016217,
     "end_time": "2024-12-18T17:14:15.422727",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.406510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {
    "id": "75c787a8",
    "papermill": {
     "duration": 0.016978,
     "end_time": "2024-12-18T17:14:15.455652",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.438674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "These are our training configurations, read them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "802d832e",
    "papermill": {
     "duration": 0.11541,
     "end_time": "2024-12-18T17:14:15.591685",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.476275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Configs():\n",
    "    return {\n",
    "        \"device\": getDevice(),\n",
    "        \"reference_checkPoint\" : \"EVA02-E-14-plus\",                # teacher\n",
    "        \"candidate_checkpoint\" : \"setu4993/smaller-LaBSE\",         # student\n",
    "        \"train_path\" : \"train.csv\",\n",
    "        \"val_path\" : \"val.csv\",\n",
    "        \"save_path\" : \"./best-model.pth\",\n",
    "        \"english\" : \"en\",                                         # dont mind them\n",
    "        \"persian\" : \"fa\",\n",
    "        \"batch_size\": 128,                                        # should have been really big, but we can't here\n",
    "        \"lr\": 1e-4,\n",
    "        \"epochs\": 5,                                               # 40 minute per epoch\n",
    "        \"tok_percentile\" : 99,\n",
    "        \"temperature\": 20,\n",
    "        \"dropout\": 0.05,\n",
    "        \"unfreezed_layers\" : 10,\n",
    "        \"weight_decay\": 1e-5,\n",
    "        \"patience\": 1,\n",
    "        \"factor\" : 0.8,\n",
    "        \"reference_embedding\": 1024,                               # DONT MIND THESE\n",
    "        \"reference_context_length\" : 77,\n",
    "        \"reference_vocab_size\" : 49408,\n",
    "        \"reference_heads\" : 20,\n",
    "        \"reference_width\" : 1280,\n",
    "        \"reference_layers\" : 32,\n",
    "        \"cls_token_index\" : 0,\n",
    "        \"project_to\" : 1024,\n",
    "    }\n",
    "\n",
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "7b95e360",
    "papermill": {
     "duration": 0.015595,
     "end_time": "2024-12-18T17:14:15.623791",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.608196",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question Box\n",
    "### TODO (10pts)\n",
    "1- Why do we use temperature in training using contrastive learning?<br/><br/>\n",
    "\n",
    "  In contrastive learning, temperature is a hyperparameter used to scale the logits before they are passed to the softmax function. The purpose of the temperature parameter is to control the softness of the probability distribution produced by the softmax function, so that it could help with better separation of positive pairs from negative pairs. Higher temperature leads to a softer distribution, and lower temperature leads to a harder distribution. Temperature is crucial to tune the hardness or softness of the distribution in similarity space.\n",
    "<br/><br/>\n",
    "2- Why do we need to freeze some layers of a model? mention 2 reasons.<br/><br/>\n",
    "  Freezing layers means making their weights untrainable during backpropagation. This is done for several reasons:\n",
    "\n",
    "    Reason 1: Preserve Pre-trained Knowledge: Pre-trained models often come with valuable knowledge about general features. Freezing initial layers prevents their weights from being drastically altered, preserving this knowledge and allowing the model to focus on learning task-specific features in the later layers.\n",
    "    Reason 2: Reduce Computational Cost: Freezing layers reduces the number of parameters being updated during training, leading to faster training and lower memory requirements. This is particularly important when fine-tuning large pre-trained models on limited resources.\n",
    "\n",
    "\n",
    "<br/>\n",
    "3- Read the whole code and find out what tok_percentile is used for.<br/><br/>\n",
    "\n",
    "tok_percentile is used to determine the maximum length of input sequences after tokenization. It is set to 99, meaning that 99% of the sentences in the training set are at or below the chosen token length, and only 1% that might be longer will be truncated. This helps ensure that the vast majority of sentences in the training dataset are included during training without exceeding a defined token length. This helps in preventing very long input texts and maintain uniform input lengths for the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {
    "id": "6a9cfd8b",
    "papermill": {
     "duration": 0.015679,
     "end_time": "2024-12-18T17:14:15.655199",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.639520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "id": "5d84bc5a",
    "papermill": {
     "duration": 0.031369,
     "end_time": "2024-12-18T17:14:15.702439",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.671070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getDatasetsCSV(prevEnCol, prevFaCol, newEnCol, newFaCol, trainPath, valPath):\n",
    "    df = pd.read_csv(trainPath)\n",
    "    dfVal = pd.read_csv(valPath)\n",
    "\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Training dataset is empty or missing\")\n",
    "\n",
    "    if dfVal.empty:\n",
    "        raise ValueError(\"Validation dataset is empty or missing\")\n",
    "\n",
    "    dfTraind = df.loc[:, [prevEnCol, prevFaCol]].rename(columns={prevEnCol: newEnCol, prevFaCol: newFaCol})\n",
    "    dfVal = dfVal.loc[:, [prevEnCol, prevFaCol]].rename(columns={prevEnCol: newEnCol, prevFaCol: newFaCol})\n",
    "\n",
    "    datasetTrain = Dataset.from_pandas(dfTraind)\n",
    "    datasetVal = Dataset.from_pandas(dfVal)\n",
    "\n",
    "    return datasetTrain, datasetVal\n",
    "\n",
    "def getDsByLang(persianCol, englishCol):\n",
    "    def getPersianDs(dataset):\n",
    "        return dataset[persianCol]\n",
    "\n",
    "    def getEnglishDs(dataset):\n",
    "        return dataset[englishCol]\n",
    "\n",
    "    getPersianDs.label = persianCol\n",
    "    getEnglishDs.label = englishCol\n",
    "\n",
    "    return getPersianDs, getEnglishDs\n",
    "\n",
    "class Normalizer():\n",
    "    def __init__(self):\n",
    "        translation_src = ' ىكي“”0123456789%إأآئيؤةك'\n",
    "        translation_dst = ' یکی\"\"۰۱۲۳۴۵۶۷۸۹٪اااییوهک'\n",
    "\n",
    "        self.translations = str.maketrans(translation_src, translation_dst)\n",
    "\n",
    "        patterns = [\n",
    "            (r' {2,}', ' '),  # remove extra spaces\n",
    "            (r'\\n+', ' '),  # replace newlines with space\n",
    "            (r'\\u200c+', ' '),  # replace ZWNJs with space\n",
    "            (r'[ـ\\r]', '')  # remove keshide, carriage returns\n",
    "        ]\n",
    "\n",
    "        self.character_refinement_patterns = [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
    "\n",
    "    def normalizeFa(self, text):\n",
    "        text = text.lower().translate(self.translations)\n",
    "        text = re.sub('[^a-zA-Z۰-۹آ-ی ]', ' ', text)\n",
    "\n",
    "        for pattern, repl in self.character_refinement_patterns:\n",
    "            text = pattern.sub(repl, text)\n",
    "        return text.strip()\n",
    "\n",
    "    def normalizeEn(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation\n",
    "        return text\n",
    "\n",
    "def applyPreprocess(datasets, configs=configs, Normalizer=Normalizer):\n",
    "    def applyRowNormalization(example):\n",
    "        example[configs['persian']] = normalizer.normalizeFa(example[configs['persian']])\n",
    "        example[configs['english']] = normalizer.normalizeEn(example[configs['english']])\n",
    "\n",
    "        return example\n",
    "\n",
    "    normalizer = Normalizer()\n",
    "\n",
    "    newDatasets = []\n",
    "    for dataset in datasets:\n",
    "        newDatasets.append(dataset.map(applyRowNormalization))\n",
    "\n",
    "    return newDatasets\n",
    "\n",
    "def preprocessSentence(text, lang, mostFreq=None, Normalizer=Normalizer, configs=configs):\n",
    "    normalizer = Normalizer()\n",
    "    if lang == configs['persian']:\n",
    "        normalized = normalizer.normalizeFa(text)\n",
    "    elif lang == configs['english']:\n",
    "        normalized = normalizer.normalizeEn(text)\n",
    "    else:\n",
    "        raise ValueError(\"Not supported lang\")\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "86190e87",
    "papermill": {
     "duration": 0.016557,
     "end_time": "2024-12-18T17:14:15.735930",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.719373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Utils\n",
    "### TODO: Complete these Utility functions (10pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "7b0c992a",
    "papermill": {
     "duration": 0.025354,
     "end_time": "2024-12-18T17:14:15.778667",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.753313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getClsToken(tensor, configs=configs):\n",
    "    \"\"\"\n",
    "    Extracts the classification (CLS) token from the input tensor.\n",
    "\n",
    "    Parameters:\n",
    "        tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, hidden_dim).\n",
    "        configs (dict): A dictionary containing configuration settings. Must include the key \"cls_token_index\"\n",
    "                        which specifies the index of the CLS token in the sequence dimension.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the CLS token for each example in the batch,\n",
    "                      of shape (batch_size, 1, hidden_dim).\n",
    "    \"\"\"\n",
    "    clsId = configs[\"cls_token_index\"]\n",
    "    return tensor[:, clsId, :].unsqueeze(1)\n",
    "\n",
    "def flattenMiddle(tensor):\n",
    "    \"\"\"\n",
    "    Flattens the middle dimension (sequence length) of the input tensor, removing it.\n",
    "\n",
    "    Parameters:\n",
    "        tensor (torch.Tensor): The input tensor of shape (batch_size, seq_length, hidden_dim).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, hidden_dim) with the middle dimension flattened out.\n",
    "    \"\"\"\n",
    "    return tensor.squeeze(1)\n",
    "\n",
    "def freezeModel(model):\n",
    "    \"\"\"\n",
    "    Freeze all parameters of a given model.\n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The PyTorch model whose parameters are to be frozen.\n",
    "    \"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "id": "2489bf4f",
    "papermill": {
     "duration": 0.026945,
     "end_time": "2024-12-18T17:14:15.822548",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.795603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotMetric(metricData, metricName):\n",
    "    if metricName == None or metricName not in metricData:\n",
    "        raise ValueError(\"No such metric\")\n",
    "    metricData[metricName].plot()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metricName)\n",
    "    plt.title(f'Plot of {metricName}')\n",
    "    plt.show()\n",
    "\n",
    "threshold=1\n",
    "def calcPrcentileTokens(dataset, tokenizer, field, percentile=configs[\"tok_percentile\"], thershold=1):\n",
    "    \"\"\"\n",
    "    Calculate the token length at a specific percentile for a dataset field.\n",
    "\n",
    "    This function tokenizes the data in the specified field of the dataset and calculates\n",
    "    the token length at the given percentile. An optional threshold can be added to the result.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (dict or Dataset): The dataset containing the data to be tokenized.\n",
    "        tokenizer (callable): A tokenizer function or object with a callable interface\n",
    "                              (e.g., HuggingFace tokenizer).\n",
    "        field (str): The field in the dataset whose token lengths are to be calculated.\n",
    "        percentile (float, optional): The percentile to compute (default is the value in\n",
    "                                       `configs[\"tok_percentile\"]`).\n",
    "        thershold (int, optional): A value to add to the calculated percentile token length\n",
    "                                    (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        int: The token length at the specified percentile plus the threshold.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If the specified field does not exist in the dataset.\n",
    "        TypeError: If `tokenized` is not in the expected format.\n",
    "\n",
    "    Example:\n",
    "        dataset = {\"text\": [\"This is a sentence.\", \"Another example sentence.\"]}\n",
    "        tokenizer = lambda x: {\"input_ids\": [[1, 2, 3, 4], [5, 6, 7, 8, 9]]}\n",
    "        field = \"text\"\n",
    "        calcPrcentileTokens(dataset, tokenizer, field, percentile=95, thershold=2)\n",
    "\n",
    "    Notes:\n",
    "        - If the tokenized output is a dictionary (e.g., HuggingFace tokenizers), it assumes\n",
    "          that `input_ids` contains the token sequences.\n",
    "        - If the tokenized output is a tensor, nonzero token counts are used to determine lengths.\n",
    "    \"\"\"\n",
    "    tokenized = tokenizer(dataset[field])\n",
    "    if not isinstance(tokenized, torch.Tensor):\n",
    "        tokenLengths = list(map(lambda sen: len(sen), tokenized['input_ids']))\n",
    "    else:\n",
    "        tokenLengths = [tensor.nonzero().size(0) for tensor in tokenized]\n",
    "    percentileLength = np.percentile(tokenLengths, percentile)\n",
    "    return int(percentileLength) + 1\n",
    "\n",
    "# Dont Touch This\n",
    "def TextEncoder(configs):\n",
    "    newModel = TE.TextTransformer(context_length=configs['reference_context_length'],\n",
    "                                 vocab_size=configs[\"reference_vocab_size\"],\n",
    "                                 width=configs[\"reference_width\"],\n",
    "                                 layers=configs[\"reference_layers\"],\n",
    "                                 heads=configs[\"reference_heads\"],\n",
    "                                 output_dim=configs[\"reference_embedding\"])\n",
    "    return newModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {
    "id": "ee90637f",
    "papermill": {
     "duration": 0.016248,
     "end_time": "2024-12-18T17:14:15.854449",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.838201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "7f75d675",
    "papermill": {
     "duration": 0.016386,
     "end_time": "2024-12-18T17:14:15.886639",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.870253",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TODO: Compelete the Swish and LinearProjection functions based on the pydoc provided (15pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "83acbdd5",
    "papermill": {
     "duration": 0.02619,
     "end_time": "2024-12-18T17:14:15.929057",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.902867",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Swish activation function.\n",
    "    Parameters:\n",
    "        beta (float, optional): The scaling parameter for the input x in the sigmoid\n",
    "                                function. Default is 1.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(self.beta * x)\n",
    "\n",
    "class LinearProjection(nn.Module):\n",
    "    \"\"\"\n",
    "    A projection layer with Swish activation, batch normalization, dropout, and residual connections.\n",
    "\n",
    "    This module takes an input tensor, applies a series of transformations, and produces an\n",
    "    output tensor of the same shape, making use of residual connections and layer normalization.\n",
    "\n",
    "    Parameters:\n",
    "        embedding_dim (int): The dimensionality of the input embeddings.\n",
    "        projection_dim (int, optional): The dimensionality of the projection. Default is\n",
    "                                        `configs['project_to']`.\n",
    "        dropout (float, optional): The dropout rate. Default is `configs['dropout']`.\n",
    "\n",
    "    Layers:\n",
    "        - projection: Linear layer that projects the input to the specified `projection_dim`.\n",
    "        - swish: Swish activation function with a fixed beta of 1.0.\n",
    "        - batch_norm: Batch normalization applied after projection.\n",
    "        - fc: Fully connected layer for further transformations.\n",
    "        - dropout: Dropout applied to the output of the fully connected layer.\n",
    "        - layer_norm: Layer normalization applied after residual connection.\n",
    "\n",
    "    Methods:\n",
    "        forward(x):\n",
    "            Applies the projection, activation, normalization, dropout, residual connection,\n",
    "            and layer normalization to the input tensor.\n",
    "\n",
    "    Example:\n",
    "        model = LinearProjection(embedding_dim=128, projection_dim=64, dropout=0.1)\n",
    "        output = model(input_tensor)\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim, projection_dim=configs['project_to'], dropout=configs['dropout']):\n",
    "        super(LinearProjection, self).__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.swish = Swish(beta=1.0)\n",
    "        self.batch_norm = nn.BatchNorm1d(projection_dim)\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.swish(projected)\n",
    "        x = self.batch_norm(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_norm(x + projected)  # Residual connection\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "ece3abb5",
    "papermill": {
     "duration": 0.024135,
     "end_time": "2024-12-18T17:14:15.969481",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.945346",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CandidateModel(nn.Module):\n",
    "    def __init__(self, model_name, unfreezeLayers, trainable=True):\n",
    "        super().__init__()\n",
    "        self.candidateProjection = LinearProjection(embedding_dim=configs[\"candidate_embedding\"])\n",
    "        self.configs = AutoConfig.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.batchNorm = nn.BatchNorm1d(1, configs[\"candidate_embedding\"])\n",
    "        self.targetTokenIdx = configs[\"cls_token_index\"]\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        clsEmbed = getClsToken(output.last_hidden_state)\n",
    "        clsEmbed = self.batchNorm(clsEmbed)\n",
    "        clsEmbed = self.candidateProjection(flattenMiddle(clsEmbed))\n",
    "        return clsEmbed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "4e0dd33e",
    "papermill": {
     "duration": 0.015974,
     "end_time": "2024-12-18T17:14:16.001750",
     "exception": false,
     "start_time": "2024-12-18T17:14:15.985776",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "92d73eeb",
    "papermill": {
     "duration": 0.015543,
     "end_time": "2024-12-18T17:14:16.033408",
     "exception": false,
     "start_time": "2024-12-18T17:14:16.017865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TODO: Compelete the calcLoss functions based on the pydoc provided (20pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "id": "c8e57071",
    "papermill": {
     "duration": 0.024464,
     "end_time": "2024-12-18T17:14:16.074093",
     "exception": false,
     "start_time": "2024-12-18T17:14:16.049629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calcLoss(batch, referenceModel, candidateModel, temperature):\n",
    "    \"\"\"\n",
    "    Compute the loss and the number of correct predictions for a contrastive learning task.\n",
    "\n",
    "    This function calculates a symmetric cross-entropy loss between the embeddings\n",
    "    generated by a reference model and a candidate model. It also computes the\n",
    "    number of correct predictions based on the alignment of the embeddings.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A batch of data containing:\n",
    "            - \"candidate\" (torch.Tensor): Tokenized input for the candidate model.\n",
    "            - \"reference\" (torch.Tensor): Tokenized input for the reference model.\n",
    "        referenceModel (nn.Module): The model generating embeddings for the reference input.\n",
    "        candidateModel (nn.Module): The model generating embeddings for the candidate input.\n",
    "        temperature (float): A scaling factor to control the logits' sharpness during similarity calculation.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - loss (torch.Tensor): The computed symmetric cross-entropy loss.\n",
    "            - corrects (int): The number of correctly predicted alignments.\n",
    "\n",
    "    Notes:\n",
    "        - The embeddings are normalized to ensure their magnitudes do not impact similarity.\n",
    "        - Logits represent scaled cosine similarity between reference and candidate embeddings.\n",
    "        - The targets are identity matrices, assuming perfect alignment between reference and candidate inputs.\n",
    "\n",
    "    Example Usage:\n",
    "        batch = {\n",
    "            \"candidate\": tokenized_candidate,\n",
    "            \"reference\": tokenized_reference\n",
    "        }\n",
    "        loss, corrects = calcLoss(batch, reference_model, candidate_model, temperature=0.1)\n",
    "    \"\"\"\n",
    "    # Move tokenized inputs to the specified device\n",
    "    candidateTokenized = batch[\"candidate\"].to(configs[\"device\"])\n",
    "    referenceTokenized = batch[\"reference\"].to(configs[\"device\"])\n",
    "\n",
    "    # Generate embeddings from reference and candidate models\n",
    "    referenceEmbeds = referenceModel(referenceTokenized)\n",
    "    candidateEmbeds = candidateModel(\n",
    "        input_ids=candidateTokenized[\"input_ids\"],\n",
    "        attention_mask=candidateTokenized[\"attention_mask\"]\n",
    "    )\n",
    "\n",
    "    # Normalize embeddings to have unit length\n",
    "    referenceEmbeds = F.normalize(referenceEmbeds, p=2, dim=-1)\n",
    "    candidateEmbeds = F.normalize(candidateEmbeds, p=2, dim=-1)\n",
    "\n",
    "    # Calculate logits (scaled cosine similarity)\n",
    "    logits = torch.matmul(candidateEmbeds, referenceEmbeds.t()) / temperature\n",
    "\n",
    "    # Define targets (identity matrix)\n",
    "    batch_size = candidateEmbeds.shape[0]\n",
    "    targets = torch.arange(batch_size, device=configs[\"device\"])\n",
    "\n",
    "    # Calculate cross-entropy loss\n",
    "    loss_candidate = F.cross_entropy(logits, targets)\n",
    "    loss_reference = F.cross_entropy(logits.t(), targets)\n",
    "    loss = (loss_candidate + loss_reference) / 2\n",
    "\n",
    "    # Calculate correct predictions\n",
    "    corrects = (logits.argmax(dim=1) == targets).sum().item()\n",
    "\n",
    "    return loss, corrects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "5f1b4edc",
    "papermill": {
     "duration": 0.015457,
     "end_time": "2024-12-18T17:14:16.105022",
     "exception": false,
     "start_time": "2024-12-18T17:14:16.089565",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### TODO: Fill the trainLoop and valLoop (5pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "05f0bf97",
    "papermill": {
     "duration": 0.027335,
     "end_time": "2024-12-18T17:14:16.147775",
     "exception": false,
     "start_time": "2024-12-18T17:14:16.120440",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def trainLoop(dataloader, models, referenceTokenizer, candidateTokenizer, optimizer, temperature):\n",
    "    models['candidateModel'].train()\n",
    "\n",
    "    totalLoss = 0.0\n",
    "    totalCorrects = 0\n",
    "\n",
    "    print(\"Training Starts!\")\n",
    "    for (index, pairs) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "\n",
    "        candidteTokenized = candidateTokenizer(getPersianDs(pairs), padding='max_length', truncation=True, return_tensors=\"pt\", max_length=configs[\"fa_tok_percentile\"])\n",
    "        referenceTextTokenized = referenceTokenizer(getEnglishDs(pairs))\n",
    "\n",
    "        batch = {\n",
    "            \"candidate\" : candidteTokenized,\n",
    "            \"reference\" : referenceTextTokenized\n",
    "        }\n",
    "\n",
    "        loss, corrects = calcLoss(batch, models['referenceModel'], models['candidateModel'], temperature)\n",
    "        totalCorrects += corrects\n",
    "        totalLoss += loss.item() * len(pairs) # Multiply by batch size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    avgLoss = totalLoss / len(dataloader.dataset)\n",
    "    avgAccuracy = totalCorrects / len(dataloader.dataset)\n",
    "\n",
    "    print(\"train loss\", avgLoss)\n",
    "    print(\"train accuracy\", avgAccuracy)\n",
    "\n",
    "    return avgLoss, avgAccuracy\n",
    "\n",
    "def valLoop(dataloader, models, referenceTokenizer, candidateTokenizer, temperature):\n",
    "    models['candidateModel'].eval()\n",
    "\n",
    "    print(\"Testing Starts!\")\n",
    "    totalLoss = 0.0\n",
    "    totalCorrects = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (index, pairs) in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            candidteTokenized = candidateTokenizer(getPersianDs(pairs), padding='max_length', truncation=True, return_tensors=\"pt\", max_length=configs[\"fa_tok_percentile\"])\n",
    "            referenceTextTokenized = referenceTokenizer(getEnglishDs(pairs))\n",
    "\n",
    "            batch = {\n",
    "                \"candidate\" : candidteTokenized,\n",
    "                \"reference\" : referenceTextTokenized\n",
    "            }\n",
    "\n",
    "            loss, corrects = calcLoss(batch, models['referenceModel'], models['candidateModel'], temperature)\n",
    "\n",
    "            totalCorrects += corrects\n",
    "            totalLoss += loss.item() * len(pairs) # Multiply by batch size\n",
    "\n",
    "    avgLoss = totalLoss / len(dataloader.dataset)\n",
    "    avgAccuracy = totalCorrects / len(dataloader.dataset)\n",
    "\n",
    "    print(\"test loss\", avgLoss)\n",
    "    print(\"test accuracy\", avgAccuracy)\n",
    "\n",
    "    return avgLoss, avgAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "b23a96c3851647f1a526b4ad6a2a60bb",
      "4fdb5caef3ab46e69f31191399ad3c3f",
      "70d04ce1006b48ab95019819cfc5db99",
      "2aa4be00c0f8461486d417f34c329d4b",
      "737f2cb028c94e82ba64397d0bcf721b",
      "9aa8a2db25e44230b6b4cd50cb1d3bcd",
      "49a7171c091e4341b73d8260540209a4",
      "35a885a90e8b48b987608236043ce621",
      "9f07640bc8cb475c92f1d3eb5619625d",
      "0c4922cc51184b8f935c47b8c96457be",
      "f05d83bea4c64b44859733e3b017e166",
      "50bfe731439649d999cdfe85dfb5f9f0",
      "f77439ae065f47aab12f528b65d1202b",
      "99b21f6cd21e432cadd974f18d342589",
      "cc286311563d4bddb53cda0aedff7130",
      "445e81a1071041b4a04bf094a8599395",
      "a283ad1edebd48e399e276cf51b9fead",
      "944704b15cd0410795e88666b3b1126a",
      "d74191a304e94ca3bd492eeb56e6ba09",
      "980bf79b83b042b79860b569c97566bd",
      "6913b33a5d4b4d9fb24c8fb8e6127761",
      "6438fb5bd9e544f2a215bcc06bc9e259"
     ]
    },
    "id": "f015a11f",
    "outputId": "188ceada-c4aa-42f8-ca4f-3d422aef632a",
    "papermill": {
     "duration": 5.07707,
     "end_time": "2024-12-18T17:14:21.240389",
     "exception": false,
     "start_time": "2024-12-18T17:14:16.163319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oldCols = [\"en\", \"fa\"]\n",
    "datasetTrain, datasetVal = getDatasetsCSV(oldCols[0], oldCols[1], configs[\"english\"], configs[\"persian\"], configs[\"train_path\"], configs[\"val_path\"])\n",
    "\n",
    "getPersianDs, getEnglishDs = getDsByLang(configs[\"persian\"], configs[\"english\"])\n",
    "print(\"Before Preproccess: \", datasetTrain[0])\n",
    "datasetTrain, datasetVal = applyPreprocess([datasetTrain, datasetVal], configs)\n",
    "print(\"After Preproccess: \", datasetTrain[0])\n",
    "trainDataloader = DataLoader(datasetTrain, batch_size=configs['batch_size'], shuffle=True)\n",
    "valDataloader = DataLoader(datasetVal, batch_size=configs['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "aea3bc61",
    "papermill": {
     "duration": 0.015811,
     "end_time": "2024-12-18T17:14:21.272664",
     "exception": false,
     "start_time": "2024-12-18T17:14:21.256853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "820c1c0e4f234de9b1fcc040cf564ce2",
      "97e44b238be242559cc7bb76049a3f17",
      "0a3cd0342104488a9013e1d1dc6c79c2",
      "54ac8cbdf32841fb848d9d36d02a515d",
      "d409870db32e4deab88db7306796df72",
      "b84f9c31032142b6b4e9e9fc45a4866b",
      "981ea5b4f7764e46b83c30b3eff1947c",
      "8b1af278fa05442ea0f100592ccc6e90",
      "7550509bf8114879b80f96af81f4cfaa",
      "c87b8362fbc24d0386a6b6304e123acb",
      "faa159fab4c8448995e082c3551055ef",
      "3e9b08ac1ef14a1b9580fd7b7178ca61",
      "62d637da7e9b42049323a7d2e240002f",
      "5f18079c6da04003b9bb17d489596cc6",
      "6504b626da464da788ed0e37dfc844d0",
      "24b7d9d8dc5041f7b88b3243df45fe35",
      "d6d0e3bc6d0e4f88851b98c73d4bb959",
      "f95158730c5c487485a515f4339916f9",
      "04ff8580474e49378ae96251ea0a751e",
      "a977523b7c314c28bb474b6efdcf1512",
      "71eec552503b44b7b22849c9a538670b",
      "e4a9fccb493741a2897fa27bc83cd956",
      "eb309ff57cbb470a813f02abe0cac47b",
      "9061e043b413467ab51aef9a1f0c6697",
      "309c83fd4034461ab3fccd640ac15b65",
      "b2b4afd18a1542d88a50c399e351640e",
      "45add9db854044b68bb7c94ca72e8911",
      "4292046b1b5c4ad8a90b17fa2e76665e",
      "6dc051bf1b204cf2b29e5958fc84b1f0",
      "822306e53f764eb18edbb0833e013ed8",
      "d7fe91f265794660a7abc48c3dea36cc",
      "3e18d427e8294e6bad822ae4bd826f2e",
      "c4c4c54e558047129f6762d899c19bda",
      "51f022334e3549b79ddbf5f17402828f",
      "947a4ba5784a4aac8f13332cfedfdfd4",
      "1fca7b8431af44289a256eec6c8eba39",
      "45258865aab0438b9844d6120c00b146",
      "e8c9ad8ce9f0467ba289bb7758497d63",
      "af2f864be2e74ec8b3e278957effd7d5",
      "4ab4663d58e2415b9ec973e96d5fc0d5",
      "64c0bf0c2cf2453c8e16892ed4cbd6dd",
      "50f2d92945a44b8d89e92049d6ef755b",
      "3b7e89c615ac4f958de69da4fffa2e9b",
      "2ff0384278834795ad154558e5cb72dc",
      "c3a124d957e24ba29987e20ce2a60d33",
      "e12ca3c456cc415999ecd83264a076c9",
      "a6a6e7165a0c4247976fe5dd39c96d88",
      "a02e1e5eff2b4938a066ac2c3a13cb89",
      "3391bba4ee92459cafc0e020fa00929d",
      "956a27a5e36b4eeaac1f43b258792214",
      "25d3f3dae8c74be99b78dc54491f9837",
      "f1314d026889467b949b2911bb329382",
      "c25a2967dcf44f00b0dcb57c6a08f5df",
      "70656eab63a947ffaab5b671db69aec1",
      "cbdf015fc94643d9b31b266faf620252"
     ]
    },
    "id": "506ef8ec",
    "outputId": "93bdbf57-8990-4a41-e7df-2fb4a6539a4f",
    "papermill": {
     "duration": 10.512708,
     "end_time": "2024-12-18T17:14:31.801887",
     "exception": false,
     "start_time": "2024-12-18T17:14:21.289179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "referenceTokenizer = open_clip.get_tokenizer(configs[\"reference_checkPoint\"])\n",
    "\n",
    "candidateConfig = AutoConfig.from_pretrained(configs[\"candidate_checkpoint\"])\n",
    "candidateTokenizer = AutoTokenizer.from_pretrained(configs[\"candidate_checkpoint\"])\n",
    "\n",
    "configs = configs | {\"candidate_embedding\" : candidateConfig.hidden_size}\n",
    "\n",
    "faTokenPercentile = calcPrcentileTokens(datasetTrain, candidateTokenizer, configs[\"persian\"])\n",
    "faTokenPercentile\n",
    "\n",
    "enTokenPercentile = calcPrcentileTokens(datasetTrain, referenceTokenizer, configs[\"english\"])\n",
    "enTokenPercentile\n",
    "\n",
    "configs = configs | {\"en_tok_percentile\" : enTokenPercentile}\n",
    "configs = configs | {\"fa_tok_percentile\" : faTokenPercentile}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {
    "id": "d92d790a",
    "papermill": {
     "duration": 0.016378,
     "end_time": "2024-12-18T17:14:31.835725",
     "exception": false,
     "start_time": "2024-12-18T17:14:31.819347",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "782057b9c1b149459761cbde3282fb1d",
      "c6e5db0894284d4ab615a85eb7bd4eb5",
      "81d9bdbc04794a218424046186384e0f",
      "dfbaf24353cd449a96395fd8880162b8",
      "f95dffa2635a4055b09914d8d872c037",
      "89c08c03228f4b2ea6b2450b7b685897",
      "b9b9379f38124f48866567ce7a4d41a2",
      "229c158603794ab1acb65683710b801b",
      "7be158be28ff48f1bf4b406061f1ffd4",
      "105575c65b774cdd89df65c0afcd1480",
      "bc5b0c911c43405cbada04f655db151e"
     ]
    },
    "id": "0cfbc447",
    "outputId": "66ec097d-1fd1-4a15-aea0-05311b1c6dbe",
    "papermill": {
     "duration": 28.039003,
     "end_time": "2024-12-18T17:14:59.891005",
     "exception": false,
     "start_time": "2024-12-18T17:14:31.852002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "referenceModel = TextEncoder(configs).to(configs[\"device\"])\n",
    "candidateModel = CandidateModel(model_name=configs[\"candidate_checkpoint\"], unfreezeLayers=configs[\"unfreezed_layers\"]).to(configs[\"device\"])\n",
    "referenceModel = freezeModel(referenceModel)\n",
    "\n",
    "candidateModel.to(configs['device'])\n",
    "referenceModel.to(configs['device'])\n",
    "\n",
    "models = {\n",
    "    \"referenceModel\" : referenceModel,\n",
    "    \"candidateModel\" : candidateModel\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "59bc9d96",
    "papermill": {
     "duration": 0.0162,
     "end_time": "2024-12-18T17:14:59.924176",
     "exception": false,
     "start_time": "2024-12-18T17:14:59.907976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training\n",
    "### TODO (10pts) for running the code and (20pts) for achieving above 70 percent test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "03a0e5e7",
    "papermill": {
     "duration": 0.025563,
     "end_time": "2024-12-18T17:14:59.966213",
     "exception": false,
     "start_time": "2024-12-18T17:14:59.940650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "temperature = torch.nn.Parameter(torch.tensor(configs['temperature']).float())\n",
    "optimizer = torch.optim.AdamW(list(models['candidateModel'].parameters()) + [temperature], weight_decay=configs[\"weight_decay\"], lr=configs['lr'])\n",
    "lrScheduler = ReduceLROnPlateau(optimizer, 'max', patience=configs['patience'], factor=configs['factor'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a7f783c",
    "outputId": "988f0fe6-8fb5-43f8-b574-11c7740169f1",
    "papermill": {
     "duration": 17782.445889,
     "end_time": "2024-12-18T22:11:22.429709",
     "exception": false,
     "start_time": "2024-12-18T17:14:59.983820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "bestValAcc = float('-inf')\n",
    "\n",
    "metrics = pd.DataFrame(columns=[\"Avg-train-loss\", \"Avg-train-accuracy\",\"Avg-val-loss\", \"Avg-val-accuracy\"])\n",
    "\n",
    "for t in range(configs['epochs']):\n",
    "    trainLoss, trainAcc = trainLoop(trainDataloader, models, referenceTokenizer, candidateTokenizer, optimizer, temperature)\n",
    "    valLoss, valAcc = valLoop(valDataloader, models, referenceTokenizer, candidateTokenizer, temperature)\n",
    "\n",
    "    metrics.loc[t+1] = [trainLoss, trainAcc, valLoss, valAcc]\n",
    "\n",
    "    lrScheduler.step(valAcc)\n",
    "\n",
    "    print(\"Temperature at this epoch was :\", temperature.item())\n",
    "\n",
    "print(f'Best accuracy of validation gained: ', bestValAcc)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {
    "id": "NlAbk3r0cRKS"
   },
   "source": [
    "Reached Colab limit for GPU usage :))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "id": "884ba162",
    "papermill": {
     "duration": 0.494729,
     "end_time": "2024-12-18T22:11:23.078241",
     "exception": false,
     "start_time": "2024-12-18T22:11:22.583512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotMetric(metrics, \"Avg-val-accuracy\")\n",
    "metrics.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {
    "id": "732adf00",
    "papermill": {
     "duration": 0.148084,
     "end_time": "2024-12-18T22:11:23.388751",
     "exception": false,
     "start_time": "2024-12-18T22:11:23.240667",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Question Box (10pts + 5pts bonus)\n",
    "\n",
    "1- Do we even need normalization for tasks like this? will they provide any accuracy gain? write what you think.\n",
    "<br/><br/>\n",
    "\n",
    "Yes, normalization is generally beneficial in tasks like this, especially when using cosine similarity for contrastive learning. It can contribute to improved accuracy and training stability.\n",
    "\n",
    "Reasoning:\n",
    "\n",
    "    Improved Accuracy: Normalization ensures that the magnitude of the embeddings doesn't dominate the similarity calculation. By focusing on the direction of the embeddings, normalization allows the model to learn more meaningful relationships between the text and image representations. This often leads to better alignment and improved accuracy in contrastive learning tasks.\n",
    "\n",
    "    Training Stability: Normalization can help stabilize the training process by preventing gradients from exploding or vanishing due to large variations in embedding magnitudes. This can lead to faster and more reliable convergence during training.\n",
    "\n",
    "    Cosine Similarity: Cosine similarity is sensitive to the magnitude of vectors. If the vectors are not normalized, the similarity score will be biased towards vectors with larger magnitudes. Normalization removes this bias, allowing the model to focus on the direction of the vectors, which is more relevant to the semantic similarity of the texts.\n",
    "<br/>\n",
    "2- When training a neural network, what takes the memory? mention at least 4 things.\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "  Model Parameters: The weights and biases of the neural network are stored in memory. Larger models with more layers and parameters require more memory.<br/><br/>\n",
    "  Activations: The intermediate outputs of each layer (activations) need to be stored during the forward pass to be used in the backward pass for gradient calculations. These activations can consume a significant amount of memory, especially for deep networks and large batch sizes.<br/><br/>\n",
    "  Optimizer State: Optimizers like Adam or SGD store internal state variables (e.g., momentum, gradients) to update model parameters. This state also contributes to memory usage.<br/>\n",
    "  Data: The training data itself needs to be loaded into memory, either in batches or entirely, depending on the training strategy. Large datasets can occupy a substantial portion of memory.<br/>\n",
    "\n",
    "3- find out the actual Open Ai's training configuration of Clip model.<br/><br/>\n",
    "\n",
    "OpenAI trained CLIP on a massive dataset of 400 million image-text pairs collected from the internet. They used a contrastive learning objective to train a dual-encoder architecture consisting of an image encoder and a text encoder. The model was trained on 592 V100 GPUs for 32 epochs. The batch size was 32,768. The image encoder was a ResNet-50, and the text encoder was a Transformer with 12 layers and 8 attention heads. More information can be found in the CLIP paper or [redacted link].\n",
    "\n",
    "\n",
    "#### Bonus\n",
    "4- We have an alternative clip's loss implementation, write its pseudocode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "id": "dcd8efa7",
    "papermill": {
     "duration": 0.152521,
     "end_time": "2024-12-18T22:11:23.693977",
     "exception": false,
     "start_time": "2024-12-18T22:11:23.541456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "function calculate_alternative_clip_loss(image_embeddings, text_embeddings, temperature):\n",
    "  # Normalize embeddings\n",
    "  image_embeddings = normalize(image_embeddings)\n",
    "  text_embeddings = normalize(text_embeddings)\n",
    "\n",
    "  # Calculate similarity matrix\n",
    "  similarity_matrix = image_embeddings @ text_embeddings.T / temperature\n",
    "\n",
    "  # Calculate loss\n",
    "  image_loss = cross_entropy_loss(similarity_matrix, targets)  # targets: identity matrix\n",
    "  text_loss = cross_entropy_loss(similarity_matrix.T, targets)\n",
    "  total_loss = (image_loss + text_loss) / 2\n",
    "\n",
    "  return total_loss"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4600270,
     "sourceId": 7845742,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4600399,
     "sourceId": 7845904,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17879.671693,
   "end_time": "2024-12-18T22:11:27.581599",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-18T17:13:27.909906",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
