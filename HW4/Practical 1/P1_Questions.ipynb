{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_J5e7oGRh-9N"
   },
   "source": [
    "# HW4 - MobileNet V1 and V2: A PyTorch Tutorial\n",
    "[CE40477: Machine Learning](https://www.sharifml.ir/)\n",
    "\n",
    "__Course Instructor__: Dr. Sharifi-Zarchi\n",
    "\n",
    "__Notebook Author__: Amirmahdi Meighani\n",
    "\n",
    "\n",
    "\n",
    "In this notebook, we present a comprehensive guide to understanding and implementing the MobileNet V1 and V2 architectures using PyTorch. MobileNet is a highly regarded family of deep learning architectures, designed by Google with a primary focus on creating models that are both lightweight and computationally efficient. These models are particularly optimized for mobile and embedded devices, where resources like memory, processing power, and battery life are often limited.\n",
    "\n",
    "MobileNet stands out for its unique design principles, particularly the use of depthwise separable convolutions, which significantly reduce the number of parameters and computation required compared to traditional convolutional neural networks. This makes MobileNet an ideal choice for applications that demand real-time performance, such as mobile apps, IoT devices, and on-device machine learning tasks.\n",
    "\n",
    "Another application for MobileNet is edge deployment. Its scalable architecture allows customization through width and resolution multipliers, making it adaptable to the resource constraints of various devices. Additionally, MobileNet's low power consumption is ideal for battery-operated systems, and its compatibility with frameworks like TensorFlow Lite and hardware accelerators like Edge TPUs ensures seamless deployment. These features make it perfect for real-time, on-device applications such as object detection, face recognition, and other AI tasks at the edge.\n",
    "\n",
    "Throughout this notebook, we will delve into both MobileNet V1 and V2 architectures, examining how each version builds on the previous one to enhance both efficiency and accuracy. By implementing these architectures step-by-step in PyTorch, we aim to provide a solid foundation for anyone interested in leveraging MobileNet’s capabilities for mobile and edge computing applications.\n",
    "\n",
    "### Objectives\n",
    "- Understand the structure and advantages of MobileNet V1 and V2\n",
    "- Learn the theory of depthwise separable convolutions and inverted residuals\n",
    "- Implement and use MobileNet V1 and V2 in PyTorch\n",
    "- Use transfer learning for better accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cjcfloLh-9P"
   },
   "outputs": [],
   "source": [
    "# Enter your information here\n",
    "first_and_last_name = 'Amir Malekhosseini'\n",
    "std_number = '401100528'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kia2fW8-h-9P"
   },
   "source": [
    "![architecture](https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fs41598-024-53349-w/MediaObjects/41598_2024_53349_Fig2_HTML.png)\n",
    "\n",
    "## 1. MobileNet V1\n",
    "\n",
    "### Theory\n",
    "MobileNet V1 was designed by Google to create an efficient and lightweight neural network model for mobile and embedded applications. The architecture is built on **depthwise separable convolutions**, which reduce computation cost and model size compared to traditional convolutions.\n",
    "\n",
    "- **Depthwise Separable Convolution**: A depthwise separable convolution splits a standard convolution into two parts:\n",
    "  1. **Depthwise Convolution**: This operation applies a single filter to each input channel independently. It significantly reduces computation by processing each channel separately.\n",
    "  2. **Pointwise Convolution (1x1 Convolution)**: This operation combines the channels outputted by the depthwise convolution by applying a 1x1 filter. It performs a linear combination across the channels, allowing the model to learn interactions between different channels.\n",
    "\n",
    "To learn more about this convolution you can check this [link](https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec)\n",
    "\n",
    "\n",
    "Compared to traditional convolution, which combines spatial and channel information simultaneously, depthwise separable convolutions drastically reduce the number of computations.\n",
    "\n",
    "MobileNet V1 also introduces two scaling hyperparameters:\n",
    "- **Width Multiplier** (α): Scales the number of channels in each layer, allowing you to shrink the model's width (number of channels).\n",
    "- **Resolution Multiplier** (ρ): Reduces the resolution of the input image, further decreasing computational cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDXJOarvjH2E"
   },
   "source": [
    "Now let's implement a MobileNet from scratch. Start by importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuobDHEVjlT3"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset\n",
    "from typing import Tuple, List\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvJJpjAQj2eF"
   },
   "source": [
    "Check your device. We recommend using GPU. You can use GPU with google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ts4zMVXWjs4v",
    "outputId": "f93f5660-8cf1-44c7-c90b-f68b2873d635"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9WJuMy9mvEKs"
   },
   "source": [
    "Before starting we want you to have some knowledge about Batch Normalization.\n",
    "\n",
    "### **Understanding Batch Normalization**\n",
    "\n",
    "Batch Normalization (BatchNorm) is a technique designed to improve the training of deep neural networks by normalizing the inputs to each layer. It helps address the issue of \"internal covariate shift,\" where the distribution of activations changes as training progresses.\n",
    "\n",
    "Batch Normalization consists of two main steps: **Normalization** and **Scaling/Shift**.\n",
    "\n",
    "### 1. **Normalization**\n",
    "\n",
    "Given a batch of data $X$ with $m$ samples and $n$ features, Batch Normalization first computes the **mean** ($\\mu$) and **variance** ($\\sigma^2$) for each feature across the batch:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i, \\quad\n",
    "\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Each feature is then normalized using:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\epsilon$: A small constant added to avoid division by zero.\n",
    "\n",
    "\n",
    "\n",
    "### 2. **Scaling and Shifting**\n",
    "\n",
    "To allow the model to learn the optimal feature representation, the normalized value $\\hat{x}_i$ is scaled and shifted using learnable parameters $\\gamma$ (scale) and $\\beta$ (shift):\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\hat{x}_i + \\beta\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### **Complete Formula**\n",
    "\n",
    "The full Batch Normalization transformation for each feature is:\n",
    "\n",
    "$$\n",
    "y_i = \\gamma \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "## **Why Use Batch Normalization?**\n",
    "\n",
    "1. **Stabilizes Training**: By normalizing inputs, gradients are more stable, which accelerates convergence.\n",
    "2. **Reduces Dependence on Initialization**: Makes the network less sensitive to weight initialization.\n",
    "3. **Acts as a Regularizer**: Introduces noise due to batch statistics, helping reduce overfitting.\n",
    "4. **Improves Generalization**: Often eliminates the need for other regularization techniques like Dropout.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-cO1oDsvPhH"
   },
   "outputs": [],
   "source": [
    "def batch_normalization(x, gamma, beta, epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Performs batch normalization on the input data.\n",
    "\n",
    "    Parameters:\n",
    "        x (numpy.ndarray): Input data of shape (batch_size, features).\n",
    "        gamma (float): Scale parameter.\n",
    "        beta (float): Shift parameter.\n",
    "        epsilon (float): Small value to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Normalized and scaled data.\n",
    "    \"\"\"\n",
    "    mean = np.mean(x, axis=0)\n",
    "    variance = np.var(x, axis=0)\n",
    "\n",
    "    # Normalize the input data\n",
    "    x_normalized = (x - mean) / np.sqrt(variance + epsilon)\n",
    "\n",
    "    # Scale and shift the normalized data\n",
    "    x_scaled_shifted = gamma * x_normalized + beta\n",
    "    return x_scaled_shifted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "045vQ7tZvUBd",
    "outputId": "d95ae716-1fb4-4df9-9b4d-6bb5026aae07"
   },
   "outputs": [],
   "source": [
    "# Example batch data (4 samples, 3 features)\n",
    "x = np.array([[1.0, 2.0, 3.0],\n",
    "              [4.0, 5.0, 6.0],\n",
    "              [7.0, 8.0, 9.0],\n",
    "              [10.0, 11.0, 12.0]])\n",
    "\n",
    "# Parameters for scaling and shifting\n",
    "gamma = 1.0\n",
    "beta = 0.0\n",
    "\n",
    "# Apply batch normalization\n",
    "normalized_data = batch_normalization(x, gamma, beta)\n",
    "\n",
    "# Print results\n",
    "print(\"Original Data:\\n\", x)\n",
    "print(\"\\nNormalized Data:\\n\", normalized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9uNGBP6k80A"
   },
   "source": [
    "We need some data for training, testing and validation. in this part we use CIFAR-10 dataset. It is a famous dataset in vision. CIFAR-10 contains 60,000 32x32 images in 10 classes.\n",
    "\n",
    "> **Note**: MobileNet models perform best on larger image sizes (like 224x224), but CIFAR-10 is used here as a simple example dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6KI6AcLDlcMN",
    "outputId": "c6c4d741-da2a-4e7d-de9d-2ee234df500e"
   },
   "outputs": [],
   "source": [
    "# TRANSFORMS\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224, scale=(0.5, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n",
    "                         std=[0.2675, 0.2565, 0.2761]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408],\n",
    "                         std=[0.2675, 0.2565, 0.2761]),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TRAIN DATA\n",
    "train_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "# TODO: create train data loader without shuffle with just half of the data\n",
    "indices = np.arange(len(train_set) // 2)\n",
    "train_subset = Subset(train_set, indices)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_subset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "# VALIDATION DATA (Using half of the original validation set)\n",
    "val_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# TEST DATA (Using half of the original test set)\n",
    "test_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=64, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdjvRTEmlu0S"
   },
   "source": [
    "Explain what we did in transfor train and tranform test:<br/>\n",
    "*answer*<br/><br/>\n",
    "In transform_train we first randomly crops a section of the image and resizes it to 224x224 pixels. Then randomly flips the image horizontally with a 50% probability.\n",
    "This is another data augmentation technique that helps the model learn features that are invariant to horizontal flips, improving generalization.<br/>\n",
    "In transform_test we first resizes the image to 224x224 pixels.\n",
    "Unlike RandomResizedCrop, it simply resizes the image without any random cropping.Then Converts the PIL Image to a PyTorch tensor, similar to transform_train.<br/>\n",
    "In other words , transform_train includes data augmentation techniques (random cropping and horizontal flipping) to make the model more robust.<br/>\n",
    "transform_test only resizes the images and normalizes the pixel values without any random transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kf9nAKEmmu62"
   },
   "source": [
    "for showing images in this notebook we define a function and use it everywhere. you should test your fucntion for 32 first images of dataset and see how our data augmentaion worked. Also print labels to see the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 735
    },
    "id": "htbtxfQ3n-p2",
    "outputId": "5304ca44-99e3-474c-ab62-78a95b13a9df"
   },
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    # TODO:\n",
    "    img = img / 2 + 0.5\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# show some training images with shuffle=False and labels\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images[:32]))\n",
    "print('Labels are: ', labels[:32])  # Print the label tensor\n",
    "# print(' '.join(f'{train_set.classes[labels[j]]:5s}' for j in range(32)))\n",
    "\n",
    "# show them again to check the augmentation\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images[:32]))\n",
    "print('Labels are: ', labels[:32])  # Print the label tensor\n",
    "# print(' '.join(f'{train_set.classes[labels[j]]:5s}' for j in range(32)))\n",
    "\n",
    "# create train loader with shuffle true\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_subset, batch_size=64, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jG7s9AHLs-7s"
   },
   "source": [
    "For trainig our models, we first define some functions to make our job easier. fill the functions according to their signature or you can change them according to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B71Ks77ut2cD"
   },
   "outputs": [],
   "source": [
    "MODEL_SAVE_PATH = './model.pt'\n",
    "\n",
    "\n",
    "# Assuming global variables 'device' and 'MODEL_SAVE_PATH' are defined elsewhere.\n",
    "\n",
    "def fit_epoch(\n",
    "    model: nn.Module,\n",
    "    data_loader: DataLoader,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: Optimizer,\n",
    "    train_mode: bool = False\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Runs one epoch of training or validation.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train or evaluate.\n",
    "        data_loader (DataLoader): The DataLoader for the dataset.\n",
    "        criterion (nn.Module): Loss function.\n",
    "        optimizer (Optimizer): Optimizer for training.\n",
    "        train_mode (bool): Flag indicating training (True) or validation (False).\n",
    "\n",
    "    Returns:\n",
    "        List[float]: List of losses for the epoch.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "\n",
    "    # Set model to training or evaluation mode\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
    "      for inputs, targets in tepoch:\n",
    "          inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "          # Forward pass\n",
    "          with torch.set_grad_enabled(train_mode):\n",
    "              outputs = model(inputs)\n",
    "              loss = criterion(outputs, targets)\n",
    "              losses.append(loss.item())\n",
    "\n",
    "              if train_mode:\n",
    "                  # Backward pass and optimization\n",
    "                  optimizer.zero_grad()\n",
    "                  loss.backward()\n",
    "                  optimizer.step()\n",
    "\n",
    "              tepoch.set_postfix(loss=loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    epochs: int,\n",
    "    lr: float = 1e-3\n",
    ") -> Tuple[List[float], List[float]]:\n",
    "    \"\"\"\n",
    "    Trains a model and validates it after each epoch.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate for the optimizer.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[float]]: Lists of training and validation losses.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "        epoch_train_losses = fit_epoch(model, train_loader, criterion, optimizer, train_mode=True)\n",
    "        train_losses.extend(epoch_train_losses)  # Accumulate training losses\n",
    "\n",
    "        epoch_val_losses = fit_epoch(model, val_loader, criterion, optimizer, train_mode=False)\n",
    "        val_losses.extend(epoch_val_losses)  # Accumulate validation losses\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "\n",
    "\n",
    "def get_acc(\n",
    "    model: nn.Module,\n",
    "    val_loader: DataLoader\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates accuracy of the model on a validation dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "\n",
    "    Returns:\n",
    "        float: Validation accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()  # Increment number of correct predictions\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "\n",
    "def plot_losses(\n",
    "    train_losses: List[float],\n",
    "    val_losses: List[float],\n",
    "    c_window: int = 10\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plots training and validation losses.\n",
    "\n",
    "    Args:\n",
    "        train_losses (List[float]): List of training losses.\n",
    "        val_losses (List[float]): List of validation losses.\n",
    "        c_window (int): Convolution window size for smoothing training losses.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    kernel = np.ones(c_window) / c_window\n",
    "    smoothed_train_losses = np.convolve(train_losses, kernel, mode='same')\n",
    "\n",
    "    # Plot the losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(smoothed_train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Losses')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sKUEbZ7uGOq"
   },
   "source": [
    "Finally we can start creating the MobileNet model. The architecture is presented here:\n",
    "![architecture](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-22_at_4.26.15_PM_ko4FqXD.png)\n",
    "\n",
    "You can see the related paper [here](https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "for cleaner implementation first complete these functions(Each layer consists of a depthwise separable convolution followed by a ReLU activation. The architecture efficiently reduces computations by using depthwise separable convolutions, making it suitable for mobile and embedded applications.):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQMFqzLm_4gI"
   },
   "outputs": [],
   "source": [
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "def conv_dw(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False),\n",
    "        nn.BatchNorm2d(inp),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNE36Q5Mg5uq"
   },
   "source": [
    "Explain how using Depthwise Separable Convolutions helps us in reducing the number of parameters(calculate and compare number of parameters in normal convolution and depthwise convolution) and how it helps us in making the model faster.\n",
    "\n",
    "\n",
    "**answer**<br/><br/>\n",
    "In a standard convolution, a single filter is applied across all input channels to produce a single output channel. If we have D_K as the kernel size (e.g., 3 for a 3x3 kernel), M as the number of input channels, and N as the number of output channels, then the total number of parameters in a standard convolutional layer is:<br/>\n",
    "Parameters = D_K * D_K * M * N <br/>\n",
    "A DSC splits the convolution into two steps:<br/>\n",
    "Depthwise Convolution: Applies a single filter to each input channel independently. The number of parameters in this step is:<br/>\n",
    "Parameters_dw = D_K * D_K * M<br/>\n",
    "Pointwise Convolution: Uses 1x1 convolutions to combine the output channels from the depthwise convolution. The number of parameters in this step is:<br/>\n",
    "Parameters_pw = M * N<br/>\n",
    "So the total parameters for DSC is:<br/>\n",
    "Parameters_dsc = Parameters_dw + Parameters_pw = D_K * D_K * M + M * N<br/>\n",
    "So the number of parameters in a DSC is significantly lower than in a standard convolution, especially when the number of output channels N is large.<br/>\n",
    "DSCs not only reduce the number of parameters but also the number of computations required during the convolution operation. This leads to faster inference times. The reduction in computations is primarily due to the separation of the spatial and channel-wise filtering operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWOAhX-2Aaex"
   },
   "source": [
    "Now please create your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wr9m_jNboGW6"
   },
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    def __init__(self, n_class=1000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  3,  32, 2),\n",
    "            conv_dw( 32,  64, 1),\n",
    "            conv_dw( 64, 128, 2),\n",
    "            conv_dw(128, 128, 1),\n",
    "            conv_dw(128, 256, 2),\n",
    "            conv_dw(256, 256, 1),\n",
    "            conv_dw(256, 512, 2),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 512, 1),\n",
    "            conv_dw(512, 1024, 2),\n",
    "            conv_dw(1024, 1024, 1),\n",
    "            nn.AvgPool2d(7),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xqcl9Av5oHmN"
   },
   "source": [
    "Let's train your model.(You can increase number of epochs or change lr but your accuracy should be above 65 and calculate training time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555,
     "referenced_widgets": [
      "c2650852ed9e49d193d94586e39b50e3",
      "44e4acb3805e4f21bf0e1e74e758633a",
      "6fc88d043f664bec92fa6ed08850b488",
      "d051dc9bf8514fc49f4182456b179a0c",
      "ca7ad817e7014d4cae9663a54a4a746f",
      "fa2fb7c3cea24ee38dc6d4dfadae970a",
      "04dd8c3ba233423089be781a6b9456a5",
      "9985dff691814de5b5580503e1247827",
      "c533d2d286154325b8333665e12b9548",
      "82703f4bb0a24f3492549d803df5dc2a",
      "c3fc7a35816e48d59ef8f161dfc26578",
      "8c320ba3dd824988ab5cbed014fc4b20",
      "631dafd081eb42ee841fc9c9c52e02b2",
      "57bb6464cf4f4857b3c5727518c95eca",
      "3962c65dab8442e7b8d9b434d2e68f96",
      "1a2b82bb5368477d9765c8f599f52467",
      "50d945d7a6ac49ada6eb9509022b8f9e",
      "8389687e2a2f4d1d94012cad301d9c1e",
      "7ec8c5f4c9384d35b1adc0b2a5305158",
      "af1888e0f42c4c73977d30dde94026ce",
      "096b6e7745dd4e7ea38a3fa15a3f5a41",
      "9c54379cd9bf4b92adc9f490d233d279",
      "cbfacc35245e45a1bf3f4d64fa3a29e6",
      "86069239f7514f2e969e2fa326270288",
      "6b58a56ba34f4626bb944552fd57f254",
      "bd3d7e141926438992de8031ff7ecfa9",
      "7b21093bd84b4014bd227a31805d8b32",
      "dbf2adc5984a4d0d958b45c0a68f8a67",
      "ad67f30f83f84957a7c2ae17a56bbf9d",
      "8a6a029d401440e9a2750bf1230c0cc0",
      "f3a3fca96e074cd7b29d57e5a23024ba",
      "167f3123e569498aac782588d99c0af5",
      "583e093569b4495bbd063eba29945cf9",
      "0de70bf358a248e1b7a49f1d7bfd5793",
      "d69f32a761994e899c17972d1e0fce7b",
      "4763f10ed76847219dd486a5b3d9bde4",
      "8386ad01da304d08a4d735480085ca44",
      "1499e77cab7d4e7ca9403ed640db066a",
      "d533a98272ce49f989a4701120b2535d",
      "8c0aba01d05b4e0db7982e39ea734298",
      "c93483c212244347bcaaa5b54d3f9a6d",
      "9fb8769425164ed6a8a774ac535572c5",
      "6c364ecd9cad4545af0c1a1bb265ed75",
      "01437e87fdfe4b6ab1768437b51baaab",
      "fb6beb9ae9484beeb50337b8202718ed",
      "ddd9d9572d634019aa8787648950aa02",
      "1f0e570ac6a346eeb2cca816244c2f5c",
      "33ba16dd1fbd49019d1f36206d048982",
      "0f659cc6138e423fb95fe880685519fa",
      "1ab50453aede4195a9b321889200cad7",
      "cf3be20fe46b4f848b6ba1c1eae268fd",
      "67654b24ade54239aaca1c02fb808443",
      "38f0e14314bc487bbff2e21764ad7b49",
      "31fb9bb37e28437caf12dbc24fa9e916",
      "e9dd675606574a6caea3d6cc0cede8f6",
      "36a85ea919214d7c8df4c1214174b40f",
      "45e82bb644f54bd5816d2cb8d1b01a89",
      "78ebf762efdd4541a150b955aa453022",
      "7d5c2e0a2fef4949b623789516a1048f",
      "51324821042d4de4a53c28fed41e8f77",
      "f068cdbcd09946a5b5b59706e35fc449",
      "432b5acfda194cadb5effd1b8301701a",
      "41cb333dd25c43b1bff78d19a3be532f",
      "af922dd705b4435ca75c67e5794457db",
      "9ffb2d4f52e441388d34417c2778199b",
      "ec02ec61244b4d8aa9debdc21093b596",
      "0d1701a7c91144619440c8b25da259bc",
      "54db7de0622848c2898016bc71c26254",
      "40d96570115a4fa28561f5fa595f7000",
      "7d4d5ce89f5948eda7b6fc11dac9b7fe",
      "fa3d65e01b2e470eb5875a838c8c24b2",
      "0180cd92c3f14f82b776a81f6855619a",
      "41aa1e6111ae46218d2632a544c8c963",
      "a69079d61270465db0fb8a6708501b7b",
      "ca08b939991f4379921329ec725cbba7",
      "b97e1b511efa4749860a303502c8db5b",
      "576d271bb1184144a3411eef2343f249",
      "4466997bc5c24ccb912c41563fa84972",
      "db619bc80a344cf48822cf0b34d326db",
      "42e8cd1b716845f787cb575d132a5629",
      "dd7a7ae70edc4c54a383f8c40171fc6e",
      "1395bc8b6ee04b349badbc0abd6dd81d",
      "adb9cdf2448f470c8bc00e3449e22887",
      "46af001629774a4bbb58a22c8564709a",
      "3f46ebd8f19343c38c1ed988fbee114f",
      "5828f1e5108c446e819ec17a57f5fec8",
      "5a8a157bbe194d759a94fbd17033911e",
      "e43a8955eb0045b69eb95bc114ae9b74",
      "5d427d5ffd974a76930d5dea976e11c3",
      "eae82ede3c3e4d11bee979e94fe97390",
      "cc47cbb7ba5644e79a37e803347f0361",
      "9fb619794e464abe8a4da9029da81482",
      "df90193298c4469bbd32d5e31e58a20e",
      "4016a4c375be45f3b1ba55bf4cea87fe",
      "49741dd03e7f43559dad294d3c7c8a9b",
      "faeef27b85e1493ba32c61ba3552e023",
      "936ca8a9dd0449fe910f801aa5e99611",
      "03190ec2ce63415b8b87d10951158a03",
      "700a25b1469d467eb36bd641679805d9",
      "61872fc3f03a4817835e54cf87f1a4ef",
      "fb2f3201e44c4d87be1e83c2d6e2b22e",
      "8ef734b82ab544b388913458797528fa",
      "4591e14ea7254de7914d656cf53d2fc7",
      "0ad3dd46b69d4fc89016d5262165e1b7",
      "7bd78d67121448faabf068c3cb06aa72",
      "3bf36798e0114c4199d16fee740a4031",
      "0a8513d96f8d43ca82f4867750f32af3",
      "e70bca72a2034350aa988e6279655e81",
      "92c36541e8b74b0d941508a347db9edc",
      "ce7c0b40d1f64a8997b6ee1e10df105b",
      "c7ce9b76eb5848bc9e067024ce920db6",
      "0b8edb4f2c8e422fbe92822dc80ad538",
      "6c1c28380177489ea095011fb4202f79",
      "b75fecc8a5794553b0cb62cf13859981",
      "ec7d5e5424da43d88e850fa734db0e74",
      "ba89750c2d0e40529fd30546d4cd0276",
      "09ccb033c591494fa5d26dd8a5f00d53",
      "4fe78045ad0f4e8ebfdb0d31538ceae9",
      "b20cb5c3e04642a1ac265dccd935d9c6",
      "f808f5cc2868435da11b81bedf2bcb5f",
      "e275f35ee89a44b5bd8b329cfaa2ed75",
      "a7e8e42b0fad4eb19081a777b06b0d06",
      "18983c8e2f904b12b91ff2bd7d637070",
      "e2c378c37cf84b2f8ce96255f3107ca7",
      "6084d68a0ab643e1a0ebd25964bae87d",
      "1c7ac24d52ca49538097f77ee615149a",
      "6b9d42a4f7244b79ba6e7ab10e9d90c9",
      "3b82a21a1a9a47c9b134ed79990cf7c0",
      "46350df35b2240c5a2dab15f85003b01",
      "74612ab3f3384550885ed3d588ca59c7",
      "3cf1327453044a6d990a1b59ae211665",
      "b2fe272088d5400faba6e31314d3cfe1",
      "96debe2bb8ed433fabe854ac7c7de88f",
      "c77cd5fd03194b979c3098150ad6441d",
      "445066a4d18a4d63ae0b8b3c43c05b90",
      "68649c87c05e4221b4aca07171b27547",
      "4d22c4637b21465b869298d95bc2a3c5",
      "00eb8c5470334c6487d70b33338db10a",
      "87935e5cbb0340008699110ec3e2a50b",
      "c800dea5abf64091a1fa2c5d53237ef6",
      "4d193736c3b04f0c9fe65e4dfb8c5675",
      "214652b85f404fb8b41ec52120954ba3",
      "0fb9ab990f04410baaaeb6b3258265cf",
      "620ef2d37377421db2e388928306d75d",
      "f93a7527574f4751b0a5ff8498b75db4",
      "36ad28c6cc3e43d5982b224fe1f289cf",
      "9a630e209ad84c55890466c0d7c5aa06",
      "2892f238f3914ee2bdae7d9b54c2f317",
      "67fc96c2d36c4356894f1abd3ce7b600",
      "b23eee87607a4b44b04e9a2efb38a01a",
      "1959cd3c013440dea28652ca28dce0c3",
      "c31d397df7f44e1d87c7613c3a25729b",
      "6b452680d5f84366bbfb5d33b689161b",
      "3e18ed1ad472480abdea5a56abd6d38c",
      "42b2008dc9144b81b16fc0710ffa9b4b",
      "615debfa7be544c580cf300076866be1",
      "40235978df4249dfbeab828913037f28",
      "174bbf095e354d74998098fd7bb59eb1",
      "b89f738608c146dc9996cc296dc9b1fa",
      "443cf4b16d1748968c411015066344a7",
      "301a36d5273b4f509b4833bf11bd6943",
      "1365ffb7f4bb4edeac7d48cc32d0a5e5",
      "8e35e40ede7d4381851604239a1839c4"
     ]
    },
    "id": "5Tpe4Cm3oMW8",
    "outputId": "93bbfb92-25d4-4e50-db9e-53564d4c2e92"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 1e-3\n",
    "\n",
    "model = MobileNet(n_class=10).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "train_losses, val_losses = train(train_loader, test_loader, model, epochs, lr)\n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2Jz-FlRoanS"
   },
   "source": [
    "Get some metrics and plot your losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "398hPNTdocHf",
    "outputId": "202fba79-b446-4584-a7da-65c695300c46"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "val_acc = get_acc(model, val_loader)\n",
    "end = time.time()\n",
    "\n",
    "print('Batch_size={}, epochs={}, lr={}'.format(128, epochs, lr))\n",
    "print('Val accuracy =', val_acc)\n",
    "print(\"Process validation time: {:0.4f} s\".format(end - start))\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Hw1IfyZhZ_9"
   },
   "source": [
    "The MobileNet paper claims it is 8 to 9 times faster than standard convolution. Using the theoretical computations of FLOPs for standard convolution and depthwise separable convolution, explain how this number was calculated.\n",
    "\n",
    "\n",
    "**answer**<br/><br/>\n",
    "For a standard convolution with kernel size D_K, input channels M, output channels N, input feature map size D_F, the total number of FLOPs is:<br/>\n",
    "FLOPs_standard = D_K * D_K * M * N * D_F * D_F<br/>\n",
    "For a depthwise separable convolution with the same parameters, the FLOPs are calculated as follows:<br/>\n",
    "FLOPs_dsc = FLOPs_dw + FLOPs_pw = D_K * D_K * M * D_F * D_F + M * N * D_F * D_F<br/>\n",
    "In the MobileNet paper, they typically use N = 32 (or larger) for the number of output channels. Plugging this value into the above equation, we get:<br/>\n",
    "FLOPs_standard / FLOPs_dsc = (9 * 32) / (9 + 32) ≈ 8.4<br/>\n",
    "This means that a depthwise separable convolution with 32 output channels requires approximately 8.4 times fewer FLOPs compared to a standard convolution with the same configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7GJ0QvzMDCX"
   },
   "source": [
    "## 2.Normal CNN\n",
    "In this part, we will give you a new architecture that is like MobileNet. But this time we will use normal CNN. The model is implemented and you should train it for 2 epochs. So now you can compare its speed in training and validating with your MobileNet.(Note that the accuracy is not important in this part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaX6faSGMwgy"
   },
   "outputs": [],
   "source": [
    "class NormalCNN(nn.Module):\n",
    "    def __init__(self, n_class=1000):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            conv_bn(  3,  32, 2),\n",
    "            conv_bn( 32,  64, 1),\n",
    "            conv_bn( 64, 128, 2),\n",
    "            conv_bn(128, 128, 1),\n",
    "            conv_bn(128, 256, 2),\n",
    "            conv_bn(256, 256, 1),\n",
    "            conv_bn(256, 512, 2),\n",
    "            conv_bn(512, 512, 1),\n",
    "            conv_bn(512, 512, 1),\n",
    "            conv_bn(512, 512, 1),\n",
    "            conv_bn(512, 512, 1),\n",
    "            conv_bn(512, 512, 1),\n",
    "            conv_bn(512, 1024, 2),\n",
    "            conv_bn(1024, 1024, 1),\n",
    "            nn.AvgPool2d(7),\n",
    "        )\n",
    "        self.fc = nn.Linear(1024, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197,
     "referenced_widgets": [
      "5e85c9f2e02e488887d1684892627e67",
      "307109ec5a7f4f0aa3935f1a90b9af9d",
      "c3865084c9f942aab1474d7f5d21aa77",
      "8d84cdb7bea8441dbf161d7869a321a8",
      "94d264c3749c421591209c56990418df",
      "daaded86088348ae8c944c9d1fa93e70",
      "df9a61379ccb43fe89b8210d4f712f9d",
      "a3b4287bc6a64518912d4fa7cd442946",
      "02b8ba189f774fa4b349eeee2dab2a90",
      "41197cbf386b4728b5b3cab1c8e542ff",
      "36f8945c4d4f477fa84eba14fc531e65",
      "40d290b9f70c4c76af105443794aed26",
      "d638a3b3bb614614b5ec751d5dc74b3f",
      "d3700b5c0d434981ad1db53198301eab",
      "c86bfe011d554235974fb3936bbc8e6c",
      "75edfe79fb3e4bc3b819b2949e7ab445",
      "958ab2de84d64286ad464c0b6570f92a",
      "b86d62819f8643e1b53e814431ec10c7",
      "2ce70f8cb61b4a5da56c0e3bb6842395",
      "d9854c054e864131982808a8f8d155d5",
      "687d838ba400459b85bbc6b141edf573",
      "3b13f8e2bbf94834afcce15bb39488dc",
      "820bb44e13c64657a713bd82d4f22b81",
      "496d3ebb2cf44febae12db9201b8fd94",
      "defd3010910149669f19b084423f29fa",
      "9152deb6652d4dbf87c43102075cec3e",
      "be2c9843697e4afb85f95d0f7362cebb",
      "ea4d305d60bb404b8b5d0570e243130f",
      "54ae394fd0394ac9ac1776f1387ff9ae",
      "0de86603353d41b7a6d2843625870d4f",
      "979a7dff249c4c0395b9e221b4cb4495",
      "5436e8533c8943a1869cd3649a2b4720",
      "127b7f26267b4c3383f7e0843881b41c",
      "bd4232783293416d832400ec9dbaa029",
      "9051167849224cb9921d0a908b8fbeb0",
      "d0047adc150e432cbce00f5cdd67626d",
      "62ce5f7fc9cd46c2b4a35a1de4177fdb",
      "aa7ae0c4472c4391ab4c475d3523bb69",
      "98feab2b239d491782c0d9af497bd803",
      "eb514c589fa94dde9db401be2cfd4f2a",
      "58ec646066b94a418bcfdac34e5ec064",
      "4c02b93d5ed244d0871dbd9707086623",
      "e5ad52a2f6fa41b3a908fe6460740cdc",
      "a571e276b88a48818e9978384c081b70",
      "033292addc8549568d744798ee3d8691",
      "6acca59ff8b64154bf1d8ebddf95af88",
      "efab5c93c8f14bcd9f11a9f540aa7ef8",
      "67e097b753ad464cab46d7cdf266d892"
     ]
    },
    "id": "uB4aBvJGNFyr",
    "outputId": "7b1a7ba4-2789-4345-9ac7-cae15694bbda"
   },
   "outputs": [],
   "source": [
    "# TODO: train model\n",
    "\n",
    "epochs = 2\n",
    "lr = 1e-3\n",
    "\n",
    "model_normal = NormalCNN(n_class=10).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "train_losses, val_losses = train(train_loader, val_loader, model_normal, epochs, lr)\n",
    "end_time = time.time()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "E5-mvUmsV0d3",
    "outputId": "b8844166-9115-49c0-c8cb-6c9d5d44f6ba"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "val_acc = get_acc(model_normal, val_loader)\n",
    "end = time.time()\n",
    "\n",
    "print('Batch_size={}, epochs={}, lr={}'.format(128, epochs, lr))\n",
    "print('Val accuracy =', val_acc)\n",
    "print(\"Process validation time: {:0.4f} s\".format(end - start))\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-i5cBfX4abmR"
   },
   "source": [
    "Compare speed of learning and validating in normal CNN vs MobileNet\n",
    "\n",
    "**answer**<br/>\n",
    "\n",
    "  Learning Speed: MobileNets generally learn faster than normal CNNs due to the reduced number of parameters and computations.<br/>\n",
    "  Validation Speed: MobileNets also have faster validation times compared to normal CNNs for the same reason.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwqeoBlYa-LS",
    "outputId": "bf962741-00a0-4f8f-8b51-dd075e654d41"
   },
   "outputs": [],
   "source": [
    "# TODO: compare number of parameters in these two models\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "mobilenet_params = count_parameters(model)\n",
    "normalcnn_params = count_parameters(model_normal)\n",
    "\n",
    "print(f\"MobileNet Parameters: {mobilenet_params}\")\n",
    "print(f\"NormalCNN Parameters: {normalcnn_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L15Pxu9OVNNN"
   },
   "source": [
    "## 3.Transfer Learning\n",
    "Now imagine that you have a well trained model of CIFAR10 and now you want to use transfer learning for a new task of classifying CIFAR100 dataset. in the next cell, you should load your good trained model with an accuracy of 80% on validation data from disk (you should have saved your good model). or use our model (model.pt) and use their parameters for a new model for classifying 100 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "v2a4SLAAWUFR",
    "outputId": "18e416b8-17d7-4425-8d38-526ddd3647d6"
   },
   "outputs": [],
   "source": [
    "model = MobileNet(n_class=100).to(device)\n",
    "\n",
    "# Load the pre-trained weights\n",
    "pretrained_dict = torch.load('model.pt')\n",
    "\n",
    "# Filter out unnecessary keys (fc layer)\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if 'fc' not in k}\n",
    "\n",
    "# Modify the keys to match the new model's expected key format\n",
    "modified_dict = {}\n",
    "for k, v in pretrained_dict.items():\n",
    "\n",
    "    modified_key = k.replace('.', '.0.')\n",
    "    modified_dict[modified_key] = v\n",
    "\n",
    "# Load the modified state dict into the model\n",
    "model.load_state_dict(modified_dict, strict=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9MjdXhOWfMS"
   },
   "source": [
    "**Freezing Layers** </br>\n",
    "To make the best use of transfer learning, you should freeze the initial layers (usually up to the last few convolutional blocks) and only fine-tune the final layers. This way, the model retains most of the learned low-level features from CIFAR-10, while also adapting to CIFAR-100.\n",
    "\n",
    "In MobileNet, it’s generally effective to freeze layers up to the point where the most complex, high-level features are computed, typically around the 7th or 8th convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZDampuhWeN2"
   },
   "outputs": [],
   "source": [
    "# Freeze up to the desired layer (e.g., the first 7 layers)\n",
    "\n",
    "for i, child in enumerate(model.model.children()):  # model.model is the feature extraction part\n",
    "    if i < 7:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IdrKiGIafr4"
   },
   "source": [
    "**Fine-tuning the Model** </br>\n",
    "\n",
    "With the above setup, you can now train the model on CIFAR-100 using a lower learning rate. A good approach would be to:\n",
    "\n",
    "Set a slightly lower learning rate (e.g., 1e-4) for the fine-tuning process.\n",
    "\n",
    "But first get the dataset of CIFAR100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "749c3c488316436abb361d3f9b1acee4"
     ]
    },
    "id": "OxAN3tvha1ON",
    "outputId": "8a5ba290-61cc-4676-c7a0-67008109f9ad"
   },
   "outputs": [],
   "source": [
    "# TRAIN DATA\n",
    "train_set100 = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=True, download=True, transform=transform_train)\n",
    "\n",
    "indices = np.arange(20000)\n",
    "train_set100 = Subset(train_set100, indices)\n",
    "\n",
    "train_loader100 = torch.utils.data.DataLoader(\n",
    "    train_set100, batch_size=128, shuffle=True, num_workers=2)\n",
    "\n",
    "# VALIDATION DATA\n",
    "val_set100 = torchvision.datasets.CIFAR100(\n",
    "    root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "val_loader100 = torch.utils.data.DataLoader(\n",
    "    val_set100, batch_size=128, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjvhYGHlr-si"
   },
   "source": [
    "Now show 16 images of your data two times to see the images and how our data augmentaion worked. Also print labels to see the format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "qbt50TqNrlue",
    "outputId": "7f79f199-1ed3-4469-a474-b7c4abe05666"
   },
   "outputs": [],
   "source": [
    "# show some training images with shuffle=False and labels\n",
    "dataiter = iter(train_loader100)\n",
    "images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images[:16]))\n",
    "print('Labels are: ', labels[:16])  # Print the label tensor\n",
    "\n",
    "# show them again to check the augmentation\n",
    "dataiter = iter(train_loader100)\n",
    "# images, labels = next(dataiter)\n",
    "imshow(torchvision.utils.make_grid(images[:16]))\n",
    "print('Labels are: ', labels[:16])  # Print the label tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4gyAmfzEsl7t"
   },
   "source": [
    "Do CIFAR10 and CIFAR100 use the same images but with diffrent labesl?(maybe you should search)\n",
    "\n",
    "**answer**<br/><br/>\n",
    "CIFAR-10 and CIFAR-100 do not use the exact same set of images but share many similarities. Both datasets contain 60,000 color images (32x32 pixels), split into 50,000 training images and 10,000 test images. However, the classification labels differ significantly:\n",
    "\n",
    "CIFAR-10: Contains 10 mutually exclusive classes (e.g., airplane, car, bird), with 6,000 images per class.<br/>\n",
    "CIFAR-100: Contains 100 fine-grained classes (e.g., specific animal or object types) and also groups these into 20 superclasses for broader categorization. Each class in CIFAR-100 has 600 images: 500 for training and 100 for testing.<br/>\n",
    "Task Complexity: CIFAR-100 is considered a more challenging dataset due to the larger number of classes and finer-grained distinctions between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktCytrhbbswx"
   },
   "source": [
    "Now train your model using your functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 587,
     "referenced_widgets": [
      "b2108b55069e4a628536ea6fabe09e58",
      "a50de66d205c48079b794979aa6ba8b9",
      "8bc9fe718494415d873fec79363060e2",
      "7bf8918991894ada8fcfbee12e5b6704",
      "36d86ba74c17423bb1a6af0614d129e8",
      "a1bcc6a05b9e4a8bb7eb6c7858e515ce",
      "4d506f002505400ca26702e902c9224f",
      "210a7424023341b8b7a018e78359f9ad",
      "bf8eaefd152c419cbbcc58e4c44ddce4",
      "1ec24bc4d2f04874821257c13f9a9f35",
      "5b4874d31a7e426ab1781d9c4e8065ad",
      "ba01ac5ebf0b4101866db0df959ad08f",
      "e15b410f068a4ae3890bba53c030a997",
      "6ce415f5ef1d4306bd417bdbadf7f904",
      "c0c9926b943d44a6a5c6ee1bbc4e2b1f",
      "240ea25dd03c4a1ba0fa0bebe156f921",
      "49bd5646fb2d4b92a623bf26f68a2387",
      "d5a154c31c4e4c91b55a497bdb78c406",
      "29cf4b44258c41e6af0ee900279acaef",
      "b6add0d76e304a6f9e1aaf407a12a7e7",
      "636d8387a88149a38ff4af04e5e13102",
      "d0be8a2ed62f4fe2b14faa757251461b",
      "322a077699de44378f790fc31a5ea741",
      "2effe7f89c234bc6b7a69b5a668d474f",
      "8ae80f7f10144cafabebdc4db7269b3f",
      "a4221949d598429ebc85fa4a269767b8",
      "9fb0d08ad24e4ca8a7c1f91d3e8fc94a",
      "205b5a144b9d4870b6bc85d6267f09fa",
      "e36e81f6ce7c4404850ece2f6a328b43",
      "3e34358bcbc54d9aa7da204618894e47",
      "37cc0a877b1c4c7b8770b4bb05a4f25c",
      "e23c7fa1201742f29ed8a2567b5c4531",
      "3506e1b7ec9a4de5b5e0987dddb57847",
      "1ef9d1adf77949c4a0f2b90b5cda9097",
      "3fe24de096414165bf959cf03f9783e8",
      "64118b8cea894a15b59f43c86498491a",
      "1e84a0552fd843e8808e09ab2cdd379c",
      "83f39581dadf4b72b658744c1a118902",
      "91e5981ef5114ae89261944a8531ffe9",
      "87674d17bb794a5a825122b3fa60490d",
      "579a4776338a43b88ee534fc452343ba",
      "3c8c5a7a8953443a8d6a19ae4fea3752",
      "36a863a6aa17453cb7c79cfc3919cd10",
      "5e3033cfbe7c4820a77b37441c4d19d8",
      "eab3ff7edb4a489b8ecf91383a835999",
      "3ddcc07f9c314cdf931e1ea4e15c9b46",
      "eeabe53d63c9491693eafdcd85965c2d",
      "57052490a530475eb99f65c765732d90",
      "040a811e3a4b4d0aaa50386b6a6eb041",
      "992450b0150a4e239164ad85f8e8e4c9",
      "693601a3425945639c7425b8e5e7d85d",
      "e2d98b246b0a4f718f3fec31cec8dc58",
      "7d945727fd91482eafe6cad9226caa22",
      "c795276ee44f4810af567db14ab1031c",
      "d92e04e3602240f9ada160ac5a1d6f5c",
      "2616fe4480cc41fc878037f956feb0dd",
      "3f9d075318b844069beaaa494a69c29c",
      "73801a9fa0304bb58514c63178b3986b",
      "e51dfc20c6cc4199b0851fdebb942d11",
      "ce0479cc66ca449dbe1b7dcc26acf9be",
      "67db08f030ef43bcbb045569b5536976",
      "f27055f9a298410c9448a83631b2953f",
      "825161f9596342ed89f6ce630681b03f",
      "76765dc137104f28985642ba21c0b2d5",
      "4a4dcf3834164271800d993c91c314d3",
      "4da4f254f9da47febee639368b4a8692",
      "c4e0bd9cb1c64bb7b6964897e68184ab",
      "5cb85f5a4e644588a4c562793ac95710",
      "e4e81494bfbe4b12b477d5089d5c3447",
      "f9ce16e17f0b4a80aeb453b0d664a583",
      "694f3199dab144578d2f7fc334409f91",
      "eeeb25cd22ba48789c2c3e7339fc41a0",
      "e0cd042e3f6b4a309319f2c34126cca8",
      "ad7dd40cdb154f529fe1c214fd744ef7",
      "3d7a536c6fa5417e883db854e5e6ba53",
      "0180a3263a604dfb9ea2ea302fa61a43",
      "ab13a3e15dbe4c2b8c806bc3c568374a",
      "a72cd4e1133345c2a5028d909b7b52ec",
      "30f41afa55704043a364ef535fb8f5a5",
      "3bf4c0e55ac4411599d52e4a93da4558",
      "51fe269a1eda49548375a69b8b60af83",
      "12f16139fe90448e80471c899ece7410",
      "9cc226c5e6fa4b8b8d84922114f4b923",
      "4b6a6d79cf1c406ea46812e99a7138ca",
      "1af48b61920a40de8ea4f57dbc8a7060",
      "3ca53733ad444158a20b7b239d6cc035",
      "8c06df2c52894d49be8d4f05ad407913",
      "56be08d413324685b0a8f27635b1bf93",
      "b056f277fc42406e9319b79f116e190d",
      "a9c843d4677248efb6f4b5fab20cdc2e",
      "cf8f914d0c5f42aeba4ed6ba1f943f92",
      "424f6524f0c94c63a310db48f1b71fd6",
      "af67a7a748e24065a14bc7c8f21a620f",
      "43a3080494ca437fbbd001490d459790",
      "485346469165451d86f52076929a39d8",
      "78c76960d4bd40e89ab80b10917d9e0b",
      "71050828b09542848d3a0604870a5dc5",
      "517c34d645754d708a57efbbd89dad6d",
      "6605b1471e784e2593b2424eb281f60e",
      "15c63eb7b3a84161ae5b6034b33b243e",
      "0de4b309aa3e43c4a35c6076f1f219e4",
      "31388aa9e9fb4cc8b712110cec5062b9",
      "85beba040b4f48c78679ac7cd54367f5",
      "4a1dd285a35b49e7aed830e71ac38dbc",
      "cbce3023cd33478a9eb54ef7f9540201",
      "6fb438eae74e40efadb01f2d6571f17a",
      "802134bfda6d4ed3845de32cb88dd0c9",
      "d792b4595fe140ac9ccdc5ab5f0ec7c0",
      "277e89f8ab4f415e91256f21d9a74421",
      "129e5e8ce9804e6a82a0a7280eea70be",
      "29c4dac2214d4d51925b4c9a75438642",
      "2ce3a12f2b6d401b9359f39694146cdd",
      "90b0354f35644c5a8414146803b376c5",
      "98a92c071f0042f984ad1d76848f0771",
      "a73efee0a6484ce6968dd596ef0f8821",
      "16b0d5098ee947a580c9819f1437e3e6",
      "4c68aad2db8b436caf18b74e31c331fb",
      "c90628de34d04bd6ab07d384c4c721d8",
      "15c4b71fa0324b6fb6354d6d8e43ba51",
      "a68e177688af417589ef0f680aea8ee9",
      "7472813dd2c2480f8bb44b4e701afc1b",
      "7c6c42de33d342edb94aa546ad41d1a8",
      "08ca792a178f4c22bf29685cd6404395",
      "b56dea77ffa54b499adcb3db2c73cc7c",
      "5d8c4fec5b9941d68c5ea22e90abe144",
      "2d3a2acdc9c14c6b906bdd4ee60c6a20",
      "5e2627321b254f1eb2f7b2ef63f2514a",
      "9f1900ca4cac4969a6bdb92af9e88938",
      "eb71cf4db0594e15bc7ff08eefb13d04",
      "ef81646e1aad40b9bd6bef4e90abeb87",
      "88ba5a2aeb314db38fe2219d7cf1d34d",
      "4944f9d890a04f0c8a848848133344c0",
      "bd62b91101224c86ba104171860b952c",
      "599d28124d4a49158ae6948a5132f413",
      "55b98924bb814435a4979d45f4879a43",
      "efcd58084fad49848ae1fa42ffdf396d",
      "89d41edad9974b5bb0d99306fc6be0fe",
      "fea15aa91d514131a5b8484c841a2fa4",
      "34a8f1b59291435a8eeefcc292947e6a",
      "15175f116c564ac6ba452ffaf9113d33",
      "feaaf5e4317e4e238f89a9f026628d04",
      "59ae6c794639429daa2dae34c21fa69f",
      "84a0528a52d0485487920511bd2f3e72",
      "5ec160edca4c489d9526b4e921e290d5",
      "142c0dc2be004d41968d4da82f5672a1",
      "3fcd413e0f7441bd97b4364c7bedfd78",
      "237b3476a29d4111b4f758a8f7368c96",
      "25599bf4ee7e46c39e0bdba8c4b099b1",
      "05ef92e995d145bcbc43a07da2aaa7ac",
      "c9aa9a0d497345ae83e3def84c51510d",
      "3f90c54f4ad14bdea5cff1d4e074b7db",
      "257737ea0f2c4a71ab26ba656bcab952",
      "5416eb9f65904e529d4455e3ec32fccc",
      "6f3ca13a16f1439ab8dc840fa23b0ef3",
      "44495e63c577419d89ca071993b6038c",
      "8242ebaa277041cb83300d394051f4c2",
      "f77d373063564b938d98618e8b8593ea",
      "bf5d0e2271e94a2b8a7dfc819395f093",
      "ff3a6acde12045aeb807f16b46ebcc15",
      "b70d3bfe1b924c97935fd9c487a654d2",
      "6dcc7e5623b947a5971ea18aeaef1bdc",
      "bd847ce9fe5f45aabb8913f034b44760",
      "713b8d17f0214983b0925af8183dd75f",
      "353d1729d012459a92765da9f5fb65cd",
      "12dce0849e5b4ed4a3401b0b5544506d",
      "983ecaa9b30447babdbf7badcda58c16",
      "2d97ea4b66414684902531d54a3ff878",
      "96f9a8c95cae4be8b823a8a3a4f5e165"
     ]
    },
    "id": "IgtgPUajb9X0",
    "outputId": "7ce1e7f3-3c13-4002-e03b-0c8a5dbf6032"
   },
   "outputs": [],
   "source": [
    "epochs = 7\n",
    "lr = 2e-4\n",
    "train_losses, val_losses = train(train_loader100, val_loader100, model, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CjxD4QLeShL"
   },
   "source": [
    "Get some metrics and plot your losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "tzOkOmIKeZTC",
    "outputId": "1da8c728-2941-4d1e-c371-1e0a754fc239"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "val_acc = get_acc(model, val_loader100)\n",
    "end = time.time()\n",
    "\n",
    "print('Batch_size={}, epochs={}, lr={}'.format(128, epochs, lr))\n",
    "print('Val accuracy =', val_acc)\n",
    "print(\"Process validation time: {:0.4f} s\".format(end - start))\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXrhKxS7h-9Q"
   },
   "source": [
    "## 4. MobileNet V2\n",
    "\n",
    "### Theory\n",
    "MobileNet V2 builds on the concepts of MobileNet V1, introducing two significant improvements:\n",
    "\n",
    "\n",
    "- **Inverted Residuals**: One of the most notable features of MobileNet V2 is the use of inverted residual blocks. Unlike traditional residual blocks that connect layers of the same depth, inverted residuals connect layers with different depths, allowing for more efficient information flow and reducing computational complexity.\n",
    "- **Linear Bottlenecks**: MobileNet V2 introduces linear bottlenecks between the layers. These bottlenecks help preserve the information by maintaining low-dimensional representations, which minimizes information loss and improves the overall accuracy of the model.\n",
    "- **Depthwise Separable Convolutions**: Similar to MobileNet V1, MobileNet V2 employs depthwise separable convolutions to reduce the number of parameters and computations. This technique splits the convolution into two separate operations: depthwise convolution and pointwise convolution, significantly reducing computational cost.\n",
    "- **ReLU6 Activation Function**: MobileNet V2 uses the ReLU6 activation function, which clips the ReLU output at 6. This helps prevent numerical instability in low-precision computations, making the model more suitable for mobile and embedded devices.\n",
    "\n",
    "These innovations enable MobileNet V2 to achieve better accuracy with minimal increase in complexity, making it highly suitable for mobile applications.\n",
    "\n",
    "You can learn more about MobileNetv2 [here](https://arxiv.org/abs/1801.04381).\n",
    "\n",
    "One of the paper’s major contributions is the linear bottleneck. In deep learning, especially in low-dimensional spaces, using non-linear layers like ReLU after compressing data can cause important information to be lost. MobileNetV2 addresses this by using linear layers instead of ReLU in the bottleneck stages.\n",
    "\n",
    "\n",
    "![lb](https://miro.medium.com/v2/resize:fit:720/format:webp/1*YGVXczC3hYtey2z-9MBkPw.png)\n",
    "\n",
    "\n",
    "Another key innovation in MobileNetV2 is the inverted residual block. Unlike traditional residual blocks (such as those used in ResNet), which expand first and then compress, inverted residuals do the opposite. In MobileNetV2, the input is first compressed and then expanded back to the original dimensions.\n",
    "\n",
    "![irb](https://miro.medium.com/v2/resize:fit:640/format:webp/1*KyvQcI3Z8AxPpxjLpzrRdg.png)\n",
    "\n",
    "As shown above, the inverted residual block (b) uses shortcuts, similar to those used in ResNet, but it compresses first and expands second, which makes it more efficient. This “inversion” allows for significant improvements in efficiency by reducing the number of operations required. To maintain performance, shortcuts (or skip connections) are used between the bottleneck layers, ensuring smooth gradient flow during training and preventing the loss of important information.\n",
    "\n",
    "Now where are going to implement the model. First complete the below functions and class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcjMeadouZEg"
   },
   "source": [
    "What is ReLU6 Activation Function? Why do we use that?\n",
    "\n",
    "**answer**<br/><br/>\n",
    "\n",
    "ReLU6 is a variant of ReLU that limits the output to a maximum of 6. It is used in MobileNetV2 to improve performance and robustness on mobile and embedded devices, where quantization and low-precision arithmetic are common. Its bounded output range helps with quantization, prevents numerical instability, and makes the model more efficient for mobile deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sU_NrBoWkJLK"
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv_bn(inp, oup, stride):\n",
    "    # Standard convolution followed by batch normalization and ReLU6 activation\n",
    "    # inp: input channels\n",
    "    # oup: output channels\n",
    "    # stride: stride for the convolution layer\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    # 1x1 pointwise convolution followed by batch normalization and ReLU6 activation\n",
    "    # inp: input channels\n",
    "    # oup: output channels\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio):\n",
    "        \"\"\"\n",
    "        Inverted Residual block with optional skip connection.\n",
    "        Args:\n",
    "            inp: Input channels.\n",
    "            oup: Output channels.\n",
    "            stride: Stride for depthwise convolution.\n",
    "            expand_ratio: Multiplier for the hidden dimension size.\n",
    "        \"\"\"\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        hidden_dim = int(inp * expand_ratio)\n",
    "        self.use_residual = (self.stride == 1 and inp == oup)\n",
    "\n",
    "        layers = []\n",
    "        if expand_ratio == 1:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup)\n",
    "            ])\n",
    "        else:\n",
    "            layers.extend([\n",
    "                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.ReLU6(inplace=True),\n",
    "\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup)\n",
    "            ])\n",
    "\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass with optional residual connection.\n",
    "        \"\"\"\n",
    "        if self.use_residual:\n",
    "            return x + self.conv(x)  # Residual connection\n",
    "        else:\n",
    "            return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyop-GGYlGrg"
   },
   "source": [
    "Now create the model using the image below. Please note that you should have the width_multiplier hyperparameter in your model. we will use it later.\n",
    "\n",
    "![architecture](https://i.sstatic.net/1RAkv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O-9b1UY8h-9R"
   },
   "outputs": [],
   "source": [
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_multiplier=0.1):\n",
    "\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        block = InvertedResidual\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        interverted_residual_setting = [# t, c, n, s\n",
    "                                        [1, 16, 1, 1],\n",
    "                                        [6, 24, 2, 2],\n",
    "                                        [6, 32, 3, 2],\n",
    "                                        [6, 64, 4, 2],\n",
    "                                        [6, 96, 3, 1],\n",
    "                                        [6, 160, 3, 2],\n",
    "                                        [6, 320, 1, 1] ]\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = int(input_channel * width_multiplier)\n",
    "        layers_features = [conv_bn(3, input_channel, 2)]\n",
    "        for t, c, n, s in interverted_residual_setting:\n",
    "            output_channel = int(c * width_multiplier)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    layers_features.append(InvertedResidual(\n",
    "                        input_channel, output_channel, s, expand_ratio=t))\n",
    "                else:\n",
    "                    layers_features.append(InvertedResidual(\n",
    "                        input_channel, output_channel, 1, expand_ratio=t))\n",
    "                input_channel = output_channel\n",
    "\n",
    "        last_channel = last_channel if width_multiplier >= 1.0 else int(\n",
    "            last_channel * width_multiplier)\n",
    "        layers_features.append(conv_1x1_bn(input_channel, last_channel))\n",
    "        self.features = nn.Sequential(*layers_features)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(last_channel, n_class)\n",
    "\n",
    "        for i in self.modules():\n",
    "            if isinstance(i, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(i.weight, mode='fan_out')\n",
    "            elif isinstance(i, nn.BatchNorm2d):\n",
    "                nn.init.ones_(i.weight)\n",
    "                nn.init.zeros_(i.bias)\n",
    "            elif isinstance(i, nn.Linear):\n",
    "                nn.init.normal_(i.weight, 0, 0.01)\n",
    "                nn.init.zeros_(i.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnTPSadvmsAy"
   },
   "source": [
    "How did you create linear bottlenecks? How do they help?\n",
    "\n",
    "\n",
    "**answer**<br/><br/>\n",
    "\n",
    "Linear bottlenecks are an essential design choice for lightweight architectures like MobileNetV2. They work by balancing the trade-offs between preserving useful information and minimizing computational overhead. By combining expansion, depthwise separable convolution, and linear projection, they enable high-performance models that are computationally efficient and suitable for mobile and embedded applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrEOMCkFmELE"
   },
   "source": [
    "Train your model with CIFAR10 dataset. Set the width_multiplier with value 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 831,
     "referenced_widgets": [
      "afcb7eff874b4feab48376c09b30460b",
      "2408c60a033a4b3085069d41f55dadc5",
      "3dfe5af158554d7d9ec907853a861d24",
      "e5dbb52fb4eb4f279838235dc741ab69",
      "12563400c89745e8a6c55f3b9cc7cf2b",
      "c6cb79670eac49a08051e25681df5f02",
      "f74bf984c05f44c6b059a5a682d340bb",
      "4db62f2441e5414eb8eca4aba46c097b",
      "ad83801f27dc4e66980c1f7f548b7370",
      "696006ff2f4448b88eb000e424a73b11",
      "080c0b46ad6142299250bbc74bc2fc85",
      "4de61728078447e3a206e3efc3650352",
      "f935a798b81d4c6a81417ceee3e11fb6",
      "e998965172de40f19928d51c9bf6ede5",
      "fe41d28bb6014e55b477744a3fdb8927",
      "1faf7ce4537b4da9865062bb10400a20",
      "e701941c27394aca9557ffcf18ea83f5",
      "7659d6a47e77428cbc2f9347ff7680f5",
      "538fac70faad4f7bb696b540dcd9783e",
      "7fc83a59ec3e40a782fc4c7eb19eab4c",
      "7dd25cfa47ea4aa1855b3175a0895067",
      "4141ba933edf45eeac7ae1ff97973b33",
      "75ef84cc6e004523b4ab85d3843e3a8b",
      "47a113e7af77472aba1ffaf392add71e",
      "96583592045c4fa2b4ca4cd66c817a78",
      "62f8810894eb41dda94a584f3165a8ff",
      "98a8860e271f4ef0b5df877e21bc5bc3",
      "8c5c72136bc542e9b0c8d8929b48e40f",
      "6fb133d0f1064fb8b42002f33138408c",
      "80707f246c334759b15d93135fec301b",
      "ed3049117b574a63902d9486b417c66e",
      "bc26667cff4a404b8675b204956f118d",
      "88cdeedcd3064e4cbfa2ae73525510ea",
      "b2a5c8483f734efdb9fd257c6f8bf9c8",
      "d607e3a481e0490c8f3d5ae051047dd9",
      "ac49966ba9c443a6ae7feab0dc954922",
      "11e1941f184e4874bf32297bbe2e25f4",
      "7d54a84ecc274ba89ecc1d327a17eb7d",
      "d45ee7bd194c4941b6f6f8479876ea24",
      "e5ec204302e74a7b8e213d3aeb53051c",
      "2393af2e586c4b2d84153e6be107d1e5",
      "02050bc9f8464a8c970ff646abd4bc64",
      "62e92655030b42b88840fdba801e2723",
      "0aac411319894b7795bd8f05b08ce0c7",
      "9357f89881354d12a3eeeb425142f53f",
      "67ac631d89dc410598054d7b5de0b32f",
      "2ea9b9e67f2e4de99a69f630fa32f023",
      "d6e5e686e3b34dbaad7dbc4f7f9f1030",
      "f191c2f4ac724c029ca76a61f4480050",
      "15fe76c2832e4cbf82ff08c60ed9ce7a",
      "d5aa49993c0540d08bb0b2c1099cbf2c",
      "f6a8e73ea18742999ec2376dd66902cd",
      "e7d08d39db5e463bbc3c0e6b4863f9ad",
      "d3f6b2a859f0441c9df4d8cf0099758a",
      "e9e42d75e6ed4b2885411eac3d42c8c0",
      "8d97887f847948379dc51ab765e2915e",
      "c1368ccdfd9d484fbc3def5d2ce3508b",
      "32128a3a140e45d5968308fec6221ebc",
      "d8f901649b9e4340b0acd1e929b8cf2c",
      "9bf2fe624f35461895e24f630b0297dc",
      "fc938e6e791b47599e4a56ed7c00c3bf",
      "47a7288ec2ff4464943ccf1209865ff6",
      "9fe867dd71314ae2923dcd497a7747f5",
      "3e8f60eeaf604402b38fdd9a30a949f5",
      "89d0992fb70040ee947671673c15adc9",
      "9e63be445b2b43249e0976d22d03b1ff",
      "bff13f6d47784529bb5ac051d0e8a924",
      "675dc6d6c24e4b338281b90d7c4216f2",
      "41771637b40444e283a145c47ac7bca4",
      "2953c9e4dbfd486e883c7dc52caed1d7",
      "7f5eee865a6e4b448b739f31509e0811",
      "4cd958573f4644cdae623f18146930aa",
      "11fb94ee687d4c17bf1c386e788810cc",
      "f85bfc1fa2334681aec8dd246688c203",
      "93d7c232f60d4a1f8c8e308941391f84",
      "4a041720fe4c48b283d1fc1ef4248f85",
      "bd4ff38e1fc946bd801304963686d5e1",
      "72f22bac9fd2489298a2405f6dcf50ea",
      "3610c797d5af48e3a56df7107ea91d5f",
      "085dbb95d17b41e084d659b43362df02",
      "f14ed57da6ca49048df39b6aa5fd8033",
      "b5fed641c02d4144bf79802691e1d150",
      "b9b5146f37224034a3c23171e264a0ef",
      "670f1b8d92fb45bba09026437494f249",
      "e922ebeb8f144018b2fd7def7d2b381f",
      "614b9cb111014ba0973e8b8ed32fb4f9",
      "e308a34f66394542b71bbf09f05dc79b",
      "d5b9f1d8034449a0b7aed503eea69696",
      "b16035c79944446a9e957a6e982dee8f",
      "5ba7f501851b4858a57bf2d7e55d60a9",
      "4d376ef485434e56a5b1704c03c44caa",
      "48abdb1ef7944535940cafb5bb8784e6",
      "79766f35eeb448f4948c8f80f10d2cd8",
      "ecf99927894348d4b83b0c1e50183891",
      "ffcb82bfc32747439702fe9713301933",
      "e82b991c25a14d42a9119b01c5b01d03",
      "ffb37f6ddf6e4047858084edf3ab8a66",
      "779aa4c35397457cb37a18fda5d3ea48",
      "dd009c441c5a4fcaa1a731d02221612b",
      "95d16dddcdfa45bbac515a756b05c1f8",
      "a1e9c4b7b61346c4adf7381509660c99",
      "a62a19dca64a4ba186b24ed56ba46353",
      "608beac0dfbb4502a2dcdbc5803c0f00",
      "2f1223d916f548f099e0abe13c50dec2",
      "5e57ba158edd460eab2f483d35ef9617",
      "18079a65172a4cb2b6f085ea4d696582",
      "66e6f517a40d42dca3b4b176453e9a40",
      "6208e5a8a47e4e1ab087f651dd8afe2e",
      "8c2f7ad7e97148d2918d6c3b30e0d162",
      "8c02613a0e8145c282f0d973df1b0d48",
      "98756bc5981e495f93b320f1c560343f",
      "c32679443200478aa47b9feea92fcf94",
      "07fe82306d9b4aacbc0fb3d21032365a",
      "2be1e16528b84be1a3331da92efa7501",
      "45ac14f8aca44ca6b4727f27bf2d596b",
      "c64a39e01cd54ec9bdb51dfe6e9c6267",
      "7ac4cffdf7e140d79f5c9270dccced0c",
      "b20792226288470c82b053e800b2df23",
      "5bfef205599b41e7a55daba69549da67",
      "b5c68afb8ea74b31989f22ae104258b0",
      "dc64b72d45914aca99460b6c2e9ded4f",
      "f7e3e271c7fd43a69c1ebd48dbb56738",
      "d97f4e8bce2a43e4931f8f2995fe1e96",
      "f0e5f2eb2a7b46e681e875ef936213e2",
      "acdaef274c054684b2e02ad94e7ed4fd",
      "a025646bfb7346848cdd4fd09e3392de",
      "8697f534a7714a4aa760c351f8c6985b",
      "c27d43f0553e4b9ebf5eea54c3b38913",
      "9e6faae98a9440aa80c5819b26137904",
      "d9cf802a56d34674b54009babae125ed",
      "537196a215fe40338fdd1d64251ce08e",
      "459a7bdc0084423cb3f882823f11f71c",
      "ad1ee5f5a3884558b3e1c673082f1b09",
      "1fbd82e5b7bb429685ea4c8e5422708e",
      "3f25f5e1f2d0475286b6a43847a38381",
      "5acce2e1f0904495a053595723dbb492",
      "2d90216482264c07ad17fda627503677",
      "b2455d416ada41e7be608556d7a03d8f",
      "b9e27eb7eddb49e3a9cf0b3843c9acad",
      "6e40f228d0084e3fa148c020d710f4cb",
      "d18c0bee98a44b788744d28148da788b",
      "d6b858d11ba4416eb9d84f762a8ec0b7",
      "1679370b5edc4bd7b149a92e75a260ea",
      "b61460614be04b13a8a9db0c8d311981",
      "3f6bf947035847408ec653f03a33a952",
      "fd04ff5a1fb349fc90417fb9aac891f6",
      "9097c396b40b4621b1442078eecaa50b",
      "4cccba86fedf4863a91f4e5736e2afc6",
      "0cf276ca8d0543fbb8dc33e1892db973",
      "d04174828baa4ca8931518111beeb740",
      "55ecbd62d8e74191a84ec9b503c859c6",
      "dfac3bd4f38a4d75aa064692d8e8e9b8",
      "2270a8832c0c48afa4092d95efe48ac2",
      "7a3902df1de44e64818ab71b9522dec3",
      "8313afe1ad1c434dba856da165ec63ca",
      "216a5cdce2e54de69a37eb5059d1c6f0",
      "cda58d523c1642cd8a491183c6f0eab0",
      "eb3c499d103d44fdb59859683257194f",
      "67bd9e53fa734bfda662a5e43b5fb41a",
      "d2d2deade1ba45378c3c705c5762d3cb",
      "f77cc3716ae542e1b8c6c93dd14a6153",
      "14fe9bbfff49433184570edef7ea8b0f",
      "4a81fdd443424df7bf65a41b475685c3",
      "0bd32362194d4c5d8973f2d7fd91380c",
      "a981b4391b904b1c9a77bc918a3f2c9f",
      "129af2f40571452696d789d2e5427923",
      "f5b2a1a37b324a0abd2e2166c45ff2c6",
      "bdf40e3fb5c24cfea49eb5b0f070240f",
      "8f9a4934975748ffbb9583081607d16e",
      "fff5a499031a4f8fa285beb73406b5f7",
      "9512e2ed3a03483e8946001a8ebef2fd",
      "2d23c913137744dfbf0959259062dc3e",
      "b2348304022d48198018aff8335014dd",
      "2208490219b4414a8be769d83dc25805",
      "2c9502bbe47345519f3583f412694ad9",
      "316ac5085f2c4bed9368e4ab8544586a",
      "76e0a3edafd84ac8ae2ee0780bc46c8d",
      "c578260f9c1144f1ba2568de0cef26fe",
      "88c82f0b403748a9a4a961f53d7268af",
      "26ec26c747074ad5a8142c8ebe51cbf5",
      "4b99d770fbff4dc1b40ea6769b357f48",
      "e838a4c51a2e45aa98cff4395d237e52",
      "8f5c1158ac644baf95ed8a11a7b731e0",
      "172d5a8a0c5d4bd2a637b459127d021b",
      "3166c81644ec4a1096b75b3c495b681c",
      "7078281c98a64ab2a80639c643ea0a64",
      "21e6663c6b5b4ff28fb51ba73620bd3c",
      "a27bdaf1568e448fbbbb987091ec67a8",
      "367a41f99c854d8c99dcb4cbea5cd9dd",
      "164a30ed943e4f6496d7194929e0202b",
      "88b79a683eae48ef90cecafac9b11dfb",
      "161ae69fce9249ff83f0ae3bf2f02c5d",
      "1827f1f57a0448259359af6a2a159008",
      "ff3fb9a80c354d1aacfba2efb44341d1",
      "9e6da3bbef7043ac8d31afee6d32632e",
      "dd10fa4ac4f44f40af95d82512505275",
      "8867a470254d4c49976fd785d56281f1",
      "fa3577ef19844427887a826d8b5ca98e",
      "19f43db9b5c54a70a6bd8dfbedf1d54e",
      "f1a926556a154f14b542559ea2d01e18",
      "c0053344358d435ba88ca6e82ae35ada",
      "1c225b87c53c440dae7ad1cf80ea57aa",
      "6e4b26b8d3b04a9f94688d81b6c2374f",
      "8f6398288fcb44059838b54864e6404a",
      "cfaf3691ff29423db9c06e64c6db94f4",
      "364cc4153a944ed489fc730bb32c19e5",
      "667690351ccb45d68f41be92aa4512fd",
      "28e0227243f74ee8957ee0e80df47291",
      "628f1087915041508bf0e54f40d1b65e",
      "4af1b2af598a436fa6c634548e9b9609",
      "e2973a7ff99b4d43be7ae1651a114b51",
      "09317a0d2a994cdfb5a5767d4645a4a1",
      "ea33737774a54d55b9eabcc5e791991e",
      "251f5bd899384fa6a2bb1aa5bbde7851",
      "348ba737fe9148b2b5932d15cbe86e3a",
      "e1eb8b1eae824a0b9a7633ceca6bc2fa",
      "52cb272621e44b23b7454d8cfb541506",
      "185f0ebd4abb48cf8403956580a8780c",
      "8c2f06a0367643a89872f71a0c43c249",
      "9a98ec87503d44ed9f89c6d69e07bc24"
     ]
    },
    "id": "24pT108Hqy_R",
    "outputId": "7c7ba4bc-ed9f-4323-c3e1-8e0fb94715a3"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = MobileNetV2(n_class=10).to(device)\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "train_losses, val_losses = train(train_loader, val_loader, model, epochs, lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Xg_j_ZcmT9n"
   },
   "source": [
    "Get some metrics and plot your losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "Pw6lqtKhstPb",
    "outputId": "24d0747d-b215-4276-8bb5-42e6502fd92d"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "val_acc = get_acc(model, val_loader)\n",
    "end = time.time()\n",
    "\n",
    "print('Batch_size={}, epochs={}, lr={}'.format(128, epochs, lr))\n",
    "print('Val accuracy =', val_acc)\n",
    "print(\"Process validation time: {:0.4f} s\".format(end - start))\n",
    "\n",
    "plot_losses(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXzuRJwTtb4k"
   },
   "source": [
    "Now we want to see the effect of the hyperparameter (width multiplier) in our model. For all values between 0.1 to 1 with step 0.1 print the number of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FNwdUnplts7u",
    "outputId": "6c0fe9fc-6ba2-4255-8834-30d900265dba"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "width_multipliers = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for width_multiplier in width_multipliers:\n",
    "    model = MobileNetV2(width_multiplier=width_multiplier)\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Width Multiplier: {width_multiplier}, Number of Parameters: {num_params}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCGGAl05cQMs"
   },
   "source": [
    "Now load the pretrained model that we trained with width_multiplier=1/2 for 15 epoches. then compare it's accuracy and time it needs for validation data with normal MobileNetv2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rTkRbb0fXN7z",
    "outputId": "59377787-dfe4-4c30-ec7d-ecc6c1b43ac3"
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "MNv2_path = './MNv2WMmodel.pt'\n",
    "\n",
    "# Load the pretrained model with width_multiplier=0.5\n",
    "model = MobileNetV2(n_class=10, width_multiplier=0.5).to(device)\n",
    "model.load_state_dict(torch.load(MNv2_path, map_location=device))\n",
    "\n",
    "# Evaluate accuracy and measure validation time for the width_multiplier=0.5 model\n",
    "start_time = time.time()\n",
    "acc_wm = get_acc(model, test_loader)\n",
    "end_time = time.time()\n",
    "time_wm = end_time - start_time\n",
    "print(\n",
    "    f\"Width multiplier=0.5 model: Accuracy = {acc_wm:.2f}%, Validation Time = {time_wm:.4f}s\")\n",
    "\n",
    "model_normal = MobileNetV2(n_class=10, width_multiplier=1.0).to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "acc_normal = get_acc(model_normal, test_loader)\n",
    "end_time = time.time()\n",
    "time_normal = end_time - start_time\n",
    "print(\n",
    "    f\"Width multiplier=1.0 model: Accuracy = {acc_normal:.2f}%, Validation Time = {time_normal:.4f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "75ebd36370b6498ca167ee22e4cb0b57",
      "34743e7fb4664d9b9660e3a9bbcbb00e",
      "0cc5eb59f0a8457a87413bc1ce715719",
      "85775a24d5d4418a8ed560950a49d1c0",
      "738e14e43d3e4a35933d68e67f5c29ec",
      "83acfcb3a6f744a8843d10751a830ae1",
      "0946bd2d9dd6422388cbdf648dab9ce3",
      "27994aa838c447d694077575520e6bf7",
      "a7a64cbff7cd42b3b66d206763db2d6e",
      "6f6b4962c5ec48338a1ff54cdae1aa8a",
      "1202ae7e7ba54bafa397b7ae7c2e4c55"
     ]
    },
    "id": "TWoq3xl9cqcb",
    "outputId": "4a4b9b5c-5c5c-4d9c-c359-0e9aaa324906"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "val_acc = get_acc(model, val_loader)\n",
    "end = time.time()\n",
    "\n",
    "print('Val accuracy =', val_acc)\n",
    "print(\"Process validation time: {:0.4f} s\".format(end - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7mRD2iKuvo8"
   },
   "source": [
    "Explain how this hyperparameter changes the number of parameters in theory. Write a formula to predict its effect without actually creating the models. Compare your formula with the above numbers and if they dont match perfectly explain a bit.\n",
    "\n",
    "**answer**<br/><br/>\n",
    "The width multiplier (α) directly scales the number of channels in each layer of the MobileNetV2 architecture. This scaling affects the number of parameters in convolutional layers, which are the primary contributors to the overall parameter count.<br/>\n",
    "\n",
    "Convolutional layers have parameters proportional to: (input channels * output channels * kernel size^2).<br/>\n",
    "In MobileNetV2, the width multiplier (α) scales both input and output channels of the depthwise and pointwise convolutions within inverted residual blocks.<br/>\n",
    "Therefore, the approximate effect on the number of parameters is:\n",
    "\n",
    "New parameters ≈ α^2 * original parameters<br/>\n",
    "\n",
    "When you printed the number of parameters for different width multipliers, there is a discrepancies compared to the formula above. This is because:\n",
    "\n",
    "Not all layers are equally affected: The width multiplier primarily impacts convolutional layers. The fully connected layer at the end and batch normalization parameters have a smaller relative change.<br/><br/>\n",
    "Rounding: The number of channels is rounded to integers after applying the width multiplier. This rounding introduces some error.<br/><br/>\n",
    "Specific architecture: Certain layers in MobileNetV2 have fixed channel numbers, which are not scaled by the width multiplier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIGDyYFvTs2n"
   },
   "source": [
    "Another hyperparameter is resolution multiplier. Can it make our model faster? Does it decrease the number of parameters? Explain.\n",
    "\n",
    "**answer**<br/><br/>\n",
    "\n",
    "The resolution multiplier (ρ) is applied to the input image size. It scales the width and height of the input image, effectively changing the resolution at which the model operates.<br/>\n",
    "\n",
    "the resolution multiplier can make the model faster. Here's because:<br/>\n",
    "\n",
    "Reduced Computation: Lowering the input resolution decreases the number of pixels the model needs to process. This directly reduces the computational load in convolutional layers, leading to faster inference times.<br/>\n",
    "Smaller Feature Maps: The intermediate feature maps within the network also become smaller with a reduced input resolution. This further reduces memory and computational requirements, leading to speed improvements.<br/>\n",
    "\n",
    "On the other hand, the resolution multiplier does not directly decrease the number of parameters. The number of parameters in a convolutional layer is primarily determined by the number of input and output channels and the kernel size, which remain unchanged by the resolution multiplier.<br/>\n",
    "\n",
    "So there is a trade-off between speed and accuracy when using the resolution multiplier. Lowering the resolution can significantly speed up inference but may come at the cost of reduced accuracy, as the model works with less detailed information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS2ZeEX-v7yc"
   },
   "source": [
    "Why is the number of parameters important for us? Now we have GPUs that can increase performance considerably. Is the number of parameters really a big deal?\n",
    "\n",
    "**answer**<br/><br/>\n",
    "\n",
    "While GPUs accelerate training and inference, the number of parameters still significantly impacts memory usage, inference speed, training time, power consumption, and the risk of overfitting. Therefore, optimizing the number of parameters remains a critical aspect of building efficient and effective deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJzkGxcp6d-F"
   },
   "source": [
    "## 5.Knowledge Distillation\n",
    "\n",
    "Now we learn how you can implement knowledge distillation using a pre-trained ResNet as the teacher model and MobileNetV2 as the student model for the CIFAR-10 dataset.\n",
    "\n",
    "Knowledge distillation is a process where a smaller, more efficient model (the student) is trained to replicate the behavior of a larger, more complex model (the teacher). This is particularly useful for deploying models on resource-constrained devices where efficiency and speed are critical.\n",
    "\n",
    "You can learn more about the loss function we use for this porpuse [here](\n",
    "https://medium.com/@aryamaanthakur/knowledge-distillation-make-your-neural-networks-smaller-398485f811c6)\n",
    "\n",
    "First create our teacher.(Pay attention that we need to finetune this teacher for out task, but because of lack of resources we will just use it with terrible accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153,
     "referenced_widgets": [
      "13b45b692dc14888b8b2420c4e7eefcb",
      "d84bc273b79a47eaa490a89a84423de2",
      "623e15052b534cd8956bdf058ce5b179",
      "8ec76b9d07884afa92da6df443167419",
      "4de8819941d94408b2bd7a13a6626d1d",
      "5a754f73352343a395b3aa5c77cd932a",
      "c6058350d5d34d2f9423fb6603097f08",
      "bc9eed156e6742a982cf6ff3c7fe8542",
      "63483fa6b16041cea79d3256ea49ab78",
      "21d56ddcc35d4f35a0143e5ab3224e57",
      "0aa20363e2234009a6e953c5709f6e30"
     ]
    },
    "id": "TDuAJ9sw8GIR",
    "outputId": "555b0e4c-4266-48b8-c75d-dc057309d0c6"
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "\n",
    "\n",
    "# Load a pre-trained ResNet18 fine-tuned on CIFAR-10\n",
    "teacher_model = timm.create_model('resnet18', pretrained=True, num_classes=10)\n",
    "teacher_model = teacher_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9E6Z60F-8UNW",
    "outputId": "44a8ea35-b11b-4b29-d165-6174307d51ed"
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "val_acc = get_acc(teacher_model, val_loader)\n",
    "end = time.time()\n",
    "\n",
    "print('Val accuracy =', val_acc)\n",
    "print(\"Process validation time: {:0.4f} s\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meq_VaaeBwt_"
   },
   "source": [
    "Now create a mobilenetv2 model and then write the code to\n",
    "\n",
    "---\n",
    "\n",
    "train it with Knowledge Distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mn6g8n2H68XM"
   },
   "outputs": [],
   "source": [
    "class DistillationLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom loss function for knowledge distillation combining KL divergence\n",
    "    and cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        temperature (float): Temperature parameter for scaling logits.\n",
    "        alpha (float): Weighting factor for distillation and classification loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, temperature: float = 3.0, alpha: float = 0.5):\n",
    "\n",
    "        super(DistillationLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha\n",
    "        self.kl_divergence = nn.KLDivLoss(reduction='batchmean')  # KL divergence\n",
    "\n",
    "    def forward(self,\n",
    "                student_logits: torch.Tensor,\n",
    "                teacher_logits: torch.Tensor,\n",
    "                labels: torch.Tensor) -> torch.Tensor:\n",
    "          \"\"\"\n",
    "          Compute the combined distillation and classification loss.\n",
    "          \"\"\"\n",
    "          # Scale logits with temperature\n",
    "          student_logits_scaled = student_logits / self.temperature\n",
    "          teacher_logits_scaled = teacher_logits / self.temperature\n",
    "\n",
    "          # Soft targets from teacher\n",
    "          teacher_probs = torch.softmax(teacher_logits_scaled, dim=1)\n",
    "          student_log_probs = torch.log_softmax(student_logits_scaled, dim=1)\n",
    "\n",
    "          # Distillation loss (KL Divergence between teacher and student logits)\n",
    "          distillation_loss = self.kl_divergence(student_log_probs, teacher_probs) * (self.temperature ** 2)\n",
    "\n",
    "          # Classification loss (CrossEntropy with hard labels)\n",
    "          classification_loss = F.cross_entropy(student_logits, labels)\n",
    "\n",
    "          # Combine the losses\n",
    "          loss = self.alpha * distillation_loss + (1 - self.alpha) * classification_loss\n",
    "          return loss\n",
    "\n",
    "\n",
    "\n",
    "def train_student(teacher: nn.Module,\n",
    "                  student: nn.Module,\n",
    "                  train_loader: torch.utils.data.DataLoader,\n",
    "                  test_loader: torch.utils.data.DataLoader,\n",
    "                  device: torch.device,\n",
    "                  epochs: int = 10,\n",
    "                  lr: float = 0.01) -> None:\n",
    "    \"\"\"\n",
    "    Train a student model using knowledge distillation.\n",
    "\n",
    "    Args:\n",
    "        teacher (nn.Module): Pre-trained teacher model.\n",
    "        student (nn.Module): Student model to train.\n",
    "        train_loader (torch.utils.data.DataLoader): DataLoader for training data.\n",
    "        test_loader (torch.utils.data.DataLoader): DataLoader for test data.\n",
    "        device (torch.device): Device to perform training on (CPU or GPU).\n",
    "        epochs (int): Number of training epochs. Default is 10.\n",
    "        lr (float): Learning rate for the optimizer. Default is 0.01.\n",
    "    \"\"\"\n",
    "    teacher.to(device).eval()\n",
    "    student.to(device)\n",
    "\n",
    "    criterion = DistillationLoss(temperature=3.0, alpha=0.5)\n",
    "    optimizer = optim.Adam(student.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        student.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Training loop\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Get teacher and student outputs\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(images)\n",
    "\n",
    "            student_logits = student(images)\n",
    "\n",
    "            # Compute distillation loss\n",
    "            loss = criterion(student_logits, teacher_logits, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    start = time.time()\n",
    "    test_acc = get_acc(student, val_loader)\n",
    "    end = time.time()\n",
    "\n",
    "    print('Test accuracy =', test_acc)\n",
    "    print(\"Process Test time: {:0.4f} s\".format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KICfRBzdB9vb"
   },
   "source": [
    "Train it for one epoch to just check if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcpAPKlfCCuX",
    "outputId": "fad184e9-6347-418b-ca3b-165d2b74afb1"
   },
   "outputs": [],
   "source": [
    "student = MobileNetV2(n_class=10)\n",
    "\n",
    "train_student(teacher_model, student, train_loader, val_loader, device, epochs=1, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_gGxABhEZzL"
   },
   "source": [
    "What are the potential trade-offs of using a high value of alpha (e.g., 0.9) versus a low value of alpha (e.g., 0.1) in the DistillationLoss? How might these trade-offs impact the student model's performance in terms of generalization to the target task and alignment with the teacher's knowledge?\n",
    "\n",
    "\n",
    "**answer**<br/><br/>\n",
    "High Alpha (e.g., 0.9):<br/>\n",
    "\n",
    "Emphasis: Focuses more on the teacher's soft predictions.<br/>\n",
    "Benefits: Captures nuanced inter-class relationships and teacher's knowledge.<br/>\n",
    "Risks: Overfits to teacher biases/errors; may reduce generalization if the teacher is misaligned with the target task.<br/>\n",
    "\n",
    "Low Alpha (e.g., 0.1):<br/>\n",
    "\n",
    "Emphasis: Focuses more on the true labels (hard labels).<br/>\n",
    "Benefits: Better generalization to the target task; less reliance on the teacher's potential errors.<br/>\n",
    "Risks: Misses out on nuanced inter-class relationships provided by the teacher.<br/>\n",
    "\n",
    "Choosing Alpha:<br/>\n",
    "\n",
    "Use high alpha if the teacher is accurate and the dataset is small/noisy.<br/>\n",
    "Use low alpha if the dataset is large and reliable or the teacher is less aligned with the target task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kjUmAtDh-9S"
   },
   "source": [
    "## Summary\n",
    "- **MobileNet V1**: Introduced depthwise separable convolutions, significantly reducing computation and model size.\n",
    "- **MobileNet V2**: Added inverted residuals and linear bottlenecks to improve efficiency and performance, reducing memory use without sacrificing accuracy.\n",
    "\n",
    "These architectures are optimized for mobile and embedded applications, providing a balance between accuracy and efficiency through lightweight, effective design.\n",
    "\n",
    "question for yourself: What new features and innovations did MobileNetV3 introduce to improve both efficiency and accuracy over its predecessors? Explore its design choices and how they further optimize MobileNet for real-time applications and mobile deployment.\n",
    "\n",
    "## Refrences\n",
    "\n",
    "[1] Andrew G. Howard, Menglong Zhu, Bo Chen,\n",
    "Dmitry Kalenichenko, Weijun Wang, Tobias\n",
    "Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR,\n",
    "abs/1704.04861, 2017\n",
    "\n",
    "[2] Mark Sandler, Andrew G. Howard, Menglong Zhu, Andrey\n",
    "Zhmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\n",
    "residuals and linear bottlenecks. mobile networks for classification, detection and segmentation. CoRR, abs/1801.04381,\n",
    "2018"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
